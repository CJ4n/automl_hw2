{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install mljar-supervised"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\JAN_CICHOMSKI\\STUDIA\\STUDIA_SEMESTR_7_2023_ZIMA\\auto_ml\\homeworks\\homework_1\\AUTOML_HM2\\.venv\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import time\n",
    "from os import path\n",
    "\n",
    "import joblib\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from autogluon.tabular import TabularPredictor\n",
    "from scipy.stats import randint, uniform\n",
    "from sklearn.ensemble import RandomForestClassifier, StackingClassifier\n",
    "from sklearn.feature_selection import SelectKBest, VarianceThreshold, f_classif\n",
    "from sklearn.linear_model import ElasticNet, LogisticRegression\n",
    "from sklearn.metrics import balanced_accuracy_score, mutual_info_score\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.utils import shuffle\n",
    "from supervised.automl import AutoML  # mljar-supervised"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Constants\n",
    "\n",
    "SEED = 42\n",
    "N_JOBS = -1\n",
    "RANDOM_SEARCH_N_ITER = 10\n",
    "TRAIN_TIME_LIMIT_AUTOGLUON = 60 * 1\n",
    "TRAIN_TIME_LIMIT_MLJAR = 60 * 1\n",
    "TRAIN_TIME_LIMIT_AUTO_SKLEARN = 60 * 1\n",
    "OUTPUT_DIR_MANUAL = path.join(\"output\", \"manual\")\n",
    "OUTPUT_DIR_AUTOGLUON = path.join(\"output\", \"autogluon\")\n",
    "OUTPUT_DIR_MLJAR = path.join(\"output\", \"mljar\")\n",
    "OUTPUT_DIR_AUTO_SKLEARN = path.join(\"output\", \"auto_sklearn\")\n",
    "UNIQUE_ID = time.strftime(\"%Y%m%d_%H%M%S\")\n",
    "APPLY_REMOVE_LOW_VARIANCE_FEATURES = True\n",
    "APPLY_REMOVE_CORRELATED_FEATURES = True\n",
    "APPLY_REMOVE_RANDOM_FEATURES = True\n",
    "APPLY_ANOVA = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating output directory output\\manual\\20240110_170735\n",
      "Creating output directory output\\autogluon\\20240110_170735\n",
      "Creating output directory output\\mljar\\20240110_170735\n",
      "Creating output directory output\\auto_sklearn\\20240110_170735\n"
     ]
    }
   ],
   "source": [
    "# prepare output directories\n",
    "for output_dir in [\n",
    "    OUTPUT_DIR_MANUAL,\n",
    "    OUTPUT_DIR_AUTOGLUON,\n",
    "    OUTPUT_DIR_MLJAR,\n",
    "    OUTPUT_DIR_AUTO_SKLEARN,\n",
    "]:\n",
    "    if not path.exists(path.join(output_dir, UNIQUE_ID)):\n",
    "        print(f\"Creating output directory {path.join(output_dir, UNIQUE_ID)}\")\n",
    "        os.makedirs(path.join(output_dir, UNIQUE_ID))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove Highly Correlated Columns\n",
    "# def remove_highly_correlated_features(train_x, valid_x, test_x, threshold=0.95):\n",
    "#     corr_matrix = np.corrcoef(train_x, rowvar=False)\n",
    "#     upper = np.triu(np.ones(corr_matrix.shape), k=1).astype(bool)\n",
    "#     to_drop = np.where(np.abs(corr_matrix[upper]) > threshold)[0]\n",
    "#     print(to_drop)\n",
    "#     train_x = np.delete(train_x, to_drop, axis=1)\n",
    "#     valid_x = np.delete(valid_x, to_drop, axis=1)\n",
    "#     test_x = np.delete(test_x, to_drop, axis=1)\n",
    "#     return train_x, valid_x, test_x\n",
    "\n",
    "def remove_highly_correlated_features(train_x, valid_x, test_x, threshold=0.95):\n",
    "    # Calculate correlation matrix\n",
    "    corr_matrix = np.corrcoef(train_x, rowvar=False)\n",
    "    # Select upper triangle of correlation matrix\n",
    "    upper = np.triu(corr_matrix, k=1)\n",
    "    # Find indices of feature columns with correlation greater than threshold\n",
    "    to_drop = [i for i in range(upper.shape[1]) if any(upper[:, i] > threshold)]\n",
    "    \n",
    "    # Drop features from train, validation, and test set\n",
    "    train_x = np.delete(train_x, to_drop, axis=1)\n",
    "    valid_x = np.delete(valid_x, to_drop, axis=1)\n",
    "    test_x = np.delete(test_x, to_drop, axis=1)\n",
    "    \n",
    "    return train_x, valid_x, test_x\n",
    "# pandas\n",
    "# # Remove Highly Correlated Columns\n",
    "# def remove_highly_correlated_features(train_x, valid_x, text_x, threshold=0.95):\n",
    "#     corr_matrix = train_x.corr().abs()\n",
    "#     # Select upper triangle of correlation matrix\n",
    "#     upper = corr_matrix.where(np.triu(np.ones(corr_matrix.shape), k=1).astype(bool))\n",
    "#     # Find index of feature columns with correlation greater than threshold\n",
    "#     to_drop = [column for column in upper.columns if any(upper[column] > threshold)]\n",
    "#     train_x = train_x.drop(to_drop, axis=1)\n",
    "#     valid_x = valid_x.drop(to_drop, axis=1)\n",
    "#     text_x = text_x.drop(to_drop, axis=1)\n",
    "#     return train_x, valid_x, text_x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove Low Variance Columns\n",
    "def remove_low_variance_features(train_x, valid_x, test_x, threshold=(0.8 * (1 - 0.8))):\n",
    "    sel = VarianceThreshold(threshold=threshold)\n",
    "    sel.fit(train_x)\n",
    "    train_x = train_x[:, sel.get_support(indices=True)]\n",
    "    valid_x = valid_x[:, sel.get_support(indices=True)]\n",
    "    test_x = test_x[:, sel.get_support(indices=True)]\n",
    "    return train_x, valid_x, test_x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove Random Columns\n",
    "def remove_random_features(\n",
    "    train_x: np.ndarray,\n",
    "    train_y: np.ndarray,\n",
    "    valid_x: np.ndarray,\n",
    "    test_x: np.ndarray,\n",
    "    importance=0.005,\n",
    "):\n",
    "    tree: DecisionTreeClassifier = DecisionTreeClassifier(random_state=0)\n",
    "    tree.fit(train_x, train_y)\n",
    "    importances = tree.feature_importances_\n",
    "\n",
    "    # Assume columns with very low importance are \"random\"\n",
    "    # This threshold can be adjusted based on domain knowledge\n",
    "    important_indices = [i for i, imp in enumerate(importances) if imp > importance]\n",
    "    train_x = train_x[:, important_indices]\n",
    "    valid_x = valid_x[:, important_indices]\n",
    "    test_x = test_x[:, important_indices]\n",
    "    return train_x, valid_x, test_x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def anova_filter(\n",
    "    train_x: np.ndarray,\n",
    "    train_y: np.ndarray,\n",
    "    valid_x: np.ndarray,\n",
    "    test_x: np.ndarray,\n",
    "    k: int = 50,\n",
    "):\n",
    "    # Using ANOVA F-test to select features\n",
    "    selector = SelectKBest(\n",
    "        f_classif, k=k\n",
    "    )  # Change k to select the number of features you want\n",
    "    selector.fit(train_x, train_y)\n",
    "\n",
    "    # Get F-values and p-values for each feature\n",
    "    # f_values = selector.scores_\n",
    "    # p_values = selector.pvalues_\n",
    "\n",
    "    # Selecting features (you can use a threshold or select top k features)\n",
    "    # selected_features = train_x.columns[selector.get_support()]\n",
    "\n",
    "    # Transforming train_x to include only the selected features\n",
    "    train_x = selector.transform(train_x)\n",
    "    valid_x = selector.transform(valid_x)\n",
    "    test_x = selector.transform(test_x)\n",
    "    return train_x, valid_x, test_x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "prefix = \"\"\n",
    "\n",
    "_test_x = pd.read_table(prefix + \"artificial_test.data\", sep=\" \", header=None)\n",
    "_test_x.drop(_test_x.columns[500], axis=1, inplace=True)\n",
    "_train_y = pd.read_table(prefix + \"artificial_train.labels\", header=None)\n",
    "_train_x = pd.read_table(prefix + \"artificial_train.data\", sep=\" \", header=None)\n",
    "_train_x.drop(_train_x.columns[500], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "_train_x, _train_y = shuffle(_train_x, _train_y, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_train_and_validation_data():\n",
    "    split = 400\n",
    "    train_x, valid_x = _train_x[split:].values, _train_x[:split].values\n",
    "    train_y, valid_y = _train_y[split:].values, _train_y[:split].values\n",
    "    return train_x, train_y, valid_x, valid_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1600, 500) (1600, 1) (400, 500) (400, 1)\n"
     ]
    }
   ],
   "source": [
    "train_x, train_y, valid_x, valid_y = get_train_and_validation_data()\n",
    "print(train_x.shape, train_y.shape, valid_x.shape, valid_y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_x.shape:  (1600, 490)\n"
     ]
    }
   ],
   "source": [
    "if APPLY_REMOVE_CORRELATED_FEATURES:\n",
    "    train_x, valid_x, test_x = remove_highly_correlated_features(\n",
    "        train_x, valid_x, _test_x\n",
    "    )\n",
    "    print(\"train_x.shape: \", train_x.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_x.shape:  (1600, 490)\n"
     ]
    }
   ],
   "source": [
    "if APPLY_REMOVE_LOW_VARIANCE_FEATURES:\n",
    "    train_x, valid_x, test_x = remove_low_variance_features(train_x, valid_x, test_x)\n",
    "    print(\"train_x.shape: \", train_x.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_x.shape:  (1600, 40)\n"
     ]
    }
   ],
   "source": [
    "if APPLY_REMOVE_RANDOM_FEATURES:\n",
    "    train_x, valid_x, test_x = remove_random_features(\n",
    "        train_x=train_x, train_y=train_y, valid_x=valid_x, test_x=test_x\n",
    "    )\n",
    "    print(\"train_x.shape: \", train_x.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_x.shape:  (1600, 40)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\JAN_CICHOMSKI\\STUDIA\\STUDIA_SEMESTR_7_2023_ZIMA\\auto_ml\\homeworks\\homework_1\\AUTOML_HM2\\.venv\\lib\\site-packages\\sklearn\\utils\\validation.py:1183: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n"
     ]
    }
   ],
   "source": [
    "if APPLY_ANOVA:\n",
    "    train_x, valid_x, test_x = anova_filter(\n",
    "        train_x=train_x, train_y=train_y, valid_x=valid_x, test_x=test_x, k=40\n",
    "    )\n",
    "    print(\"train_x.shape: \", train_x.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert to dataframe\n",
    "# train_x = pd.DataFrame(train_x)\n",
    "# valid_x = pd.DataFrame(valid_x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_x.shape:  (1600, 40)\n",
      "train_y.shape:  (1600, 1)\n"
     ]
    }
   ],
   "source": [
    "print(\"train_x.shape: \", train_x.shape)\n",
    "print(\"train_y.shape: \", train_y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# label = \"class\"\n",
    "# train_y = train_y.rename(columns={0: label})\n",
    "# valid_y = valid_y.rename(columns={0: label})\n",
    "# train_data = pd.concat([train_x, train_y[label]], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sanity check\n",
    "(\n",
    "    original_train_x,\n",
    "    original_train_y,\n",
    "    original_valid_x,\n",
    "    original_valid_y,\n",
    ") = get_train_and_validation_data()\n",
    "for y, original_y in zip([train_y, valid_y], [original_train_y, original_valid_y]):\n",
    "    assert y.shape == original_y.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### manual model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_y = train_y.ravel()\n",
    "# valid_y = valid_y.ravel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_classifiers = [\n",
    "    (\n",
    "        \"rf\",\n",
    "        make_pipeline(\n",
    "            StandardScaler(), RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "        ),\n",
    "    ),\n",
    "    (\"svc\", make_pipeline(StandardScaler(), SVC(random_state=42))),\n",
    "    (\"dt\", make_pipeline(StandardScaler(), DecisionTreeClassifier(random_state=42))),\n",
    "    (\n",
    "        \"elasticnet\",\n",
    "        make_pipeline(\n",
    "            StandardScaler(),\n",
    "            ElasticNet(\n",
    "                alpha=0.0001, l1_ratio=0.15, max_iter=1000, tol=1e-3, random_state=42\n",
    "            ),\n",
    "        ),\n",
    "    ),\n",
    "    (\n",
    "        \"mlp\",\n",
    "        make_pipeline(\n",
    "            StandardScaler(),\n",
    "            MLPClassifier(\n",
    "                random_state=42,\n",
    "                max_iter=1000,\n",
    "                tol=1e-3,\n",
    "                hidden_layer_sizes=(100, 300, 200, 100),\n",
    "            ),\n",
    "        ),\n",
    "    ),\n",
    "]\n",
    "\n",
    "param_distributions = {\n",
    "    \"stackingclassifier__rf__randomforestclassifier__n_estimators\": randint(50, 200),\n",
    "    \"stackingclassifier__rf__randomforestclassifier__max_depth\": randint(3, 50),\n",
    "    \"stackingclassifier__rf__randomforestclassifier__min_samples_split\": randint(2, 20),\n",
    "    \"stackingclassifier__rf__randomforestclassifier__min_samples_leaf\": randint(1, 20),\n",
    "    \"stackingclassifier__svc__svc__C\": uniform(0.1, 10),\n",
    "    \"stackingclassifier__svc__svc__gamma\": [\"scale\", \"auto\"],\n",
    "    \"stackingclassifier__dt__decisiontreeclassifier__max_depth\": randint(3, 50),\n",
    "    \"stackingclassifier__dt__decisiontreeclassifier__min_samples_split\": randint(2, 20),\n",
    "    \"stackingclassifier__dt__decisiontreeclassifier__min_samples_leaf\": randint(1, 20),\n",
    "    \"stackingclassifier__elasticnet__elasticnet__alpha\": uniform(0.0001, 1),\n",
    "    \"stackingclassifier__elasticnet__elasticnet__l1_ratio\": uniform(0, 1),\n",
    "    \"stackingclassifier__mlp__mlpclassifier__alpha\": uniform(0.0001, 1),\n",
    "    \"stackingclassifier__mlp__mlpclassifier__learning_rate_init\": uniform(0.001, 0.1),\n",
    "    \"stackingclassifier__mlp__mlpclassifier__hidden_layer_sizes\": [\n",
    "        (100, 300, 200, 100),\n",
    "        (100, 300, 200, 100, 50),\n",
    "        (100, 300, 200, 100, 50, 25),\n",
    "    ],\n",
    "    \"stackingclassifier__final_estimator__C\": uniform(0.01, 10),\n",
    "}\n",
    "\n",
    "\n",
    "stacked_ensemble_model = make_pipeline(\n",
    "    StackingClassifier(\n",
    "        estimators=base_classifiers,\n",
    "        final_estimator=LogisticRegression(),\n",
    "        cv=5,\n",
    "    )\n",
    ")\n",
    "\n",
    "random_search = RandomizedSearchCV(\n",
    "    stacked_ensemble_model,\n",
    "    param_distributions=param_distributions,\n",
    "    scoring=\"balanced_accuracy\",\n",
    "    n_iter=1,\n",
    "    cv=5,\n",
    "    verbose=4,\n",
    "    random_state=SEED,\n",
    "    n_jobs=-1,\n",
    ")\n",
    "random_search.fit(train_x, train_y)\n",
    "y_pred = random_search.predict(valid_x)\n",
    "balanced_accuracy = balanced_accuracy_score(valid_y, y_pred)\n",
    "print(f\"Model Balanced Accuracy: {balanced_accuracy}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "proba = random_search.predict_proba(test_x)\n",
    "output_path = path.join(OUTPUT_DIR_MANUAL, UNIQUE_ID, \"manual_model.txt\")\n",
    "np.savetxt(output_path, proba, delimiter=\"\\n\")\n",
    "joblib.dump(random_search, path.join(OUTPUT_DIR_MANUAL, UNIQUE_ID, \"manual_model_pred.pkl\"))\n",
    "# random_search.save(path.join(OUTPUT_DIR_MANUAL, UNIQUE_ID, \"manual_model.pkl\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Autogluon"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1600, 40) (1600, 1)\n",
      "(400, 40) (400, 1)\n"
     ]
    }
   ],
   "source": [
    "print(train_x.shape, train_y.shape)\n",
    "print(valid_x.shape, valid_y.shape )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data= np.concatenate((train_x, train_y), axis=1 )\n",
    "train_data_pd = pd.DataFrame(train_data)\n",
    "train_data_pd.rename(columns={train_data_pd.columns[-1]: \"class\"}, inplace=True)\n",
    "\n",
    "valid_data = np.concatenate((valid_x, valid_y), axis=1)\n",
    "valid_data_pd = pd.DataFrame(valid_data)\n",
    "valid_data_pd.rename(columns={valid_data_pd.columns[-1]: \"class\"}, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1600, 41) (400, 41)\n"
     ]
    }
   ],
   "source": [
    "print(train_data_pd.shape, valid_data_pd.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning: path already exists! This predictor may overwrite an existing predictor! path=\"autogluon_save\"\n",
      "Presets specified: ['best_quality']\n",
      "Stack configuration (auto_stack=True): num_stack_levels=1, num_bag_folds=8, num_bag_sets=1\n",
      "Dynamic stacking is enabled (dynamic_stacking=True). AutoGluon will try to determine whether the input data is affected by stacked overfitting and enable or disable stacking as a consequence.\n",
      "Detecting stacked overfitting by sub-fitting AutoGluon on the input data. That is, copies of AutoGluon will be sub-fit on subset(s) of the data. Then, the holdout validation data is used to detect stacked overfitting.\n",
      "Sub-fit(s) time limit is: 60 seconds.\n",
      "Starting holdout-based sub-fit for dynamic stacking. Context path is: autogluon_save/ds_sub_fit/sub_fit_ho.\n",
      "2024-01-10 17:08:54,168\tINFO util.py:159 -- Missing packages: ['ipywidgets']. Run `pip install -U ipywidgets`, then restart the notebook server for rich notebook output.\n",
      "Beginning AutoGluon training ... Time limit = 15s\n",
      "AutoGluon will save models to \"autogluon_save/ds_sub_fit/sub_fit_ho\"\n",
      "=================== System Info ===================\n",
      "AutoGluon Version:  1.0.0\n",
      "Python Version:     3.8.10\n",
      "Operating System:   Windows\n",
      "Platform Machine:   AMD64\n",
      "Platform Version:   10.0.22621\n",
      "CPU Count:          8\n",
      "Memory Avail:       7.81 GB / 15.71 GB (49.7%)\n",
      "Disk Space Avail:   58.77 GB / 357.30 GB (16.4%)\n",
      "===================================================\n",
      "Train Data Rows:    1422\n",
      "Train Data Columns: 40\n",
      "Label Column:       class\n",
      "Problem Type:       binary\n",
      "Preprocessing data ...\n",
      "Selected class <--> label mapping:  class 1 = 1, class 0 = -1\n",
      "\tNote: For your binary classification, AutoGluon arbitrarily selected which label-value represents positive (1) vs negative (-1) class.\n",
      "\tTo explicitly set the positive_class, either rename classes to 1 and 0, or specify positive_class in Predictor init.\n",
      "Using Feature Generators to preprocess the data ...\n",
      "Fitting AutoMLPipelineFeatureGenerator...\n",
      "\tAvailable Memory:                    8001.08 MB\n",
      "\tTrain Data (Original)  Memory Usage: 0.43 MB (0.0% of available memory)\n",
      "\tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n",
      "\tStage 1 Generators:\n",
      "\t\tFitting AsTypeFeatureGenerator...\n",
      "\tStage 2 Generators:\n",
      "\t\tFitting FillNaFeatureGenerator...\n",
      "\tStage 3 Generators:\n",
      "\t\tFitting IdentityFeatureGenerator...\n",
      "\tStage 4 Generators:\n",
      "\t\tFitting DropUniqueFeatureGenerator...\n",
      "\tStage 5 Generators:\n",
      "\t\tFitting DropDuplicatesFeatureGenerator...\n",
      "\tTypes of features in original data (raw dtype, special dtypes):\n",
      "\t\t('int', []) : 40 | ['0', '1', '2', '3', '4', ...]\n",
      "\tTypes of features in processed data (raw dtype, special dtypes):\n",
      "\t\t('int', []) : 40 | ['0', '1', '2', '3', '4', ...]\n",
      "\t0.3s = Fit runtime\n",
      "\t40 features in original data used to generate 40 features in processed data.\n",
      "\tTrain Data (Processed) Memory Usage: 0.43 MB (0.0% of available memory)\n",
      "Data preprocessing and feature engineering runtime = 0.37s ...\n",
      "AutoGluon will gauge predictive performance using evaluation metric: 'balanced_accuracy'\n",
      "\tTo change this, specify the eval_metric parameter of Predictor()\n",
      "User-specified model hyperparameters to be fit:\n",
      "{\n",
      "\t'NN_TORCH': {},\n",
      "\t'GBM': [{'extra_trees': True, 'ag_args': {'name_suffix': 'XT'}}, {}, 'GBMLarge'],\n",
      "\t'CAT': {},\n",
      "\t'XGB': {},\n",
      "\t'FASTAI': {},\n",
      "\t'RF': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
      "\t'XT': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
      "\t'KNN': [{'weights': 'uniform', 'ag_args': {'name_suffix': 'Unif'}}, {'weights': 'distance', 'ag_args': {'name_suffix': 'Dist'}}],\n",
      "}\n",
      "AutoGluon will fit 2 stack levels (L1 to L2) ...\n",
      "Fitting 13 L1 models ...\n",
      "Fitting model: KNeighborsUnif_BAG_L1 ... Training model for up to 9.74s of the 14.62s of remaining time.\n",
      "c:\\JAN_CICHOMSKI\\STUDIA\\STUDIA_SEMESTR_7_2023_ZIMA\\auto_ml\\homeworks\\homework_1\\AUTOML_HM2\\.venv\\lib\\site-packages\\joblib\\externals\\loky\\backend\\context.py:136: UserWarning: Could not find the number of physical cores for the following reason:\n",
      "found 0 physical cores < 1\n",
      "Returning the number of logical cores instead. You can silence this warning by setting LOKY_MAX_CPU_COUNT to the number of cores you want to use.\n",
      "  File \"c:\\JAN_CICHOMSKI\\STUDIA\\STUDIA_SEMESTR_7_2023_ZIMA\\auto_ml\\homeworks\\homework_1\\AUTOML_HM2\\.venv\\lib\\site-packages\\joblib\\externals\\loky\\backend\\context.py\", line 282, in _count_physical_cores\n",
      "    raise ValueError(f\"found {cpu_count_physical} physical cores < 1\")\n",
      "\t0.8151\t = Validation score   (balanced_accuracy)\n",
      "\t0.01s\t = Training   runtime\n",
      "\t0.27s\t = Validation runtime\n",
      "Fitting model: KNeighborsDist_BAG_L1 ... Training model for up to 9.45s of the 14.32s of remaining time.\n",
      "\t0.8151\t = Validation score   (balanced_accuracy)\n",
      "\t0.0s\t = Training   runtime\n",
      "\t0.04s\t = Validation runtime\n",
      "Fitting model: LightGBMXT_BAG_L1 ... Training model for up to 9.37s of the 14.24s of remaining time.\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (8 workers, per: cpus=1, gpus=0, memory=0.13%)\n",
      "\t0.6184\t = Validation score   (balanced_accuracy)\n",
      "\t0.59s\t = Training   runtime\n",
      "\t0.02s\t = Validation runtime\n",
      "Fitting model: WeightedEnsemble_L2 ... Training model for up to 14.63s of the 1.62s of remaining time.\n",
      "\tEnsemble Weights: {'KNeighborsUnif_BAG_L1': 1.0}\n",
      "\t0.8151\t = Validation score   (balanced_accuracy)\n",
      "\t0.45s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "Fitting 11 L2 models ...\n",
      "Fitting model: LightGBMXT_BAG_L2 ... Training model for up to 1.15s of the 1.12s of remaining time.\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (8 workers, per: cpus=1, gpus=0, memory=0.16%)\n",
      "\t0.8235\t = Validation score   (balanced_accuracy)\n",
      "\t1.84s\t = Training   runtime\n",
      "\t0.02s\t = Validation runtime\n",
      "Fitting model: WeightedEnsemble_L3 ... Training model for up to 14.63s of the -5.9s of remaining time.\n",
      "\tEnsemble Weights: {'LightGBMXT_BAG_L2': 1.0}\n",
      "\t0.8235\t = Validation score   (balanced_accuracy)\n",
      "\t0.4s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "AutoGluon training complete, total runtime = 21.4s ... Best model: \"WeightedEnsemble_L3\"\n",
      "TabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"autogluon_save/ds_sub_fit/sub_fit_ho\")\n",
      "Leaderboard on holdout data from dynamic stacking:\n",
      "                   model  holdout_score  score_val        eval_metric  pred_time_test  pred_time_val  fit_time  pred_time_test_marginal  pred_time_val_marginal  fit_time_marginal  stack_level  can_infer  fit_order\n",
      "0  KNeighborsUnif_BAG_L1       0.786517   0.815096  balanced_accuracy        0.006010       0.267870  0.008013                 0.006010                0.267870           0.008013            1       True          1\n",
      "1  KNeighborsDist_BAG_L1       0.786517   0.815096  balanced_accuracy        0.009600       0.043610  0.000000                 0.009600                0.043610           0.000000            1       True          2\n",
      "2    WeightedEnsemble_L2       0.786517   0.815096  balanced_accuracy        0.010378       0.271917  0.453051                 0.004369                0.004046           0.445038            2       True          4\n",
      "3      LightGBMXT_BAG_L2       0.786517   0.823529  balanced_accuracy        0.057986       0.352796  2.441601                 0.023875                0.017618           1.841902            2       True          5\n",
      "4    WeightedEnsemble_L3       0.786517   0.823529  balanced_accuracy        0.061136       0.355887  2.844925                 0.003149                0.003092           0.403324            3       True          6\n",
      "5      LightGBMXT_BAG_L1       0.668539   0.618401  balanced_accuracy        0.018502       0.023696  0.591686                 0.018502                0.023696           0.591686            1       True          3\n",
      "Stacked overfitting occurred: True.\n",
      "Spend 22 seconds for the sub-fit(s) during dynamic stacking.\n",
      "Time left for full fit of AutoGluon: 38 seconds.\n",
      "Starting full fit now with num_stack_levels 0.\n",
      "Beginning AutoGluon training ... Time limit = 38s\n",
      "AutoGluon will save models to \"autogluon_save\"\n",
      "=================== System Info ===================\n",
      "AutoGluon Version:  1.0.0\n",
      "Python Version:     3.8.10\n",
      "Operating System:   Windows\n",
      "Platform Machine:   AMD64\n",
      "Platform Version:   10.0.22621\n",
      "CPU Count:          8\n",
      "Memory Avail:       7.60 GB / 15.71 GB (48.4%)\n",
      "Disk Space Avail:   58.77 GB / 357.30 GB (16.4%)\n",
      "===================================================\n",
      "Train Data Rows:    1600\n",
      "Train Data Columns: 40\n",
      "Label Column:       class\n",
      "Problem Type:       binary\n",
      "Preprocessing data ...\n",
      "Selected class <--> label mapping:  class 1 = 1, class 0 = -1\n",
      "\tNote: For your binary classification, AutoGluon arbitrarily selected which label-value represents positive (1) vs negative (-1) class.\n",
      "\tTo explicitly set the positive_class, either rename classes to 1 and 0, or specify positive_class in Predictor init.\n",
      "Using Feature Generators to preprocess the data ...\n",
      "Fitting AutoMLPipelineFeatureGenerator...\n",
      "\tAvailable Memory:                    7789.80 MB\n",
      "\tTrain Data (Original)  Memory Usage: 0.49 MB (0.0% of available memory)\n",
      "\tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n",
      "\tStage 1 Generators:\n",
      "\t\tFitting AsTypeFeatureGenerator...\n",
      "\tStage 2 Generators:\n",
      "\t\tFitting FillNaFeatureGenerator...\n",
      "\tStage 3 Generators:\n",
      "\t\tFitting IdentityFeatureGenerator...\n",
      "\tStage 4 Generators:\n",
      "\t\tFitting DropUniqueFeatureGenerator...\n",
      "\tStage 5 Generators:\n",
      "\t\tFitting DropDuplicatesFeatureGenerator...\n",
      "\tTypes of features in original data (raw dtype, special dtypes):\n",
      "\t\t('int', []) : 40 | ['0', '1', '2', '3', '4', ...]\n",
      "\tTypes of features in processed data (raw dtype, special dtypes):\n",
      "\t\t('int', []) : 40 | ['0', '1', '2', '3', '4', ...]\n",
      "\t0.1s = Fit runtime\n",
      "\t40 features in original data used to generate 40 features in processed data.\n",
      "\tTrain Data (Processed) Memory Usage: 0.49 MB (0.0% of available memory)\n",
      "Data preprocessing and feature engineering runtime = 0.09s ...\n",
      "AutoGluon will gauge predictive performance using evaluation metric: 'balanced_accuracy'\n",
      "\tTo change this, specify the eval_metric parameter of Predictor()\n",
      "User-specified model hyperparameters to be fit:\n",
      "{\n",
      "\t'NN_TORCH': {},\n",
      "\t'GBM': [{'extra_trees': True, 'ag_args': {'name_suffix': 'XT'}}, {}, 'GBMLarge'],\n",
      "\t'CAT': {},\n",
      "\t'XGB': {},\n",
      "\t'FASTAI': {},\n",
      "\t'RF': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
      "\t'XT': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
      "\t'KNN': [{'weights': 'uniform', 'ag_args': {'name_suffix': 'Unif'}}, {'weights': 'distance', 'ag_args': {'name_suffix': 'Dist'}}],\n",
      "}\n",
      "Fitting 13 L1 models ...\n",
      "Fitting model: KNeighborsUnif_BAG_L1 ... Training model for up to 37.91s of the 37.91s of remaining time.\n",
      "\t0.8157\t = Validation score   (balanced_accuracy)\n",
      "\t0.01s\t = Training   runtime\n",
      "\t0.03s\t = Validation runtime\n",
      "Fitting model: KNeighborsDist_BAG_L1 ... Training model for up to 37.82s of the 37.82s of remaining time.\n",
      "\t0.8157\t = Validation score   (balanced_accuracy)\n",
      "\t0.01s\t = Training   runtime\n",
      "\t0.03s\t = Validation runtime\n",
      "Fitting model: LightGBMXT_BAG_L1 ... Training model for up to 37.76s of the 37.76s of remaining time.\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (8 workers, per: cpus=1, gpus=0, memory=0.13%)\n",
      "\t0.7957\t = Validation score   (balanced_accuracy)\n",
      "\t3.82s\t = Training   runtime\n",
      "\t0.02s\t = Validation runtime\n",
      "Fitting model: LightGBM_BAG_L1 ... Training model for up to 29.43s of the 29.43s of remaining time.\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (8 workers, per: cpus=1, gpus=0, memory=0.14%)\n",
      "\t0.8518\t = Validation score   (balanced_accuracy)\n",
      "\t5.02s\t = Training   runtime\n",
      "\t0.04s\t = Validation runtime\n",
      "Fitting model: RandomForestGini_BAG_L1 ... Training model for up to 19.7s of the 19.7s of remaining time.\n",
      "\t0.7926\t = Validation score   (balanced_accuracy)\n",
      "\t1.1s\t = Training   runtime\n",
      "\t0.18s\t = Validation runtime\n",
      "Fitting model: RandomForestEntr_BAG_L1 ... Training model for up to 18.37s of the 18.37s of remaining time.\n",
      "\t0.8039\t = Validation score   (balanced_accuracy)\n",
      "\t0.99s\t = Training   runtime\n",
      "\t0.19s\t = Validation runtime\n",
      "Fitting model: CatBoost_BAG_L1 ... Training model for up to 17.13s of the 17.13s of remaining time.\n",
      "\tFitting 8 child models (S1F1 - S1F8) | Fitting with ParallelLocalFoldFittingStrategy (8 workers, per: cpus=1, gpus=0, memory=0.23%)\n",
      "\t0.86\t = Validation score   (balanced_accuracy)\n",
      "\t14.28s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "Fitting model: WeightedEnsemble_L2 ... Training model for up to 37.91s of the -1.63s of remaining time.\n",
      "\tEnsemble Weights: {'CatBoost_BAG_L1': 1.0}\n",
      "\t0.86\t = Validation score   (balanced_accuracy)\n",
      "\t0.68s\t = Training   runtime\n",
      "\t0.02s\t = Validation runtime\n",
      "AutoGluon training complete, total runtime = 40.35s ... Best model: \"WeightedEnsemble_L2\"\n",
      "TabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"autogluon_save\")\n"
     ]
    }
   ],
   "source": [
    "save_path = path.join(OUTPUT_DIR_AUTOGLUON, UNIQUE_ID)\n",
    "predictor = TabularPredictor(\n",
    "    label=\"class\", path=save_path, eval_metric=\"balanced_accuracy\", problem_type=\"binary\"\n",
    ").fit(\n",
    "    train_data_pd,\n",
    "    time_limit=TRAIN_TIME_LIMIT_AUTOGLUON,\n",
    "    presets=\"best_quality\",\n",
    "    hyperparameters=\"default\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>model</th>\n",
       "      <th>score_val</th>\n",
       "      <th>eval_metric</th>\n",
       "      <th>pred_time_val</th>\n",
       "      <th>fit_time</th>\n",
       "      <th>pred_time_val_marginal</th>\n",
       "      <th>fit_time_marginal</th>\n",
       "      <th>stack_level</th>\n",
       "      <th>can_infer</th>\n",
       "      <th>fit_order</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>CatBoost_BAG_L1</td>\n",
       "      <td>0.859984</td>\n",
       "      <td>balanced_accuracy</td>\n",
       "      <td>0.000551</td>\n",
       "      <td>14.284969</td>\n",
       "      <td>0.000551</td>\n",
       "      <td>14.284969</td>\n",
       "      <td>1</td>\n",
       "      <td>True</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>WeightedEnsemble_L2</td>\n",
       "      <td>0.859984</td>\n",
       "      <td>balanced_accuracy</td>\n",
       "      <td>0.016125</td>\n",
       "      <td>14.967341</td>\n",
       "      <td>0.015574</td>\n",
       "      <td>0.682373</td>\n",
       "      <td>2</td>\n",
       "      <td>True</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>LightGBM_BAG_L1</td>\n",
       "      <td>0.851847</td>\n",
       "      <td>balanced_accuracy</td>\n",
       "      <td>0.037408</td>\n",
       "      <td>5.016441</td>\n",
       "      <td>0.037408</td>\n",
       "      <td>5.016441</td>\n",
       "      <td>1</td>\n",
       "      <td>True</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>KNeighborsDist_BAG_L1</td>\n",
       "      <td>0.815672</td>\n",
       "      <td>balanced_accuracy</td>\n",
       "      <td>0.032716</td>\n",
       "      <td>0.006662</td>\n",
       "      <td>0.032716</td>\n",
       "      <td>0.006662</td>\n",
       "      <td>1</td>\n",
       "      <td>True</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>KNeighborsUnif_BAG_L1</td>\n",
       "      <td>0.815672</td>\n",
       "      <td>balanced_accuracy</td>\n",
       "      <td>0.033736</td>\n",
       "      <td>0.009012</td>\n",
       "      <td>0.033736</td>\n",
       "      <td>0.009012</td>\n",
       "      <td>1</td>\n",
       "      <td>True</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>RandomForestEntr_BAG_L1</td>\n",
       "      <td>0.803855</td>\n",
       "      <td>balanced_accuracy</td>\n",
       "      <td>0.185975</td>\n",
       "      <td>0.990400</td>\n",
       "      <td>0.185975</td>\n",
       "      <td>0.990400</td>\n",
       "      <td>1</td>\n",
       "      <td>True</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>LightGBMXT_BAG_L1</td>\n",
       "      <td>0.795690</td>\n",
       "      <td>balanced_accuracy</td>\n",
       "      <td>0.019990</td>\n",
       "      <td>3.819994</td>\n",
       "      <td>0.019990</td>\n",
       "      <td>3.819994</td>\n",
       "      <td>1</td>\n",
       "      <td>True</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>RandomForestGini_BAG_L1</td>\n",
       "      <td>0.792605</td>\n",
       "      <td>balanced_accuracy</td>\n",
       "      <td>0.180362</td>\n",
       "      <td>1.098191</td>\n",
       "      <td>0.180362</td>\n",
       "      <td>1.098191</td>\n",
       "      <td>1</td>\n",
       "      <td>True</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                     model  score_val        eval_metric  pred_time_val  \\\n",
       "0          CatBoost_BAG_L1   0.859984  balanced_accuracy       0.000551   \n",
       "1      WeightedEnsemble_L2   0.859984  balanced_accuracy       0.016125   \n",
       "2          LightGBM_BAG_L1   0.851847  balanced_accuracy       0.037408   \n",
       "3    KNeighborsDist_BAG_L1   0.815672  balanced_accuracy       0.032716   \n",
       "4    KNeighborsUnif_BAG_L1   0.815672  balanced_accuracy       0.033736   \n",
       "5  RandomForestEntr_BAG_L1   0.803855  balanced_accuracy       0.185975   \n",
       "6        LightGBMXT_BAG_L1   0.795690  balanced_accuracy       0.019990   \n",
       "7  RandomForestGini_BAG_L1   0.792605  balanced_accuracy       0.180362   \n",
       "\n",
       "    fit_time  pred_time_val_marginal  fit_time_marginal  stack_level  \\\n",
       "0  14.284969                0.000551          14.284969            1   \n",
       "1  14.967341                0.015574           0.682373            2   \n",
       "2   5.016441                0.037408           5.016441            1   \n",
       "3   0.006662                0.032716           0.006662            1   \n",
       "4   0.009012                0.033736           0.009012            1   \n",
       "5   0.990400                0.185975           0.990400            1   \n",
       "6   3.819994                0.019990           3.819994            1   \n",
       "7   1.098191                0.180362           1.098191            1   \n",
       "\n",
       "   can_infer  fit_order  \n",
       "0       True          7  \n",
       "1       True          8  \n",
       "2       True          4  \n",
       "3       True          2  \n",
       "4       True          1  \n",
       "5       True          6  \n",
       "6       True          3  \n",
       "7       True          5  "
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictor.leaderboard()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'balanced_accuracy': 0.8550423845365207,\n",
       " 'accuracy': 0.855,\n",
       " 'mcc': 0.7100137552699213,\n",
       " 'roc_auc': 0.9167062589082544,\n",
       " 'f1': 0.8535353535353536,\n",
       " 'precision': 0.8492462311557789,\n",
       " 'recall': 0.8578680203045685}"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictor.evaluate(valid_data_pd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "proba = predictor.predict_proba(pd.DataFrame(test_x))\n",
    "output_path = path.join(OUTPUT_DIR_AUTOGLUON, UNIQUE_ID, \"manual_model_pred.txt\")\n",
    "np.savetxt(output_path, proba, delimiter=\"\\n\")\n",
    "# predictor.save(path.join(OUTPUT_DIR_AUTOGLUON, UNIQUE_ID, \"manual_model.pkl\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MLJar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AutoML directory: output\\mljar\\20240110_170735\n",
      "The task is binary_classification with evaluation metric f1\n",
      "AutoML will use algorithms: ['Decision Tree', 'Linear', 'Random Forest', 'Extra Trees', 'LightGBM', 'Xgboost', 'CatBoost', 'Neural Network', 'Nearest Neighbors']\n",
      "AutoML will stack models\n",
      "AutoML will ensemble available models\n",
      "AutoML steps: ['adjust_validation', 'simple_algorithms', 'default_algorithms', 'not_so_random', 'golden_features', 'kmeans_features', 'insert_random_feature', 'features_selection', 'hill_climbing_1', 'hill_climbing_2', 'boost_on_errors', 'ensemble', 'stack', 'ensemble_stacked']\n",
      "* Step adjust_validation will try to check up to 1 model\n",
      "1_DecisionTree f1 0.757764 trained in 4.76 seconds\n",
      "Disable stacking for split validation\n",
      "* Step simple_algorithms will try to check up to 3 models\n",
      "2_DecisionTree f1 0.618182 trained in 3.4 seconds\n",
      "3_DecisionTree f1 0.618182 trained in 3.15 seconds\n",
      "4_Linear f1 0.591195 trained in 4.4 seconds\n",
      "Skip default_algorithms because of the time limit.\n",
      "* Step not_so_random will try to check up to 63 models\n",
      "14_LightGBM f1 0.824242 trained in 3.49 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\JAN_CICHOMSKI\\STUDIA\\STUDIA_SEMESTR_7_2023_ZIMA\\auto_ml\\homeworks\\homework_1\\AUTOML_HM2\\.venv\\lib\\site-packages\\xgboost\\training.py:38: UserWarning: `feval` is deprecated, use `custom_metric` instead.  They have different behavior when custom objective is also used.See https://xgboost.readthedocs.io/en/latest/tutorials/custom_metric_obj.html for details on the `custom_metric`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5_Xgboost f1 0.797546 trained in 4.86 seconds\n",
      "23_CatBoost f1 0.851852 trained in 4.44 seconds\n",
      "Skip golden_features because of the time limit.\n",
      "* Step kmeans_features will try to check up to 3 models\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\JAN_CICHOMSKI\\STUDIA\\STUDIA_SEMESTR_7_2023_ZIMA\\auto_ml\\homeworks\\homework_1\\AUTOML_HM2\\.venv\\lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1934: FutureWarning: The default value of `n_init` will change from 3 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "14_LightGBM_KMeansFeatures f1 0.817073 trained in 4.69 seconds\n",
      "Not enough time to perform features selection. Skip\n",
      "Time needed for features selection ~ 10.0 seconds\n",
      "Please increase total_time_limit to at least (156 seconds) to have features selection\n",
      "Skip insert_random_feature because no parameters were generated.\n",
      "Skip features_selection because no parameters were generated.\n",
      "* Step hill_climbing_1 will try to check up to 10 models\n",
      "24_CatBoost f1 0.829268 trained in 4.45 seconds\n",
      "* Step hill_climbing_2 will try to check up to 13 models\n",
      "25_CatBoost f1 0.851852 trained in 4.07 seconds\n",
      "* Step ensemble will try to check up to 1 model\n",
      "Ensemble f1 0.851852 trained in 2.88 seconds\n",
      "AutoML fit time: 67.57 seconds\n",
      "AutoML best model: 23_CatBoost\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-1 {color: black;}#sk-container-id-1 pre{padding: 0;}#sk-container-id-1 div.sk-toggleable {background-color: white;}#sk-container-id-1 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-1 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-1 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-1 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-1 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-1 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-1 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-1 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-1 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-1 div.sk-item {position: relative;z-index: 1;}#sk-container-id-1 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-1 div.sk-item::before, #sk-container-id-1 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-1 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-1 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-1 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-1 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-1 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-1 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-1 div.sk-label-container {text-align: center;}#sk-container-id-1 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-1 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-1\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>AutoML(eval_metric=&#x27;f1&#x27;, ml_task=&#x27;binary_classification&#x27;, mode=&#x27;Compete&#x27;,\n",
       "       random_state=42, results_path=&#x27;output\\\\mljar\\\\20240110_170735&#x27;,\n",
       "       total_time_limit=60)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-1\" type=\"checkbox\" checked><label for=\"sk-estimator-id-1\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">AutoML</label><div class=\"sk-toggleable__content\"><pre>AutoML(eval_metric=&#x27;f1&#x27;, ml_task=&#x27;binary_classification&#x27;, mode=&#x27;Compete&#x27;,\n",
       "       random_state=42, results_path=&#x27;output\\\\mljar\\\\20240110_170735&#x27;,\n",
       "       total_time_limit=60)</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "AutoML(eval_metric='f1', ml_task='binary_classification', mode='Compete',\n",
       "       random_state=42, results_path='output\\\\mljar\\\\20240110_170735',\n",
       "       total_time_limit=60)"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "automl = AutoML(\n",
    "    mode=\"Compete\",\n",
    "    ml_task=\"binary_classification\",\n",
    "    total_time_limit=TRAIN_TIME_LIMIT_MLJAR,\n",
    "    eval_metric=\"f1\",\n",
    "    random_state=SEED,\n",
    "    results_path=path.join(OUTPUT_DIR_MLJAR, UNIQUE_ID),\n",
    ")\n",
    "\n",
    "automl.fit(train_x, train_y.ravel())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8248855992598334"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictions = automl.predict(valid_x)\n",
    "balanced_accuracy_score(valid_y, predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "proba = automl.predict_proba(test_x)\n",
    "output_path = path.join(OUTPUT_DIR_MLJAR, UNIQUE_ID, \"manual_model_proba.txt\")\n",
    "np.savetxt(output_path, proba, delimiter=\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Auto SKLearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install auto-sklearn\n",
    "# !pip install ydata-profiling\n",
    "from autosklearn.classification import AutoSklearnClassifier\n",
    "from autosklearn.experimental.askl2 import AutoSklearn2Classifier\n",
    "from autosklearn.metrics import balanced_accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "settings = {\n",
    "    \"time_left_for_this_task\": TRAIN_TIME_LIMIT_AUTO_SKLEARN,\n",
    "    \"seed\": SEED,\n",
    "    \"metric\": balanced_accuracy,\n",
    "    \"n_jobs\": -1,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "askl2 = AutoSklearn2Classifier(**settings)\n",
    "askl2.fit(train_x, train_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "leaderboard = askl2.leaderboard(sort_by=\"model_id\", ensemble_only=True)\n",
    "print(leaderboard)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = askl2.predict(valid_x)\n",
    "balanced_accuracy_score(valid_y, predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "proba = askl2.predict_proba(test_x)\n",
    "output_path = path.join(OUTPUT_DIR_AUTO_SKLEARN, UNIQUE_ID, \"manual_model.txt\")\n",
    "np.savetxt(output_path, proba, delimiter=\"\\n\")\n",
    "askl2.save(path.join(OUTPUT_DIR_AUTO_SKLEARN, UNIQUE_ID, \"manual_model.pkl\"))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
