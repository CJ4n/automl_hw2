{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install mljar-supervised"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\JAN_CICHOMSKI\\STUDIA\\STUDIA_SEMESTR_7_2023_ZIMA\\auto_ml\\homeworks\\homework_1\\AUTOML_HM2\\.venv\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import time\n",
    "from os import path\n",
    "\n",
    "import joblib\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from autogluon.tabular import TabularPredictor\n",
    "from scipy.stats import randint, uniform\n",
    "from sklearn.ensemble import RandomForestClassifier, StackingClassifier\n",
    "from sklearn.feature_selection import SelectKBest, VarianceThreshold, f_classif\n",
    "from sklearn.linear_model import ElasticNet, LogisticRegression\n",
    "from sklearn.metrics import balanced_accuracy_score, mutual_info_score\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.utils import shuffle\n",
    "from supervised.automl import AutoML  # mljar-supervised"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Constants\n",
    "\n",
    "SEED = 42\n",
    "N_JOBS = -1\n",
    "RANDOM_SEARCH_N_ITER = 50\n",
    "TRAIN_TIME_LIMIT_AUTOGLUON = 60 * 30\n",
    "TRAIN_TIME_LIMIT_MLJAR = 60 * 30\n",
    "TRAIN_TIME_LIMIT_AUTO_SKLEARN = 60 * 30\n",
    "OUTPUT_DIR_MANUAL = path.join(\"output\", \"manual\")\n",
    "OUTPUT_DIR_AUTOGLUON = path.join(\"output\", \"autogluon\")\n",
    "OUTPUT_DIR_MLJAR = path.join(\"output\", \"mljar\")\n",
    "OUTPUT_DIR_AUTO_SKLEARN = path.join(\"output\", \"auto_sklearn\")\n",
    "UNIQUE_ID = time.strftime(\"%Y%m%d_%H%M%S\")\n",
    "APPLY_REMOVE_LOW_VARIANCE_FEATURES = True\n",
    "APPLY_REMOVE_CORRELATED_FEATURES = True\n",
    "APPLY_REMOVE_RANDOM_FEATURES = False\n",
    "APPLY_ANOVA = True\n",
    "ANOVE_FEATURES = 15"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating output directory output\\manual\\20240112_090129\n",
      "Creating output directory output\\autogluon\\20240112_090129\n",
      "Creating output directory output\\mljar\\20240112_090129\n",
      "Creating output directory output\\auto_sklearn\\20240112_090129\n"
     ]
    }
   ],
   "source": [
    "# prepare output directories\n",
    "for output_dir in [\n",
    "    OUTPUT_DIR_MANUAL,\n",
    "    OUTPUT_DIR_AUTOGLUON,\n",
    "    OUTPUT_DIR_MLJAR,\n",
    "    OUTPUT_DIR_AUTO_SKLEARN,\n",
    "]:\n",
    "    if not path.exists(path.join(output_dir, UNIQUE_ID)):\n",
    "        print(f\"Creating output directory {path.join(output_dir, UNIQUE_ID)}\")\n",
    "        os.makedirs(path.join(output_dir, UNIQUE_ID))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove Highly Correlated Columns\n",
    "# def remove_highly_correlated_features(train_x, valid_x, test_x, threshold=0.95):\n",
    "#     corr_matrix = np.corrcoef(train_x, rowvar=False)\n",
    "#     upper = np.triu(np.ones(corr_matrix.shape), k=1).astype(bool)\n",
    "#     to_drop = np.where(np.abs(corr_matrix[upper]) > threshold)[0]\n",
    "#     print(to_drop)\n",
    "#     train_x = np.delete(train_x, to_drop, axis=1)\n",
    "#     valid_x = np.delete(valid_x, to_drop, axis=1)\n",
    "#     test_x = np.delete(test_x, to_drop, axis=1)\n",
    "#     return train_x, valid_x, test_x\n",
    "\n",
    "\n",
    "def remove_highly_correlated_features(train_x, valid_x, test_x, threshold=0.95):\n",
    "    # Calculate correlation matrix\n",
    "    corr_matrix = np.corrcoef(train_x, rowvar=False)\n",
    "    # Select upper triangle of correlation matrix\n",
    "    upper = np.triu(corr_matrix, k=1)\n",
    "    # Find indices of feature columns with correlation greater than threshold\n",
    "    to_drop = [i for i in range(upper.shape[1]) if any(upper[:, i] > threshold)]\n",
    "\n",
    "    # Drop features from train, validation, and test set\n",
    "    train_x = np.delete(train_x, to_drop, axis=1)\n",
    "    valid_x = np.delete(valid_x, to_drop, axis=1)\n",
    "    test_x = np.delete(test_x, to_drop, axis=1)\n",
    "\n",
    "    return train_x, valid_x, test_x\n",
    "\n",
    "\n",
    "# pandas\n",
    "# # Remove Highly Correlated Columns\n",
    "# def remove_highly_correlated_features(train_x, valid_x, text_x, threshold=0.95):\n",
    "#     corr_matrix = train_x.corr().abs()\n",
    "#     # Select upper triangle of correlation matrix\n",
    "#     upper = corr_matrix.where(np.triu(np.ones(corr_matrix.shape), k=1).astype(bool))\n",
    "#     # Find index of feature columns with correlation greater than threshold\n",
    "#     to_drop = [column for column in upper.columns if any(upper[column] > threshold)]\n",
    "#     train_x = train_x.drop(to_drop, axis=1)\n",
    "#     valid_x = valid_x.drop(to_drop, axis=1)\n",
    "#     text_x = text_x.drop(to_drop, axis=1)\n",
    "#     return train_x, valid_x, text_x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove Low Variance Columns\n",
    "def remove_low_variance_features(train_x, valid_x, test_x, threshold=(0.8 * (1 - 0.8))):\n",
    "    sel = VarianceThreshold(threshold=threshold)\n",
    "    sel.fit(train_x)\n",
    "    train_x = train_x[:, sel.get_support(indices=True)]\n",
    "    valid_x = valid_x[:, sel.get_support(indices=True)]\n",
    "    test_x = test_x[:, sel.get_support(indices=True)]\n",
    "    return train_x, valid_x, test_x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove Random Columns\n",
    "def remove_random_features(\n",
    "    train_x: np.ndarray,\n",
    "    train_y: np.ndarray,\n",
    "    valid_x: np.ndarray,\n",
    "    test_x: np.ndarray,\n",
    "    importance=0.005,\n",
    "):\n",
    "    tree: DecisionTreeClassifier = DecisionTreeClassifier(random_state=0)\n",
    "    tree.fit(train_x, train_y)\n",
    "    importances = tree.feature_importances_\n",
    "\n",
    "    # Assume columns with very low importance are \"random\"\n",
    "    # This threshold can be adjusted based on domain knowledge\n",
    "    important_indices = [i for i, imp in enumerate(importances) if imp > importance]\n",
    "    train_x = train_x[:, important_indices]\n",
    "    valid_x = valid_x[:, important_indices]\n",
    "    test_x = test_x[:, important_indices]\n",
    "    return train_x, valid_x, test_x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def anova_filter(\n",
    "    train_x: np.ndarray,\n",
    "    train_y: np.ndarray,\n",
    "    valid_x: np.ndarray,\n",
    "    test_x: np.ndarray,\n",
    "    k: int = 50,\n",
    "):\n",
    "    # Using ANOVA F-test to select features\n",
    "    selector = SelectKBest(\n",
    "        f_classif, k=k\n",
    "    )  # Change k to select the number of features you want\n",
    "    selector.fit(train_x, train_y)\n",
    "\n",
    "    # Get F-values and p-values for each feature\n",
    "    # f_values = selector.scores_\n",
    "    # p_values = selector.pvalues_\n",
    "\n",
    "    # Selecting features (you can use a threshold or select top k features)\n",
    "    # selected_features = train_x.columns[selector.get_support()]\n",
    "\n",
    "    # Transforming train_x to include only the selected features\n",
    "    train_x = selector.transform(train_x)\n",
    "    valid_x = selector.transform(valid_x)\n",
    "    test_x = selector.transform(test_x)\n",
    "    return train_x, valid_x, test_x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "prefix = \"\"\n",
    "\n",
    "_test_x = pd.read_table(prefix + \"artificial_test.data\", sep=\" \", header=None)\n",
    "_test_x.drop(_test_x.columns[500], axis=1, inplace=True)\n",
    "_train_y = pd.read_table(prefix + \"artificial_train.labels\", header=None)\n",
    "_train_x = pd.read_table(prefix + \"artificial_train.data\", sep=\" \", header=None)\n",
    "_train_x.drop(_train_x.columns[500], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "_train_x, _train_y = shuffle(_train_x, _train_y, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_train_and_validation_data():\n",
    "    split = 400\n",
    "    train_x, valid_x = _train_x[split:].values, _train_x[:split].values\n",
    "    train_y, valid_y = _train_y[split:].values, _train_y[:split].values\n",
    "    return train_x, train_y, valid_x, valid_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1600, 500) (1600, 1) (400, 500) (400, 1)\n"
     ]
    }
   ],
   "source": [
    "train_x, train_y, valid_x, valid_y = get_train_and_validation_data()\n",
    "train_y = train_y.reshape(-1, 1)\n",
    "valid_y = valid_y.reshape(-1, 1)\n",
    "print(train_x.shape, train_y.shape, valid_x.shape, valid_y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_x.shape:  (1600, 490)\n"
     ]
    }
   ],
   "source": [
    "if APPLY_REMOVE_CORRELATED_FEATURES:\n",
    "    train_x, valid_x, test_x = remove_highly_correlated_features(\n",
    "        train_x, valid_x, _test_x\n",
    "    )\n",
    "    print(\"train_x.shape: \", train_x.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_x.shape:  (1600, 490)\n"
     ]
    }
   ],
   "source": [
    "if APPLY_REMOVE_LOW_VARIANCE_FEATURES:\n",
    "    train_x, valid_x, test_x = remove_low_variance_features(train_x, valid_x, test_x)\n",
    "    print(\"train_x.shape: \", train_x.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "if APPLY_REMOVE_RANDOM_FEATURES:\n",
    "    train_x, valid_x, test_x = remove_random_features(\n",
    "        train_x=train_x, train_y=train_y, valid_x=valid_x, test_x=test_x\n",
    "    )\n",
    "    print(\"train_x.shape: \", train_x.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_x.shape:  (1600, 15)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\JAN_CICHOMSKI\\STUDIA\\STUDIA_SEMESTR_7_2023_ZIMA\\auto_ml\\homeworks\\homework_1\\AUTOML_HM2\\.venv\\lib\\site-packages\\sklearn\\utils\\validation.py:1183: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n"
     ]
    }
   ],
   "source": [
    "if APPLY_ANOVA:\n",
    "    train_x, valid_x, test_x = anova_filter(\n",
    "        train_x=train_x,\n",
    "        train_y=train_y,\n",
    "        valid_x=valid_x,\n",
    "        test_x=test_x,\n",
    "        k=ANOVE_FEATURES,\n",
    "    )\n",
    "    print(\"train_x.shape: \", train_x.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert to dataframe\n",
    "# train_x = pd.DataFrame(train_x)\n",
    "# valid_x = pd.DataFrame(valid_x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_x.shape:  (1600, 15)\n",
      "train_y.shape:  (1600, 1)\n"
     ]
    }
   ],
   "source": [
    "print(\"train_x.shape: \", train_x.shape)\n",
    "print(\"train_y.shape: \", train_y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# label = \"class\"\n",
    "# train_y = train_y.rename(columns={0: label})\n",
    "# valid_y = valid_y.rename(columns={0: label})\n",
    "# train_data = pd.concat([train_x, train_y[label]], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sanity check\n",
    "(\n",
    "    original_train_x,\n",
    "    original_train_y,\n",
    "    original_valid_x,\n",
    "    original_valid_y,\n",
    ") = get_train_and_validation_data()\n",
    "for y, original_y in zip([train_y, valid_y], [original_train_y, original_valid_y]):\n",
    "    assert y.shape == original_y.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### manual model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_y = train_y.ravel()\n",
    "# valid_y = valid_y.ravel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "Model Balanced Accuracy: 0.795328949013528\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import GradientBoostingClassifier, VotingClassifier\n",
    "from sklearn.metrics import balanced_accuracy_score\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "\n",
    "base_classifiers_1 = [\n",
    "    (\n",
    "        \"rf\",\n",
    "        make_pipeline(\n",
    "            StandardScaler(),\n",
    "            RandomForestClassifier(n_estimators=100, random_state=SEED),\n",
    "        ),\n",
    "    ),\n",
    "    # (\"svc\", make_pipeline(StandardScaler(), SVC(random_state=SEED))),\n",
    "    (\"dt\", make_pipeline(StandardScaler(), DecisionTreeClassifier(random_state=SEED))),\n",
    "    # (\n",
    "    #     \"elasticnet\",\n",
    "    #     make_pipeline(\n",
    "    #         StandardScaler(),\n",
    "    #         ElasticNet(\n",
    "    #             alpha=0.0001, l1_ratio=0.15, max_iter=1000, tol=1e-3, random_state=SEED\n",
    "    #         ),\n",
    "    #     ),\n",
    "    # ),\n",
    "    (\n",
    "        \"mlp\",\n",
    "        make_pipeline(\n",
    "            StandardScaler(),\n",
    "            MLPClassifier(\n",
    "                random_state=SEED,\n",
    "                max_iter=1000,\n",
    "                tol=1e-3,\n",
    "                hidden_layer_sizes=(100, 300, 200, 100),\n",
    "            ),\n",
    "        ),\n",
    "    ),\n",
    "    (\n",
    "        \"gbc\",\n",
    "        make_pipeline(StandardScaler(), GradientBoostingClassifier(random_state=SEED)),\n",
    "    ),\n",
    "]\n",
    "\n",
    "# First Stacking Layer\n",
    "first_layer = StackingClassifier(\n",
    "    estimators=base_classifiers_1, final_estimator=LogisticRegression(), cv=5\n",
    ")\n",
    "\n",
    "base_classifiers_2 = [\n",
    "    (\"first_layer\", first_layer),\n",
    "    (\n",
    "        \"mlp\",\n",
    "        make_pipeline(\n",
    "            StandardScaler(),\n",
    "            MLPClassifier(\n",
    "                random_state=SEED,\n",
    "                max_iter=1000,\n",
    "                tol=1e-3,\n",
    "                hidden_layer_sizes=(100, 300, 200, 100),\n",
    "            ),\n",
    "        ),\n",
    "    ),\n",
    "    (\n",
    "        \"gbc\",\n",
    "        make_pipeline(StandardScaler(), GradientBoostingClassifier(random_state=SEED)),\n",
    "    ),\n",
    "]\n",
    "\n",
    "\n",
    "param_distributions = {\n",
    "    # Parameters for the First Stacking Layer\n",
    "    \"stacked_ensemble__first_layer__rf__randomforestclassifier__n_estimators\": randint(\n",
    "        50, 200\n",
    "    ),\n",
    "    \"stacked_ensemble__first_layer__rf__randomforestclassifier__max_depth\": randint(\n",
    "        15, 50\n",
    "    ),\n",
    "    \"stacked_ensemble__first_layer__rf__randomforestclassifier__min_samples_split\": randint(\n",
    "        4, 20\n",
    "    ),\n",
    "    \"stacked_ensemble__first_layer__rf__randomforestclassifier__min_samples_leaf\": randint(\n",
    "        4, 20\n",
    "    ),\n",
    "    # \"first_layer__svc__svc__C\": uniform(0.1, 10),\n",
    "    # \"first_layer__svc__svc__gamma\": [\"scale\", \"auto\"],\n",
    "    \"stacked_ensemble__first_layer__dt__decisiontreeclassifier__max_depth\": randint(\n",
    "        15, 50\n",
    "    ),\n",
    "    \"stacked_ensemble__first_layer__dt__decisiontreeclassifier__min_samples_split\": randint(\n",
    "        5, 20\n",
    "    ),\n",
    "    \"stacked_ensemble__first_layer__dt__decisiontreeclassifier__min_samples_leaf\": randint(\n",
    "        5, 20\n",
    "    ),\n",
    "    # \"first_layer__elasticnet__elasticnet__alpha\": uniform(0.0001, 1),\n",
    "    # \"first_layer__elasticnet__elasticnet__l1_ratio\": uniform(0, 1),\n",
    "    \"stacked_ensemble__first_layer__mlp__mlpclassifier__alpha\": uniform(0.0001, 1),\n",
    "    \"stacked_ensemble__first_layer__mlp__mlpclassifier__learning_rate_init\": uniform(\n",
    "        0.001, 0.1\n",
    "    ),\n",
    "    \"stacked_ensemble__first_layer__mlp__mlpclassifier__hidden_layer_sizes\": [\n",
    "        (50, 40),\n",
    "        (50, 100, 50),\n",
    "        (50, 150, 100, 50),\n",
    "    ],\n",
    "    # Parameters for the Second Stacking Layer\n",
    "    \"stacked_ensemble__mlp__mlpclassifier__alpha\": uniform(0.0001, 1),\n",
    "    \"stacked_ensemble__mlp__mlpclassifier__learning_rate_init\": uniform(0.001, 0.1),\n",
    "    \"stacked_ensemble__mlp__mlpclassifier__hidden_layer_sizes\": [\n",
    "        (50, 40),\n",
    "        (50, 100, 50),\n",
    "        (50, 150, 100, 50),\n",
    "    ],\n",
    "    \"stacked_ensemble__gbc__gradientboostingclassifier__n_estimators\": randint(50, 200),\n",
    "    \"stacked_ensemble__gbc__gradientboostingclassifier__max_depth\": randint(15, 50),\n",
    "    \"stacked_ensemble__gbc__gradientboostingclassifier__min_samples_split\": randint(\n",
    "        4, 20\n",
    "    ),\n",
    "    \"stacked_ensemble__gbc__gradientboostingclassifier__min_samples_leaf\": randint(\n",
    "        4, 20\n",
    "    ),\n",
    "    # Parameters for the Final Estimator\n",
    "    \"stacked_ensemble__final_estimator__C\": uniform(0.01, 10),\n",
    "    \"gbc__n_estimators\": randint(50, 200),\n",
    "    \"gbc__max_depth\": randint(15, 50),\n",
    "    \"gbc__min_samples_split\": randint(4, 20),\n",
    "    \"gbc__min_samples_leaf\": randint(4, 20),\n",
    "    \"rf__n_estimators\": randint(50, 200),\n",
    "    \"rf__max_depth\": randint(15, 50),\n",
    "    \"rf__min_samples_split\": randint(4, 20),\n",
    "    \"rf__min_samples_leaf\": randint(4, 20),\n",
    "}\n",
    "\n",
    "\n",
    "# Second Stacking Layer\n",
    "stacked_ensemble_model = StackingClassifier(\n",
    "    estimators=base_classifiers_2, final_estimator=LogisticRegression(), cv=5\n",
    ")\n",
    "# Define the committee of models\n",
    "committee_models = [\n",
    "    (\"stacked_ensemble\", stacked_ensemble_model),\n",
    "    (\n",
    "        \"gbc\",\n",
    "        GradientBoostingClassifier(random_state=SEED, n_estimators=100, max_depth=15),\n",
    "    ),\n",
    "    (\"rf\", RandomForestClassifier(random_state=SEED, n_estimators=100, max_depth=15)),\n",
    "    (\n",
    "        \"mlp\",\n",
    "        MLPClassifier(\n",
    "            random_state=SEED,\n",
    "            max_iter=1000,\n",
    "            tol=1e-3,\n",
    "            hidden_layer_sizes=(100, 300, 200, 100),\n",
    "        ),\n",
    "    ),\n",
    "]\n",
    "\n",
    "# Create the committee model\n",
    "committee_model = VotingClassifier(committee_models, voting=\"soft\")\n",
    "# print(committee_model)\n",
    "# Perform randomized search\n",
    "random_search = RandomizedSearchCV(\n",
    "    committee_model,\n",
    "    param_distributions=param_distributions,\n",
    "    scoring=\"balanced_accuracy\",\n",
    "    n_iter=RANDOM_SEARCH_N_ITER,\n",
    "    cv=5,\n",
    "    verbose=4,\n",
    "    random_state=SEED,\n",
    "    n_jobs=8,\n",
    ")\n",
    "\n",
    "random_search.fit(train_x, train_y.ravel())\n",
    "\n",
    "y_pred = random_search.predict(valid_x)\n",
    "\n",
    "balanced_accuracy = balanced_accuracy_score(valid_y, y_pred)\n",
    "\n",
    "print(f\"Model Balanced Accuracy: {balanced_accuracy}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(random_search.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'RandomizedSearchCV' object has no attribute 'save'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[33], line 7\u001b[0m\n\u001b[0;32m      3\u001b[0m np\u001b[38;5;241m.\u001b[39msavetxt(output_path, proba[:, \u001b[38;5;241m1\u001b[39m], delimiter\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m      4\u001b[0m joblib\u001b[38;5;241m.\u001b[39mdump(\n\u001b[0;32m      5\u001b[0m     random_search, path\u001b[38;5;241m.\u001b[39mjoin(OUTPUT_DIR_MANUAL, UNIQUE_ID, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmanual_model_pred.pkl\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m      6\u001b[0m )\n\u001b[1;32m----> 7\u001b[0m \u001b[43mrandom_search\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msave\u001b[49m(path\u001b[38;5;241m.\u001b[39mjoin(OUTPUT_DIR_MANUAL, UNIQUE_ID, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmanual_model.pkl\u001b[39m\u001b[38;5;124m\"\u001b[39m))\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'RandomizedSearchCV' object has no attribute 'save'"
     ]
    }
   ],
   "source": [
    "proba = random_search.predict_proba(test_x)\n",
    "output_path = path.join(OUTPUT_DIR_MANUAL, UNIQUE_ID, \"manual_model.txt\")\n",
    "np.savetxt(output_path, proba[:, 1], delimiter=\"\\n\")\n",
    "joblib.dump(\n",
    "    random_search, path.join(OUTPUT_DIR_MANUAL, UNIQUE_ID, \"manual_model_pred.pkl\")\n",
    ")\n",
    "# random_search.save(path.join(OUTPUT_DIR_MANUAL, UNIQUE_ID, \"manual_model.pkl\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Autogluon"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(train_x.shape, train_y.shape)\n",
    "print(valid_x.shape, valid_y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = np.concatenate((train_x, train_y), axis=1)\n",
    "train_data_pd = pd.DataFrame(train_data)\n",
    "train_data_pd.rename(columns={train_data_pd.columns[-1]: \"class\"}, inplace=True)\n",
    "\n",
    "valid_data = np.concatenate((valid_x, valid_y), axis=1)\n",
    "valid_data_pd = pd.DataFrame(valid_data)\n",
    "valid_data_pd.rename(columns={valid_data_pd.columns[-1]: \"class\"}, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(train_data_pd.shape, valid_data_pd.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_path = path.join(OUTPUT_DIR_AUTOGLUON, UNIQUE_ID)\n",
    "predictor = TabularPredictor(\n",
    "    label=\"class\",\n",
    "    path=save_path,\n",
    "    eval_metric=\"balanced_accuracy\",\n",
    "    problem_type=\"binary\",\n",
    ").fit(\n",
    "    train_data_pd,\n",
    "    time_limit=TRAIN_TIME_LIMIT_AUTOGLUON,\n",
    "    presets=\"best_quality\",\n",
    "    hyperparameters=\"default\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictor.leaderboard()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictor.evaluate(valid_data_pd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "proba = predictor.predict_proba(pd.DataFrame(test_x))\n",
    "output_path = path.join(OUTPUT_DIR_AUTOGLUON, UNIQUE_ID, \"manual_model_pred.txt\")\n",
    "np.savetxt(output_path, proba.values[:, 1], delimiter=\"\\n\")\n",
    "# predictor.save(path.join(OUTPUT_DIR_AUTOGLUON, UNIQUE_ID, \"manual_model.pkl\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MLJar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "automl = AutoML(\n",
    "    mode=\"Compete\",\n",
    "    ml_task=\"binary_classification\",\n",
    "    total_time_limit=TRAIN_TIME_LIMIT_MLJAR,\n",
    "    eval_metric=\"f1\",\n",
    "    random_state=SEED,\n",
    "    results_path=path.join(OUTPUT_DIR_MLJAR, UNIQUE_ID),\n",
    ")\n",
    "\n",
    "automl.fit(train_x, train_y.ravel())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(valid_x.shape, valid_y.shape)\n",
    "print(train_x.shape, train_y.shape)\n",
    "predictions = automl.predict(valid_x)\n",
    "score = balanced_accuracy_score(valid_y, predictions)\n",
    "print(f\"Model Balanced Accuracy: {score}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "proba = automl.predict_proba(test_x)\n",
    "output_path = path.join(OUTPUT_DIR_MLJAR, UNIQUE_ID, \"mljar_model_proba.txt\")\n",
    "np.savetxt(output_path, proba[:, 1], delimiter=\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(proba)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ensemble_predict(X, model1, model2):\n",
    "    pred1 = model1.predict_proba(pd.DataFrame(X)).values[:, 1]\n",
    "    pred2 = model2.predict_proba(X)[:, 1]\n",
    "    # pred3 = model3.predict_proba(X)[:,1]\n",
    "    print(pred1, pred2)\n",
    "    avg_pred = (pred1 + pred2) / 2\n",
    "\n",
    "    return avg_pred\n",
    "\n",
    "\n",
    "# Example of using the ensemble\n",
    "final_predictions = ensemble_predict(test_x, predictor, automl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(final_predictions)\n",
    "os.makedirs(path.join(\"ensamble\", UNIQUE_ID), exist_ok=True)\n",
    "\n",
    "\n",
    "np.savetxt(\n",
    "    path.join(\"ensamble\", UNIQUE_ID, \"123manual_model_pred.txt\"),\n",
    "    final_predictions,\n",
    "    delimiter=\"\\n\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Auto SKLearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install auto-sklearn\n",
    "# !pip install ydata-profiling\n",
    "# from autosklearn.classification import AutoSklearnClassifier\n",
    "from autosklearn.experimental.askl2 import AutoSklearn2Classifier\n",
    "from autosklearn.metrics import balanced_accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "settings = {\n",
    "    \"time_left_for_this_task\": TRAIN_TIME_LIMIT_AUTO_SKLEARN,\n",
    "    \"seed\": SEED,\n",
    "    \"metric\": balanced_accuracy,\n",
    "    \"n_jobs\": -1,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "askl2 = AutoSklearn2Classifier(**settings)\n",
    "askl2.fit(train_x, train_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "leaderboard = askl2.leaderboard(sort_by=\"model_id\", ensemble_only=True)\n",
    "print(leaderboard)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = askl2.predict(valid_x)\n",
    "balanced_accuracy_score(valid_y, predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "proba = askl2.predict_proba(test_x)\n",
    "output_path = path.join(OUTPUT_DIR_AUTO_SKLEARN, UNIQUE_ID, \"manual_model.txt\")\n",
    "np.savetxt(output_path, proba, delimiter=\"\\n\")\n",
    "askl2.save(path.join(OUTPUT_DIR_AUTO_SKLEARN, UNIQUE_ID, \"manual_model.pkl\"))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
