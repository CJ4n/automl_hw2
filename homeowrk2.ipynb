{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install mljar-supervised"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "from os import path\n",
    "\n",
    "import joblib\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from autogluon.tabular import TabularPredictor\n",
    "from catboost import CatBoostClassifier\n",
    "from scipy.stats import randint, uniform\n",
    "from sklearn.ensemble import RandomForestClassifier, StackingClassifier\n",
    "from sklearn.feature_selection import SelectKBest, VarianceThreshold, f_classif\n",
    "from sklearn.linear_model import ElasticNet, LogisticRegression\n",
    "from sklearn.metrics import balanced_accuracy_score, mutual_info_score\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.utils import shuffle\n",
    "from supervised.automl import AutoML\n",
    "from xgboost import XGBClassifier\n",
    "\n",
    "# from supervised.automl import AutoML  # mljar-supervised"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Constants\n",
    "\n",
    "SEED = 42\n",
    "N_JOBS = -1\n",
    "RANDOM_SEARCH_N_ITER = 1\n",
    "TRAIN_TIME_LIMIT_AUTOGLUON = 60 * 60 * 10\n",
    "TRAIN_TIME_LIMIT_MLJAR = 60 * 60 * 10\n",
    "TRAIN_TIME_LIMIT_AUTO_SKLEARN = 60 * 30\n",
    "OUTPUT_DIR_MANUAL = path.join(\"output\", \"manual\")\n",
    "OUTPUT_DIR_AUTOGLUON = path.join(\"output\", \"autogluon\")\n",
    "OUTPUT_DIR_MLJAR = path.join(\"output\", \"mljar\")\n",
    "OUTPUT_DIR_AUTO_SKLEARN = path.join(\"output\", \"auto_sklearn\")\n",
    "UNIQUE_ID = time.strftime(\"%Y%m%d_%H%M%S\")\n",
    "APPLY_REMOVE_LOW_VARIANCE_FEATURES = True\n",
    "APPLY_REMOVE_CORRELATED_FEATURES = True\n",
    "APPLY_REMOVE_RANDOM_FEATURES = False\n",
    "APPLY_ANOVA = True\n",
    "ANOVE_FEATURES = 25"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating output directory output\\manual\\20240114_140359\n",
      "Creating output directory output\\autogluon\\20240114_140359\n",
      "Creating output directory output\\mljar\\20240114_140359\n",
      "Creating output directory output\\auto_sklearn\\20240114_140359\n"
     ]
    }
   ],
   "source": [
    "# prepare output directories\n",
    "for output_dir in [\n",
    "    OUTPUT_DIR_MANUAL,\n",
    "    OUTPUT_DIR_AUTOGLUON,\n",
    "    OUTPUT_DIR_MLJAR,\n",
    "    OUTPUT_DIR_AUTO_SKLEARN,\n",
    "]:\n",
    "    if not path.exists(path.join(output_dir, UNIQUE_ID)):\n",
    "        print(f\"Creating output directory {path.join(output_dir, UNIQUE_ID)}\")\n",
    "        os.makedirs(path.join(output_dir, UNIQUE_ID))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_highly_correlated_features(train_x, valid_x, test_x, threshold=0.95):\n",
    "    # Calculate correlation matrix\n",
    "    corr_matrix = np.corrcoef(train_x, rowvar=False)\n",
    "    # Select upper triangle of correlation matrix\n",
    "    upper = np.triu(corr_matrix, k=1)\n",
    "    # Find indices of feature columns with correlation greater than threshold\n",
    "    to_drop = [i for i in range(upper.shape[1]) if any(upper[:, i] > threshold)]\n",
    "\n",
    "    # Drop features from train, validation, and test set\n",
    "    train_x = np.delete(train_x, to_drop, axis=1)\n",
    "    valid_x = np.delete(valid_x, to_drop, axis=1)\n",
    "    test_x = np.delete(test_x, to_drop, axis=1)\n",
    "\n",
    "    return train_x, valid_x, test_x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove Low Variance Columns\n",
    "def remove_low_variance_features(train_x, valid_x, test_x, threshold=(0.8 * (1 - 0.8))):\n",
    "    sel = VarianceThreshold(threshold=threshold)\n",
    "    sel.fit(train_x)\n",
    "    train_x = train_x[:, sel.get_support(indices=True)]\n",
    "    valid_x = valid_x[:, sel.get_support(indices=True)]\n",
    "    test_x = test_x[:, sel.get_support(indices=True)]\n",
    "    return train_x, valid_x, test_x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove Random Columns\n",
    "def remove_random_features(\n",
    "    train_x: np.ndarray,\n",
    "    train_y: np.ndarray,\n",
    "    valid_x: np.ndarray,\n",
    "    test_x: np.ndarray,\n",
    "    importance=0.005,\n",
    "):\n",
    "    tree: DecisionTreeClassifier = DecisionTreeClassifier(random_state=0)\n",
    "    tree.fit(train_x, train_y)\n",
    "    importances = tree.feature_importances_\n",
    "\n",
    "    # Assume columns with very low importance are \"random\"\n",
    "    # This threshold can be adjusted based on domain knowledge\n",
    "    important_indices = [i for i, imp in enumerate(importances) if imp > importance]\n",
    "    train_x = train_x[:, important_indices]\n",
    "    valid_x = valid_x[:, important_indices]\n",
    "    test_x = test_x[:, important_indices]\n",
    "    return train_x, valid_x, test_x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def anova_filter(\n",
    "    train_x: np.ndarray,\n",
    "    train_y: np.ndarray,\n",
    "    valid_x: np.ndarray,\n",
    "    test_x: np.ndarray,\n",
    "    k: int = 50,\n",
    "):\n",
    "    selector = SelectKBest(f_classif, k=k)\n",
    "    selector.fit(train_x, train_y)\n",
    "\n",
    "    train_x = selector.transform(train_x)\n",
    "    valid_x = selector.transform(valid_x)\n",
    "    test_x = selector.transform(test_x)\n",
    "    return train_x, valid_x, test_x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dump_proba(model, test_x, output_path_proba):\n",
    "    proba = model.predict_proba(test_x)\n",
    "\n",
    "    if isinstance(proba, pd.DataFrame):\n",
    "        proba = proba.values\n",
    "\n",
    "    np.savetxt(\n",
    "        output_path_proba,\n",
    "        proba[:, 1],\n",
    "        delimiter=\"\\n\",\n",
    "        header='\"313201_313212\"',\n",
    "        comments=\"\",\n",
    "        # fmt=\"%.19f\",\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dump_model(model, output_path_model):\n",
    "    joblib.dump(model, output_path_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import ExtraTreesClassifier, RandomForestClassifier\n",
    "from sklearn.feature_selection import RFE, RFECV\n",
    "\n",
    "\n",
    "def perform_feature_selection(\n",
    "    train_x, train_y, valid_x, test_x, n_features_to_select=None\n",
    "):\n",
    "    estimator_et = ExtraTreesClassifier()\n",
    "    rfe_et = RFE(estimator=estimator_et, n_features_to_select=n_features_to_select)\n",
    "    rfe_et.fit(train_x, train_y)\n",
    "    train_x = train_x[:,rfe_et.support_]\n",
    "    valid_x = valid_x[:,rfe_et.support_]\n",
    "    test_x = test_x[:,rfe_et.support_]\n",
    "    print(train_x.shape, valid_x.shape, test_x.shape)\n",
    "\n",
    "    estimator_rf = RandomForestClassifier()\n",
    "    rfe_rf = RFE(estimator=estimator_rf, n_features_to_select=n_features_to_select)\n",
    "    rfe_rf.fit(train_x, train_y)\n",
    "    train_x = train_x[:,rfe_rf.support_]\n",
    "    valid_x = valid_x[:,rfe_rf.support_]\n",
    "    test_x = test_x[:,rfe_rf.support_]\n",
    "    print(train_x.shape, valid_x.shape, test_x.shape)\n",
    "\n",
    "    rfecv_et = RFECV(estimator=estimator_et, cv=3)\n",
    "    rfecv_et.fit(train_x, train_y)\n",
    "    train_x = train_x[:,rfecv_et.support_]\n",
    "    valid_x = valid_x[:,rfecv_et.support_]\n",
    "    test_x = test_x[:,rfecv_et.support_]\n",
    "    print(train_x.shape, valid_x.shape, test_x.shape)\n",
    "\n",
    "    rfecv_rf = RFECV(estimator=estimator_rf, cv=3)\n",
    "    rfecv_rf.fit(train_x, train_y)\n",
    "    train_x = train_x[:,rfecv_rf.support_]\n",
    "    valid_x = valid_x[:,rfecv_rf.support_]\n",
    "    test_x = test_x[:,rfecv_rf.support_]\n",
    "    print(train_x.shape, valid_x.shape, test_x.shape)\n",
    "\n",
    "    return train_x, train_y, valid_x, test_x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "prefix = \"\"\n",
    "\n",
    "_test_x = pd.read_table(prefix + \"artificial_test.data\", sep=\" \", header=None)\n",
    "_test_x.drop(_test_x.columns[500], axis=1, inplace=True)\n",
    "_train_y = pd.read_table(prefix + \"artificial_train.labels\", header=None)\n",
    "_train_x = pd.read_table(prefix + \"artificial_train.data\", sep=\" \", header=None)\n",
    "_train_x.drop(_train_x.columns[500], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "_test_x = np.array(_test_x, dtype=float, copy=True)\n",
    "_train_x = np.array(_train_x, dtype=float, copy=True)\n",
    "_train_y = np.array(_train_y, dtype=float, copy=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "_train_x, _train_y = shuffle(_train_x, _train_y, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_train_and_validation_data():\n",
    "    split = 400\n",
    "    train_x, valid_x = _train_x[split:].copy(), _train_x[:split].copy()\n",
    "    train_y, valid_y = _train_y[split:].copy(), _train_y[:split].copy()\n",
    "    return train_x, train_y, valid_x, valid_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1600, 500) (1600, 1) (400, 500) (400, 1)\n"
     ]
    }
   ],
   "source": [
    "train_x, train_y, valid_x, valid_y = get_train_and_validation_data()\n",
    "# train_y = train_y.reshape(-1, 1)\n",
    "# valid_y = valid_y.reshape(-1, 1)\n",
    "print(train_x.shape, train_y.shape, valid_x.shape, valid_y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1600, 480) (400, 480) (600, 480)\n",
      "(1600, 480) (400, 480) (600, 480)\n"
     ]
    }
   ],
   "source": [
    "train_x, valid_x, test_x = perform_feature_selection(\n",
    "    train_x, train_y.copy().ravel(), valid_x, _test_x, n_features_to_select=None\n",
    ")\n",
    "print(train_x.shape, valid_x.shape, test_x.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_x.shape:  (1600, 490)\n",
      "valid_x.shape:  (400, 490)\n",
      "test_x.shape:  (600, 490)\n"
     ]
    }
   ],
   "source": [
    "if APPLY_REMOVE_CORRELATED_FEATURES:\n",
    "    train_x, valid_x, test_x = remove_highly_correlated_features(\n",
    "        train_x, valid_x, _test_x\n",
    "    )\n",
    "    print(\"train_x.shape: \", train_x.shape)\n",
    "    print(\"valid_x.shape: \", valid_x.shape)\n",
    "    print(\"test_x.shape: \", test_x.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_x.shape:  (1600, 490)\n",
      "valid_x.shape:  (400, 490)\n",
      "test_x.shape:  (600, 490)\n"
     ]
    }
   ],
   "source": [
    "if APPLY_REMOVE_LOW_VARIANCE_FEATURES:\n",
    "    train_x, valid_x, test_x = remove_low_variance_features(train_x, valid_x, test_x)\n",
    "    print(\"train_x.shape: \", train_x.shape)\n",
    "    print(\"valid_x.shape: \", valid_x.shape)\n",
    "    print(\"test_x.shape: \", test_x.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "if APPLY_REMOVE_RANDOM_FEATURES:\n",
    "    train_x, valid_x, test_x = remove_random_features(\n",
    "        train_x=train_x, train_y=train_y, valid_x=valid_x, test_x=test_x\n",
    "    )\n",
    "    print(\"train_x.shape: \", train_x.shape)\n",
    "    print(\"valid_x.shape: \", valid_x.shape)\n",
    "    print(\"test_x.shape: \", test_x.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_x.shape:  (1600, 25)\n",
      "valid_x.shape:  (400, 25)\n",
      "test_x.shape:  (600, 25)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\YanPC\\Downloads\\automl_hw2-master\\.venv\\lib\\site-packages\\sklearn\\utils\\validation.py:1183: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n"
     ]
    }
   ],
   "source": [
    "if APPLY_ANOVA:\n",
    "    train_x, valid_x, test_x = anova_filter(\n",
    "        train_x=train_x,\n",
    "        train_y=train_y,\n",
    "        valid_x=valid_x,\n",
    "        test_x=test_x,\n",
    "        k=ANOVE_FEATURES,\n",
    "    )\n",
    "    print(\"train_x.shape: \", train_x.shape)\n",
    "    print(\"valid_x.shape: \", valid_x.shape)\n",
    "    print(\"test_x.shape: \", test_x.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_x.shape:  (1600, 25)\n",
      "train_y.shape:  (1600, 1)\n",
      "valid_x.shape:  (400, 25)\n",
      "valid_y.shape:  (400, 1)\n",
      "test_x.shape:  (600, 25)\n"
     ]
    }
   ],
   "source": [
    "print(\"train_x.shape: \", train_x.shape)\n",
    "print(\"train_y.shape: \", train_y.shape)\n",
    "print(\"valid_x.shape: \", valid_x.shape)\n",
    "print(\"valid_y.shape: \", valid_y.shape)\n",
    "print(\"test_x.shape: \", test_x.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ok\n",
      "ok\n",
      "ok\n",
      "ok\n",
      "ok\n"
     ]
    }
   ],
   "source": [
    "if isinstance(train_x, np.ndarray):\n",
    "    print(\"ok\")\n",
    "if isinstance(train_y, np.ndarray):\n",
    "    print(\"ok\")\n",
    "if isinstance(valid_x, np.ndarray):\n",
    "    print(\"ok\")\n",
    "if isinstance(valid_y, np.ndarray):\n",
    "    print(\"ok\")\n",
    "if isinstance(test_x, np.ndarray):\n",
    "    print(\"ok\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sanity check\n",
    "(\n",
    "    original_train_x,\n",
    "    original_train_y,\n",
    "    original_valid_x,\n",
    "    original_valid_y,\n",
    ") = get_train_and_validation_data()\n",
    "for y, original_y in zip([train_y, valid_y], [original_train_y, original_valid_y]):\n",
    "    assert y.shape == original_y.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### manual model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0:\tlearn: 0.7412500\ttotal: 148ms\tremaining: 2m 27s\n",
      "100:\tlearn: 0.9037500\ttotal: 489ms\tremaining: 4.35s\n",
      "200:\tlearn: 0.9512500\ttotal: 823ms\tremaining: 3.27s\n",
      "300:\tlearn: 0.9787500\ttotal: 1.17s\tremaining: 2.73s\n",
      "400:\tlearn: 0.9906250\ttotal: 1.5s\tremaining: 2.24s\n",
      "500:\tlearn: 0.9975000\ttotal: 1.83s\tremaining: 1.82s\n",
      "600:\tlearn: 0.9987500\ttotal: 2.15s\tremaining: 1.43s\n",
      "700:\tlearn: 1.0000000\ttotal: 2.48s\tremaining: 1.06s\n",
      "800:\tlearn: 1.0000000\ttotal: 2.81s\tremaining: 698ms\n",
      "900:\tlearn: 1.0000000\ttotal: 3.14s\tremaining: 345ms\n",
      "999:\tlearn: 1.0000000\ttotal: 3.46s\tremaining: 0us\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-1 {color: black;}#sk-container-id-1 pre{padding: 0;}#sk-container-id-1 div.sk-toggleable {background-color: white;}#sk-container-id-1 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-1 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-1 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-1 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-1 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-1 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-1 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-1 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-1 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-1 div.sk-item {position: relative;z-index: 1;}#sk-container-id-1 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-1 div.sk-item::before, #sk-container-id-1 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-1 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-1 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-1 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-1 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-1 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-1 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-1 div.sk-label-container {text-align: center;}#sk-container-id-1 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-1 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-1\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>VotingClassifier(estimators=[(&#x27;stacked_ensemble_1&#x27;,\n",
       "                              StackingClassifier(cv=5,\n",
       "                                                 estimators=[(&#x27;mlp&#x27;,\n",
       "                                                              Pipeline(steps=[(&#x27;standardscaler&#x27;,\n",
       "                                                                               StandardScaler()),\n",
       "                                                                              (&#x27;mlpclassifier&#x27;,\n",
       "                                                                               MLPClassifier(alpha=0.001,\n",
       "                                                                                             early_stopping=True,\n",
       "                                                                                             hidden_layer_sizes=(100,\n",
       "                                                                                                                 300,\n",
       "                                                                                                                 200,\n",
       "                                                                                                                 100),\n",
       "                                                                                             max_iter=1000,\n",
       "                                                                                             random_state=42,\n",
       "                                                                                             solver=&#x27;lbfgs&#x27;,\n",
       "                                                                                             tol=0.001))])),\n",
       "                                                             (&#x27;gbc&#x27;,\n",
       "                                                              Pipeline(steps=[(&#x27;standardscaler&#x27;,\n",
       "                                                                               Sta...\n",
       "                                                             grow_policy=None,\n",
       "                                                             importance_type=None,\n",
       "                                                             interaction_constraints=None,\n",
       "                                                             learning_rate=0.02,\n",
       "                                                             max_bin=None,\n",
       "                                                             max_cat_threshold=None,\n",
       "                                                             max_cat_to_onehot=None,\n",
       "                                                             max_delta_step=None,\n",
       "                                                             max_depth=6,\n",
       "                                                             max_leaves=None,\n",
       "                                                             min_child_weight=1,\n",
       "                                                             missing=nan,\n",
       "                                                             monotone_constraints=None,\n",
       "                                                             multi_strategy=None,\n",
       "                                                             n_estimators=1000,\n",
       "                                                             n_jobs=None,\n",
       "                                                             num_parallel_tree=None,\n",
       "                                                             random_state=42, ...))]))],\n",
       "                 voting=&#x27;soft&#x27;)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item sk-dashed-wrapped\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-1\" type=\"checkbox\" ><label for=\"sk-estimator-id-1\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">VotingClassifier</label><div class=\"sk-toggleable__content\"><pre>VotingClassifier(estimators=[(&#x27;stacked_ensemble_1&#x27;,\n",
       "                              StackingClassifier(cv=5,\n",
       "                                                 estimators=[(&#x27;mlp&#x27;,\n",
       "                                                              Pipeline(steps=[(&#x27;standardscaler&#x27;,\n",
       "                                                                               StandardScaler()),\n",
       "                                                                              (&#x27;mlpclassifier&#x27;,\n",
       "                                                                               MLPClassifier(alpha=0.001,\n",
       "                                                                                             early_stopping=True,\n",
       "                                                                                             hidden_layer_sizes=(100,\n",
       "                                                                                                                 300,\n",
       "                                                                                                                 200,\n",
       "                                                                                                                 100),\n",
       "                                                                                             max_iter=1000,\n",
       "                                                                                             random_state=42,\n",
       "                                                                                             solver=&#x27;lbfgs&#x27;,\n",
       "                                                                                             tol=0.001))])),\n",
       "                                                             (&#x27;gbc&#x27;,\n",
       "                                                              Pipeline(steps=[(&#x27;standardscaler&#x27;,\n",
       "                                                                               Sta...\n",
       "                                                             grow_policy=None,\n",
       "                                                             importance_type=None,\n",
       "                                                             interaction_constraints=None,\n",
       "                                                             learning_rate=0.02,\n",
       "                                                             max_bin=None,\n",
       "                                                             max_cat_threshold=None,\n",
       "                                                             max_cat_to_onehot=None,\n",
       "                                                             max_delta_step=None,\n",
       "                                                             max_depth=6,\n",
       "                                                             max_leaves=None,\n",
       "                                                             min_child_weight=1,\n",
       "                                                             missing=nan,\n",
       "                                                             monotone_constraints=None,\n",
       "                                                             multi_strategy=None,\n",
       "                                                             n_estimators=1000,\n",
       "                                                             n_jobs=None,\n",
       "                                                             num_parallel_tree=None,\n",
       "                                                             random_state=42, ...))]))],\n",
       "                 voting=&#x27;soft&#x27;)</pre></div></div></div><div class=\"sk-parallel\"><div class=\"sk-parallel-item\"><div class=\"sk-item\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><label>stacked_ensemble_1</label></div></div><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-parallel\"><div class=\"sk-parallel-item\"><div class=\"sk-item\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><label>mlp</label></div></div><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-2\" type=\"checkbox\" ><label for=\"sk-estimator-id-2\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">StandardScaler</label><div class=\"sk-toggleable__content\"><pre>StandardScaler()</pre></div></div></div><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-3\" type=\"checkbox\" ><label for=\"sk-estimator-id-3\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">MLPClassifier</label><div class=\"sk-toggleable__content\"><pre>MLPClassifier(alpha=0.001, early_stopping=True,\n",
       "              hidden_layer_sizes=(100, 300, 200, 100), max_iter=1000,\n",
       "              random_state=42, solver=&#x27;lbfgs&#x27;, tol=0.001)</pre></div></div></div></div></div></div></div></div><div class=\"sk-parallel-item\"><div class=\"sk-item\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><label>gbc</label></div></div><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-4\" type=\"checkbox\" ><label for=\"sk-estimator-id-4\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">StandardScaler</label><div class=\"sk-toggleable__content\"><pre>StandardScaler()</pre></div></div></div><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-5\" type=\"checkbox\" ><label for=\"sk-estimator-id-5\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">GradientBoostingClassifier</label><div class=\"sk-toggleable__content\"><pre>GradientBoostingClassifier(max_depth=30, min_samples_leaf=2,\n",
       "                           min_samples_split=5, n_estimators=1000,\n",
       "                           random_state=42)</pre></div></div></div></div></div></div></div></div><div class=\"sk-parallel-item\"><div class=\"sk-item\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><label>rf</label></div></div><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-6\" type=\"checkbox\" ><label for=\"sk-estimator-id-6\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">StandardScaler</label><div class=\"sk-toggleable__content\"><pre>StandardScaler()</pre></div></div></div><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-7\" type=\"checkbox\" ><label for=\"sk-estimator-id-7\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">RandomForestClassifier</label><div class=\"sk-toggleable__content\"><pre>RandomForestClassifier(max_depth=30, min_samples_leaf=4, n_estimators=1000,\n",
       "                       random_state=42)</pre></div></div></div></div></div></div></div></div></div></div><div class=\"sk-item\"><div class=\"sk-parallel\"><div class=\"sk-parallel-item\"><div class=\"sk-item\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><label>final_estimator</label></div></div><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-8\" type=\"checkbox\" ><label for=\"sk-estimator-id-8\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">LogisticRegression</label><div class=\"sk-toggleable__content\"><pre>LogisticRegression()</pre></div></div></div></div></div></div></div></div></div></div></div></div></div><div class=\"sk-parallel-item\"><div class=\"sk-item\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><label>stacked_ensemble_2</label></div></div><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-parallel\"><div class=\"sk-parallel-item\"><div class=\"sk-item\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><label>mlp</label></div></div><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-9\" type=\"checkbox\" ><label for=\"sk-estimator-id-9\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">StandardScaler</label><div class=\"sk-toggleable__content\"><pre>StandardScaler()</pre></div></div></div><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-10\" type=\"checkbox\" ><label for=\"sk-estimator-id-10\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">MLPClassifier</label><div class=\"sk-toggleable__content\"><pre>MLPClassifier(alpha=0.001, early_stopping=True,\n",
       "              hidden_layer_sizes=(100, 300, 200, 100), max_iter=1000,\n",
       "              random_state=42, solver=&#x27;lbfgs&#x27;, tol=0.001)</pre></div></div></div></div></div></div></div></div><div class=\"sk-parallel-item\"><div class=\"sk-item\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><label>gbc</label></div></div><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-11\" type=\"checkbox\" ><label for=\"sk-estimator-id-11\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">StandardScaler</label><div class=\"sk-toggleable__content\"><pre>StandardScaler()</pre></div></div></div><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-12\" type=\"checkbox\" ><label for=\"sk-estimator-id-12\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">GradientBoostingClassifier</label><div class=\"sk-toggleable__content\"><pre>GradientBoostingClassifier(max_depth=30, min_samples_leaf=2,\n",
       "                           min_samples_split=5, n_estimators=1000,\n",
       "                           random_state=42)</pre></div></div></div></div></div></div></div></div></div></div><div class=\"sk-item\"><div class=\"sk-parallel\"><div class=\"sk-parallel-item\"><div class=\"sk-item\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><label>final_estimator</label></div></div><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-13\" type=\"checkbox\" ><label for=\"sk-estimator-id-13\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">LogisticRegression</label><div class=\"sk-toggleable__content\"><pre>LogisticRegression()</pre></div></div></div></div></div></div></div></div></div></div></div></div></div><div class=\"sk-parallel-item\"><div class=\"sk-item\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><label>gbc</label></div></div><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-14\" type=\"checkbox\" ><label for=\"sk-estimator-id-14\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">StandardScaler</label><div class=\"sk-toggleable__content\"><pre>StandardScaler()</pre></div></div></div><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-15\" type=\"checkbox\" ><label for=\"sk-estimator-id-15\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">GradientBoostingClassifier</label><div class=\"sk-toggleable__content\"><pre>GradientBoostingClassifier(max_depth=30, min_samples_leaf=2,\n",
       "                           min_samples_split=5, n_estimators=1000,\n",
       "                           random_state=42)</pre></div></div></div></div></div></div></div></div><div class=\"sk-parallel-item\"><div class=\"sk-item\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><label>rf</label></div></div><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-16\" type=\"checkbox\" ><label for=\"sk-estimator-id-16\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">StandardScaler</label><div class=\"sk-toggleable__content\"><pre>StandardScaler()</pre></div></div></div><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-17\" type=\"checkbox\" ><label for=\"sk-estimator-id-17\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">RandomForestClassifier</label><div class=\"sk-toggleable__content\"><pre>RandomForestClassifier(max_depth=30, min_samples_leaf=4, n_estimators=1000,\n",
       "                       random_state=42)</pre></div></div></div></div></div></div></div></div><div class=\"sk-parallel-item\"><div class=\"sk-item\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><label>mlp</label></div></div><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-18\" type=\"checkbox\" ><label for=\"sk-estimator-id-18\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">StandardScaler</label><div class=\"sk-toggleable__content\"><pre>StandardScaler()</pre></div></div></div><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-19\" type=\"checkbox\" ><label for=\"sk-estimator-id-19\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">MLPClassifier</label><div class=\"sk-toggleable__content\"><pre>MLPClassifier(alpha=0.001, early_stopping=True,\n",
       "              hidden_layer_sizes=(100, 300, 200, 100), max_iter=1000,\n",
       "              random_state=42, solver=&#x27;lbfgs&#x27;, tol=0.001)</pre></div></div></div></div></div></div></div></div><div class=\"sk-parallel-item\"><div class=\"sk-item\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><label>cb</label></div></div><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-20\" type=\"checkbox\" ><label for=\"sk-estimator-id-20\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">StandardScaler</label><div class=\"sk-toggleable__content\"><pre>StandardScaler()</pre></div></div></div><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-21\" type=\"checkbox\" ><label for=\"sk-estimator-id-21\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">CatBoostClassifier</label><div class=\"sk-toggleable__content\"><pre>&lt;catboost.core.CatBoostClassifier object at 0x000001DD920088B0&gt;</pre></div></div></div></div></div></div></div></div><div class=\"sk-parallel-item\"><div class=\"sk-item\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><label>xgb</label></div></div><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-22\" type=\"checkbox\" ><label for=\"sk-estimator-id-22\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">StandardScaler</label><div class=\"sk-toggleable__content\"><pre>StandardScaler()</pre></div></div></div><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-23\" type=\"checkbox\" ><label for=\"sk-estimator-id-23\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">XGBClassifier</label><div class=\"sk-toggleable__content\"><pre>XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
       "              colsample_bylevel=None, colsample_bynode=None,\n",
       "              colsample_bytree=0.8, device=None, early_stopping_rounds=None,\n",
       "              enable_categorical=False,\n",
       "              eval_metric=&lt;function balanced_accuracy_score at 0x000001DDFF8895E0&gt;,\n",
       "              feature_types=None, gamma=0, grow_policy=None,\n",
       "              importance_type=None, interaction_constraints=None,\n",
       "              learning_rate=0.02, max_bin=None, max_cat_threshold=None,\n",
       "              max_cat_to_onehot=None, max_delta_step=None, max_depth=6,\n",
       "              max_leaves=None, min_child_weight=1, missing=nan,\n",
       "              monotone_constraints=None, multi_strategy=None, n_estimators=1000,\n",
       "              n_jobs=None, num_parallel_tree=None, random_state=42, ...)</pre></div></div></div></div></div></div></div></div></div></div></div></div>"
      ],
      "text/plain": [
       "VotingClassifier(estimators=[('stacked_ensemble_1',\n",
       "                              StackingClassifier(cv=5,\n",
       "                                                 estimators=[('mlp',\n",
       "                                                              Pipeline(steps=[('standardscaler',\n",
       "                                                                               StandardScaler()),\n",
       "                                                                              ('mlpclassifier',\n",
       "                                                                               MLPClassifier(alpha=0.001,\n",
       "                                                                                             early_stopping=True,\n",
       "                                                                                             hidden_layer_sizes=(100,\n",
       "                                                                                                                 300,\n",
       "                                                                                                                 200,\n",
       "                                                                                                                 100),\n",
       "                                                                                             max_iter=1000,\n",
       "                                                                                             random_state=42,\n",
       "                                                                                             solver='lbfgs',\n",
       "                                                                                             tol=0.001))])),\n",
       "                                                             ('gbc',\n",
       "                                                              Pipeline(steps=[('standardscaler',\n",
       "                                                                               Sta...\n",
       "                                                             grow_policy=None,\n",
       "                                                             importance_type=None,\n",
       "                                                             interaction_constraints=None,\n",
       "                                                             learning_rate=0.02,\n",
       "                                                             max_bin=None,\n",
       "                                                             max_cat_threshold=None,\n",
       "                                                             max_cat_to_onehot=None,\n",
       "                                                             max_delta_step=None,\n",
       "                                                             max_depth=6,\n",
       "                                                             max_leaves=None,\n",
       "                                                             min_child_weight=1,\n",
       "                                                             missing=nan,\n",
       "                                                             monotone_constraints=None,\n",
       "                                                             multi_strategy=None,\n",
       "                                                             n_estimators=1000,\n",
       "                                                             n_jobs=None,\n",
       "                                                             num_parallel_tree=None,\n",
       "                                                             random_state=42, ...))]))],\n",
       "                 voting='soft')"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.ensemble import GradientBoostingClassifier, VotingClassifier\n",
    "from sklearn.metrics import balanced_accuracy_score\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "\n",
    "base_classifiers_1 = [\n",
    "    (\n",
    "        \"mlp\",\n",
    "        make_pipeline(\n",
    "            StandardScaler(),\n",
    "            MLPClassifier(\n",
    "                random_state=SEED,\n",
    "                max_iter=1000,\n",
    "                tol=1e-3,\n",
    "                solver=\"lbfgs\",\n",
    "                hidden_layer_sizes=(100, 300, 200, 100),\n",
    "                alpha=0.001,\n",
    "                early_stopping=True,\n",
    "            ),\n",
    "        ),\n",
    "    ),\n",
    "    (\n",
    "        \"gbc\",\n",
    "        make_pipeline(\n",
    "            StandardScaler(),\n",
    "            GradientBoostingClassifier(\n",
    "                random_state=SEED,\n",
    "                max_features=None,\n",
    "                n_estimators=1000,\n",
    "                max_depth=30,\n",
    "                min_samples_leaf=2,\n",
    "                min_samples_split=5,\n",
    "            ),\n",
    "        ),\n",
    "    ),\n",
    "    (\n",
    "        \"rf\",\n",
    "        make_pipeline(\n",
    "            StandardScaler(),\n",
    "            RandomForestClassifier(\n",
    "                random_state=SEED,\n",
    "                n_estimators=1000,\n",
    "                max_depth=30,\n",
    "                min_samples_leaf=4,\n",
    "                min_samples_split=2,\n",
    "            ),\n",
    "        ),\n",
    "    ),\n",
    "]\n",
    "\n",
    "base_classifiers_2 = [\n",
    "    (\n",
    "        \"mlp\",\n",
    "        make_pipeline(\n",
    "            StandardScaler(),\n",
    "            MLPClassifier(\n",
    "                random_state=SEED,\n",
    "                max_iter=1000,\n",
    "                tol=1e-3,\n",
    "                solver=\"lbfgs\",\n",
    "                hidden_layer_sizes=(100, 300, 200, 100),\n",
    "                alpha=0.001,\n",
    "                early_stopping=True,\n",
    "            ),\n",
    "        ),\n",
    "    ),\n",
    "    (\n",
    "        \"gbc\",\n",
    "        make_pipeline(\n",
    "            StandardScaler(),\n",
    "            GradientBoostingClassifier(\n",
    "                random_state=SEED,\n",
    "                max_features=None,\n",
    "                n_estimators=1000,\n",
    "                max_depth=30,\n",
    "                min_samples_leaf=2,\n",
    "                min_samples_split=5,\n",
    "            ),\n",
    "        ),\n",
    "    ),\n",
    "]\n",
    "\n",
    "param_distributions = {\n",
    "    # Parameters for the first Stacking Layer\n",
    "    \"stacked_ensemble_1__mlp__mlpclassifier__alpha\": uniform(0.0001, 1),\n",
    "    \"stacked_ensemble_1__mlp__mlpclassifier__learning_rate_init\": uniform(0.001, 0.1),\n",
    "    \"stacked_ensemble_1__mlp__mlpclassifier__hidden_layer_sizes\": [\n",
    "        (70, 90),\n",
    "        (70, 80, 90),\n",
    "        (70, 100, 80, 90),\n",
    "    ],\n",
    "    \"stacked_ensemble_1__gbc__gradientboostingclassifier__n_estimators\": [\n",
    "        32,\n",
    "        64,\n",
    "        100,\n",
    "        200,\n",
    "        400,\n",
    "    ],\n",
    "    \"stacked_ensemble_1__gbc__gradientboostingclassifier__max_depth\": [\n",
    "        10,\n",
    "        20,\n",
    "        30,\n",
    "        40,\n",
    "        50,\n",
    "        60,\n",
    "    ],\n",
    "    \"stacked_ensemble_1__gbc__gradientboostingclassifier__min_samples_split\": [\n",
    "        4,\n",
    "        8,\n",
    "        12,\n",
    "        16,\n",
    "        20,\n",
    "    ],\n",
    "    \"stacked_ensemble_1__gbc__gradientboostingclassifier__min_samples_leaf\": [\n",
    "        2,\n",
    "        4,\n",
    "        6,\n",
    "        8,\n",
    "        10,\n",
    "    ],\n",
    "    \"stacked_ensemble_1__rf__randomforestclassifier__n_estimators\": [\n",
    "        200,\n",
    "        400,\n",
    "        600,\n",
    "        800,\n",
    "        1000,\n",
    "        1200,\n",
    "    ],\n",
    "    \"stacked_ensemble_1__rf__randomforestclassifier__max_depth\": [\n",
    "        10,\n",
    "        20,\n",
    "        30,\n",
    "        40,\n",
    "        50,\n",
    "        60,\n",
    "        70,\n",
    "        80,\n",
    "        90,\n",
    "        100,\n",
    "    ],\n",
    "    \"stacked_ensemble_1__rf__randomforestclassifier__min_samples_split\": [2, 3, 4, 5],\n",
    "    \"stacked_ensemble_1__rf__randomforestclassifier__min_samples_leaf\": [4, 6, 10],\n",
    "    # Parameters for the Second Stacking Layer\n",
    "    \"stacked_ensemble_2__mlp__mlpclassifier__alpha\": uniform(0.0001, 1),\n",
    "    \"stacked_ensemble_2__mlp__mlpclassifier__learning_rate_init\": uniform(0.001, 0.1),\n",
    "    \"stacked_ensemble_2__mlp__mlpclassifier__hidden_layer_sizes\": [\n",
    "        (50, 100),\n",
    "        (50, 100, 100),\n",
    "        (50, 150, 100, 100),\n",
    "    ],\n",
    "    \"stacked_ensemble_2__gbc__gradientboostingclassifier__n_estimators\": [\n",
    "        32,\n",
    "        64,\n",
    "        100,\n",
    "        200,\n",
    "        400,\n",
    "    ],\n",
    "    \"stacked_ensemble_2__gbc__gradientboostingclassifier__max_depth\": [\n",
    "        10,\n",
    "        20,\n",
    "        30,\n",
    "        40,\n",
    "        50,\n",
    "        60,\n",
    "    ],\n",
    "    \"stacked_ensemble_2__gbc__gradientboostingclassifier__min_samples_split\": [\n",
    "        4,\n",
    "        8,\n",
    "        12,\n",
    "        16,\n",
    "        20,\n",
    "    ],\n",
    "    \"stacked_ensemble_2__gbc__gradientboostingclassifier__min_samples_leaf\": [\n",
    "        2,\n",
    "        4,\n",
    "        6,\n",
    "        8,\n",
    "        10,\n",
    "    ],\n",
    "    # Parameters for the Final Estimator\n",
    "    \"stacked_ensemble_1__final_estimator__C\": uniform(0.01, 10),\n",
    "    \"stacked_ensemble_2__final_estimator__C\": uniform(0.01, 10),\n",
    "    # Parameters for the Committee\n",
    "    \"gbc__n_estimators\": [32, 64, 100, 200, 400],\n",
    "    \"gbc__max_depth\": [10, 20, 30, 40, 50, 60],\n",
    "    \"gbc__min_samples_split\": [4, 8, 12, 16, 20],\n",
    "    \"gbc__min_samples_leaf\": [2, 4, 6, 8, 10],\n",
    "    \"rf__n_estimators\": [200, 400, 600, 800, 1000, 1200],\n",
    "    \"rf__max_depth\": [10, 20, 30, 40, 50, 60, 70, 80, 90, 100],\n",
    "    \"rf__min_samples_split\": [2, 3, 4, 5],\n",
    "    \"rf__min_samples_leaf\": [4, 6, 10],\n",
    "}\n",
    "\n",
    "# First Stacking Layer\n",
    "stacked_ensamble_1 = StackingClassifier(\n",
    "    estimators=base_classifiers_1, final_estimator=LogisticRegression(), cv=5\n",
    ")\n",
    "\n",
    "# Second Stacking Layer\n",
    "stacked_ensamble_2 = StackingClassifier(\n",
    "    estimators=base_classifiers_2, final_estimator=LogisticRegression(), cv=5\n",
    ")\n",
    "# Define the committee of models\n",
    "committee_models = [\n",
    "    (\"stacked_ensemble_1\", stacked_ensamble_1),\n",
    "    (\"stacked_ensemble_2\", stacked_ensamble_2),\n",
    "    (\n",
    "        \"gbc\",\n",
    "        make_pipeline(\n",
    "            StandardScaler(),\n",
    "            GradientBoostingClassifier(\n",
    "                random_state=SEED,\n",
    "                max_features=None,\n",
    "                n_estimators=1000,\n",
    "                max_depth=30,\n",
    "                min_samples_leaf=2,\n",
    "                min_samples_split=5,\n",
    "            ),\n",
    "        ),\n",
    "    ),\n",
    "    (\n",
    "        \"rf\",\n",
    "        make_pipeline(\n",
    "            StandardScaler(),\n",
    "            RandomForestClassifier(\n",
    "                random_state=SEED,\n",
    "                n_estimators=1000,\n",
    "                max_depth=30,\n",
    "                min_samples_leaf=4,\n",
    "                min_samples_split=2,\n",
    "            ),\n",
    "        ),\n",
    "    ),\n",
    "    (\n",
    "        \"mlp\",\n",
    "        make_pipeline(\n",
    "            StandardScaler(),\n",
    "            MLPClassifier(\n",
    "                random_state=SEED,\n",
    "                max_iter=1000,\n",
    "                tol=1e-3,\n",
    "                solver=\"lbfgs\",\n",
    "                hidden_layer_sizes=(100, 300, 200, 100),\n",
    "                alpha=0.001,\n",
    "                early_stopping=True,\n",
    "            ),\n",
    "        ),\n",
    "    ),\n",
    "    (\"cb\", make_pipeline(StandardScaler(),\n",
    "    CatBoostClassifier(\n",
    "        iterations=1000, \n",
    "        learning_rate=0.03, \n",
    "        depth=6, \n",
    "        l2_leaf_reg=3, \n",
    "        border_count=32,\n",
    "        cat_features=None,  \n",
    "        loss_function='Logloss', \n",
    "        eval_metric='Accuracy',  \n",
    "        random_seed=SEED,\n",
    "        early_stopping_rounds=50,\n",
    "        verbose=100 \n",
    "    ))),\n",
    "    (\"xgb\", make_pipeline( StandardScaler(),\n",
    "    XGBClassifier(\n",
    "        random_state=SEED,\n",
    "        use_label_encoder=False,  \n",
    "        eval_metric=balanced_accuracy_score, \n",
    "        n_estimators=1000,  \n",
    "        learning_rate=0.02, \n",
    "        max_depth=6, \n",
    "        min_child_weight=1,\n",
    "        subsample=0.8, \n",
    "        colsample_bytree=0.8, \n",
    "        gamma=0,  \n",
    "        reg_alpha=0.1, \n",
    "        reg_lambda=1.0,\n",
    "        scale_pos_weight=1, \n",
    "    )\n",
    "))]\n",
    "\n",
    "# Create the committee model\n",
    "committee_model = VotingClassifier(committee_models, voting=\"soft\")\n",
    "committee_model.fit(train_x.copy(), train_y.copy().ravel())\n",
    "# print(committee_model)\n",
    "# Perform randomized search\n",
    "# random_search = RandomizedSearchCV(\n",
    "#     committee_model,\n",
    "#     param_distributions=param_distributions,\n",
    "#     scoring=\"balanced_accuracy\",\n",
    "#     n_iter=1,\n",
    "#     cv=5,\n",
    "#     verbose=4,\n",
    "#     random_state=SEED,\n",
    "#     n_jobs=8,\n",
    "# )\n",
    "\n",
    "# random_search.fit(train_x, train_y.ravel())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model Balanced Accuracy: 0.8195093896126628\n"
     ]
    }
   ],
   "source": [
    "y_pred = committee_model.predict(valid_x)\n",
    "balanced_accuracy = balanced_accuracy_score(valid_y, y_pred)\n",
    "\n",
    "print(f\"Model Balanced Accuracy: {balanced_accuracy}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_path_proba = path.join(OUTPUT_DIR_MANUAL, UNIQUE_ID, \"manual_model_proba.txt\")\n",
    "output_path_model = path.join(OUTPUT_DIR_MANUAL, UNIQUE_ID, \"manual_model.pkl\")\n",
    "dump_proba(committee_model, test_x, output_path_proba)\n",
    "dump_model(committee_model, output_path_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Autogluon"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = np.concatenate((train_x.copy(), train_y.copy()), axis=1)\n",
    "train_data_pd = pd.DataFrame(train_data, copy=True)\n",
    "train_data_pd.rename(columns={train_data_pd.columns[-1]: \"class\"}, inplace=True)\n",
    "\n",
    "valid_data = np.concatenate((valid_x.copy(), valid_y.copy()), axis=1)\n",
    "valid_data_pd = pd.DataFrame(data=valid_data, copy=True)\n",
    "valid_data_pd.rename(columns={valid_data_pd.columns[-1]: \"class\"}, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1600, 26) (400, 26)\n"
     ]
    }
   ],
   "source": [
    "print(train_data_pd.shape, valid_data_pd.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning: path already exists! This predictor may overwrite an existing predictor! path=\"output\\autogluon\\20240114_114214\"\n",
      "Presets specified: ['best_quality']\n",
      "Warning: hyperparameter tuning is currently experimental and may cause the process to hang.\n",
      "Stack configuration (auto_stack=True): num_stack_levels=3, num_bag_folds=15, num_bag_sets=25\n",
      "Dynamic stacking is enabled (dynamic_stacking=True). AutoGluon will try to determine whether the input data is affected by stacked overfitting and enable or disable stacking as a consequence.\n",
      "Detecting stacked overfitting by sub-fitting AutoGluon on the input data. That is, copies of AutoGluon will be sub-fit on subset(s) of the data. Then, the holdout validation data is used to detect stacked overfitting.\n",
      "Sub-fit(s) time limit is: 36000 seconds.\n",
      "Starting holdout-based sub-fit for dynamic stacking. Context path is: output\\autogluon\\20240114_114214/ds_sub_fit/sub_fit_ho.\n",
      "Beginning AutoGluon training ... Time limit = 9000s\n",
      "AutoGluon will save models to \"output\\autogluon\\20240114_114214/ds_sub_fit/sub_fit_ho\"\n",
      "=================== System Info ===================\n",
      "AutoGluon Version:  1.0.0\n",
      "Python Version:     3.9.11\n",
      "Operating System:   Windows\n",
      "Platform Machine:   AMD64\n",
      "Platform Version:   10.0.19041\n",
      "CPU Count:          8\n",
      "Memory Avail:       9.68 GB / 15.95 GB (60.7%)\n",
      "Disk Space Avail:   16.34 GB / 222.52 GB (7.3%)\n",
      "===================================================\n",
      "Train Data Rows:    1422\n",
      "Train Data Columns: 25\n",
      "Label Column:       class\n",
      "Problem Type:       binary\n",
      "Preprocessing data ...\n",
      "Selected class <--> label mapping:  class 1 = 1.0, class 0 = -1.0\n",
      "\tNote: For your binary classification, AutoGluon arbitrarily selected which label-value represents positive (1.0) vs negative (-1.0) class.\n",
      "\tTo explicitly set the positive_class, either rename classes to 1 and 0, or specify positive_class in Predictor init.\n",
      "Using Feature Generators to preprocess the data ...\n",
      "Fitting AutoMLPipelineFeatureGenerator...\n",
      "\tAvailable Memory:                    9915.89 MB\n",
      "\tTrain Data (Original)  Memory Usage: 0.27 MB (0.0% of available memory)\n",
      "\tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n",
      "\tStage 1 Generators:\n",
      "\t\tFitting AsTypeFeatureGenerator...\n",
      "\tStage 2 Generators:\n",
      "\t\tFitting FillNaFeatureGenerator...\n",
      "\tStage 3 Generators:\n",
      "\t\tFitting IdentityFeatureGenerator...\n",
      "\tStage 4 Generators:\n",
      "\t\tFitting DropUniqueFeatureGenerator...\n",
      "\tStage 5 Generators:\n",
      "\t\tFitting DropDuplicatesFeatureGenerator...\n",
      "\tTypes of features in original data (raw dtype, special dtypes):\n",
      "\t\t('float', []) : 25 | ['0', '1', '2', '3', '4', ...]\n",
      "\tTypes of features in processed data (raw dtype, special dtypes):\n",
      "\t\t('float', []) : 25 | ['0', '1', '2', '3', '4', ...]\n",
      "\t0.1s = Fit runtime\n",
      "\t25 features in original data used to generate 25 features in processed data.\n",
      "\tTrain Data (Processed) Memory Usage: 0.27 MB (0.0% of available memory)\n",
      "Data preprocessing and feature engineering runtime = 0.1s ...\n",
      "AutoGluon will gauge predictive performance using evaluation metric: 'balanced_accuracy'\n",
      "\tTo change this, specify the eval_metric parameter of Predictor()\n",
      "User-specified model hyperparameters to be fit:\n",
      "{\n",
      "\t'NN_TORCH': {},\n",
      "\t'GBM': [{'extra_trees': True, 'ag_args': {'name_suffix': 'XT'}}, {}, 'GBMLarge'],\n",
      "\t'CAT': {},\n",
      "\t'XGB': {},\n",
      "\t'FASTAI': {},\n",
      "\t'RF': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
      "\t'XT': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
      "\t'KNN': [{'weights': 'uniform', 'ag_args': {'name_suffix': 'Unif'}}, {'weights': 'distance', 'ag_args': {'name_suffix': 'Dist'}}],\n",
      "}\n",
      "AutoGluon will fit 4 stack levels (L1 to L4) ...\n",
      "Fitting 13 L1 models ...\n",
      "Hyperparameter tuning model: KNeighborsUnif_BAG_L1 ... Tuning model for up to 207.64s of the 8999.89s of remaining time.\n",
      "Warning: Exception caused KNeighborsUnif_BAG_L1 to fail during hyperparameter tuning... Skipping this model.\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\YanPC\\Downloads\\automl_hw2-master\\.venv\\lib\\site-packages\\autogluon\\core\\trainer\\abstract_trainer.py\", line 2135, in _train_single_full\n",
      "    hpo_models, hpo_results = model.hyperparameter_tune(\n",
      "  File \"c:\\Users\\YanPC\\Downloads\\automl_hw2-master\\.venv\\lib\\site-packages\\autogluon\\core\\models\\abstract\\abstract_model.py\", line 1466, in hyperparameter_tune\n",
      "    hpo_executor.initialize(hyperparameter_tune_kwargs, default_num_trials=default_num_trials, time_limit=time_limit)\n",
      "  File \"c:\\Users\\YanPC\\Downloads\\automl_hw2-master\\.venv\\lib\\site-packages\\autogluon\\core\\hpo\\executors.py\", line 489, in initialize\n",
      "    hyperparameter_tune_kwargs = scheduler_factory(hyperparameter_tune_kwargs, num_trials=num_trials, nthreads_per_trial=\"auto\", ngpus_per_trial=\"auto\")\n",
      "  File \"c:\\Users\\YanPC\\Downloads\\automl_hw2-master\\.venv\\lib\\site-packages\\autogluon\\core\\scheduler\\scheduler_factory.py\", line 191, in scheduler_factory\n",
      "    raise ValueError(f\"Required key 'scheduler' is not present in hyperparameter_tune_kwargs: {hyperparameter_tune_kwargs}\")\n",
      "ValueError: Required key 'scheduler' is not present in hyperparameter_tune_kwargs: {'searcher': 'auto', 'time_out': 1200, 'num_trials': 30}\n",
      "Required key 'scheduler' is not present in hyperparameter_tune_kwargs: {'searcher': 'auto', 'time_out': 1200, 'num_trials': 30}\n",
      "Hyperparameter tuning model: KNeighborsDist_BAG_L1 ... Tuning model for up to 207.64s of the 8999.86s of remaining time.\n",
      "Warning: Exception caused KNeighborsDist_BAG_L1 to fail during hyperparameter tuning... Skipping this model.\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\YanPC\\Downloads\\automl_hw2-master\\.venv\\lib\\site-packages\\autogluon\\core\\trainer\\abstract_trainer.py\", line 2135, in _train_single_full\n",
      "    hpo_models, hpo_results = model.hyperparameter_tune(\n",
      "  File \"c:\\Users\\YanPC\\Downloads\\automl_hw2-master\\.venv\\lib\\site-packages\\autogluon\\core\\models\\abstract\\abstract_model.py\", line 1466, in hyperparameter_tune\n",
      "    hpo_executor.initialize(hyperparameter_tune_kwargs, default_num_trials=default_num_trials, time_limit=time_limit)\n",
      "  File \"c:\\Users\\YanPC\\Downloads\\automl_hw2-master\\.venv\\lib\\site-packages\\autogluon\\core\\hpo\\executors.py\", line 489, in initialize\n",
      "    hyperparameter_tune_kwargs = scheduler_factory(hyperparameter_tune_kwargs, num_trials=num_trials, nthreads_per_trial=\"auto\", ngpus_per_trial=\"auto\")\n",
      "  File \"c:\\Users\\YanPC\\Downloads\\automl_hw2-master\\.venv\\lib\\site-packages\\autogluon\\core\\scheduler\\scheduler_factory.py\", line 191, in scheduler_factory\n",
      "    raise ValueError(f\"Required key 'scheduler' is not present in hyperparameter_tune_kwargs: {hyperparameter_tune_kwargs}\")\n",
      "ValueError: Required key 'scheduler' is not present in hyperparameter_tune_kwargs: {'searcher': 'auto', 'time_out': 1200, 'num_trials': 30}\n",
      "Required key 'scheduler' is not present in hyperparameter_tune_kwargs: {'searcher': 'auto', 'time_out': 1200, 'num_trials': 30}\n",
      "Hyperparameter tuning model: LightGBMXT_BAG_L1 ... Tuning model for up to 207.64s of the 8999.85s of remaining time.\n",
      "Warning: Exception caused LightGBMXT_BAG_L1 to fail during hyperparameter tuning... Skipping this model.\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\YanPC\\Downloads\\automl_hw2-master\\.venv\\lib\\site-packages\\autogluon\\core\\trainer\\abstract_trainer.py\", line 2135, in _train_single_full\n",
      "    hpo_models, hpo_results = model.hyperparameter_tune(\n",
      "  File \"c:\\Users\\YanPC\\Downloads\\automl_hw2-master\\.venv\\lib\\site-packages\\autogluon\\core\\models\\abstract\\abstract_model.py\", line 1466, in hyperparameter_tune\n",
      "    hpo_executor.initialize(hyperparameter_tune_kwargs, default_num_trials=default_num_trials, time_limit=time_limit)\n",
      "  File \"c:\\Users\\YanPC\\Downloads\\automl_hw2-master\\.venv\\lib\\site-packages\\autogluon\\core\\hpo\\executors.py\", line 489, in initialize\n",
      "    hyperparameter_tune_kwargs = scheduler_factory(hyperparameter_tune_kwargs, num_trials=num_trials, nthreads_per_trial=\"auto\", ngpus_per_trial=\"auto\")\n",
      "  File \"c:\\Users\\YanPC\\Downloads\\automl_hw2-master\\.venv\\lib\\site-packages\\autogluon\\core\\scheduler\\scheduler_factory.py\", line 191, in scheduler_factory\n",
      "    raise ValueError(f\"Required key 'scheduler' is not present in hyperparameter_tune_kwargs: {hyperparameter_tune_kwargs}\")\n",
      "ValueError: Required key 'scheduler' is not present in hyperparameter_tune_kwargs: {'searcher': 'auto', 'time_out': 1200, 'num_trials': 30}\n",
      "Required key 'scheduler' is not present in hyperparameter_tune_kwargs: {'searcher': 'auto', 'time_out': 1200, 'num_trials': 30}\n",
      "Hyperparameter tuning model: LightGBM_BAG_L1 ... Tuning model for up to 207.64s of the 8999.85s of remaining time.\n",
      "Warning: Exception caused LightGBM_BAG_L1 to fail during hyperparameter tuning... Skipping this model.\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\YanPC\\Downloads\\automl_hw2-master\\.venv\\lib\\site-packages\\autogluon\\core\\trainer\\abstract_trainer.py\", line 2135, in _train_single_full\n",
      "    hpo_models, hpo_results = model.hyperparameter_tune(\n",
      "  File \"c:\\Users\\YanPC\\Downloads\\automl_hw2-master\\.venv\\lib\\site-packages\\autogluon\\core\\models\\abstract\\abstract_model.py\", line 1466, in hyperparameter_tune\n",
      "    hpo_executor.initialize(hyperparameter_tune_kwargs, default_num_trials=default_num_trials, time_limit=time_limit)\n",
      "  File \"c:\\Users\\YanPC\\Downloads\\automl_hw2-master\\.venv\\lib\\site-packages\\autogluon\\core\\hpo\\executors.py\", line 489, in initialize\n",
      "    hyperparameter_tune_kwargs = scheduler_factory(hyperparameter_tune_kwargs, num_trials=num_trials, nthreads_per_trial=\"auto\", ngpus_per_trial=\"auto\")\n",
      "  File \"c:\\Users\\YanPC\\Downloads\\automl_hw2-master\\.venv\\lib\\site-packages\\autogluon\\core\\scheduler\\scheduler_factory.py\", line 191, in scheduler_factory\n",
      "    raise ValueError(f\"Required key 'scheduler' is not present in hyperparameter_tune_kwargs: {hyperparameter_tune_kwargs}\")\n",
      "ValueError: Required key 'scheduler' is not present in hyperparameter_tune_kwargs: {'searcher': 'auto', 'time_out': 1200, 'num_trials': 30}\n",
      "Required key 'scheduler' is not present in hyperparameter_tune_kwargs: {'searcher': 'auto', 'time_out': 1200, 'num_trials': 30}\n",
      "Hyperparameter tuning model: RandomForestGini_BAG_L1 ... Tuning model for up to 207.64s of the 8999.84s of remaining time.\n",
      "Warning: Exception caused RandomForestGini_BAG_L1 to fail during hyperparameter tuning... Skipping this model.\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\YanPC\\Downloads\\automl_hw2-master\\.venv\\lib\\site-packages\\autogluon\\core\\trainer\\abstract_trainer.py\", line 2135, in _train_single_full\n",
      "    hpo_models, hpo_results = model.hyperparameter_tune(\n",
      "  File \"c:\\Users\\YanPC\\Downloads\\automl_hw2-master\\.venv\\lib\\site-packages\\autogluon\\core\\models\\abstract\\abstract_model.py\", line 1466, in hyperparameter_tune\n",
      "    hpo_executor.initialize(hyperparameter_tune_kwargs, default_num_trials=default_num_trials, time_limit=time_limit)\n",
      "  File \"c:\\Users\\YanPC\\Downloads\\automl_hw2-master\\.venv\\lib\\site-packages\\autogluon\\core\\hpo\\executors.py\", line 489, in initialize\n",
      "    hyperparameter_tune_kwargs = scheduler_factory(hyperparameter_tune_kwargs, num_trials=num_trials, nthreads_per_trial=\"auto\", ngpus_per_trial=\"auto\")\n",
      "  File \"c:\\Users\\YanPC\\Downloads\\automl_hw2-master\\.venv\\lib\\site-packages\\autogluon\\core\\scheduler\\scheduler_factory.py\", line 191, in scheduler_factory\n",
      "    raise ValueError(f\"Required key 'scheduler' is not present in hyperparameter_tune_kwargs: {hyperparameter_tune_kwargs}\")\n",
      "ValueError: Required key 'scheduler' is not present in hyperparameter_tune_kwargs: {'searcher': 'auto', 'time_out': 1200, 'num_trials': 30}\n",
      "Required key 'scheduler' is not present in hyperparameter_tune_kwargs: {'searcher': 'auto', 'time_out': 1200, 'num_trials': 30}\n",
      "Hyperparameter tuning model: RandomForestEntr_BAG_L1 ... Tuning model for up to 207.64s of the 8999.83s of remaining time.\n",
      "Warning: Exception caused RandomForestEntr_BAG_L1 to fail during hyperparameter tuning... Skipping this model.\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\YanPC\\Downloads\\automl_hw2-master\\.venv\\lib\\site-packages\\autogluon\\core\\trainer\\abstract_trainer.py\", line 2135, in _train_single_full\n",
      "    hpo_models, hpo_results = model.hyperparameter_tune(\n",
      "  File \"c:\\Users\\YanPC\\Downloads\\automl_hw2-master\\.venv\\lib\\site-packages\\autogluon\\core\\models\\abstract\\abstract_model.py\", line 1466, in hyperparameter_tune\n",
      "    hpo_executor.initialize(hyperparameter_tune_kwargs, default_num_trials=default_num_trials, time_limit=time_limit)\n",
      "  File \"c:\\Users\\YanPC\\Downloads\\automl_hw2-master\\.venv\\lib\\site-packages\\autogluon\\core\\hpo\\executors.py\", line 489, in initialize\n",
      "    hyperparameter_tune_kwargs = scheduler_factory(hyperparameter_tune_kwargs, num_trials=num_trials, nthreads_per_trial=\"auto\", ngpus_per_trial=\"auto\")\n",
      "  File \"c:\\Users\\YanPC\\Downloads\\automl_hw2-master\\.venv\\lib\\site-packages\\autogluon\\core\\scheduler\\scheduler_factory.py\", line 191, in scheduler_factory\n",
      "    raise ValueError(f\"Required key 'scheduler' is not present in hyperparameter_tune_kwargs: {hyperparameter_tune_kwargs}\")\n",
      "ValueError: Required key 'scheduler' is not present in hyperparameter_tune_kwargs: {'searcher': 'auto', 'time_out': 1200, 'num_trials': 30}\n",
      "Required key 'scheduler' is not present in hyperparameter_tune_kwargs: {'searcher': 'auto', 'time_out': 1200, 'num_trials': 30}\n",
      "Hyperparameter tuning model: CatBoost_BAG_L1 ... Tuning model for up to 207.64s of the 8999.83s of remaining time.\n",
      "Warning: Exception caused CatBoost_BAG_L1 to fail during hyperparameter tuning... Skipping this model.\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\YanPC\\Downloads\\automl_hw2-master\\.venv\\lib\\site-packages\\autogluon\\core\\trainer\\abstract_trainer.py\", line 2135, in _train_single_full\n",
      "    hpo_models, hpo_results = model.hyperparameter_tune(\n",
      "  File \"c:\\Users\\YanPC\\Downloads\\automl_hw2-master\\.venv\\lib\\site-packages\\autogluon\\core\\models\\abstract\\abstract_model.py\", line 1466, in hyperparameter_tune\n",
      "    hpo_executor.initialize(hyperparameter_tune_kwargs, default_num_trials=default_num_trials, time_limit=time_limit)\n",
      "  File \"c:\\Users\\YanPC\\Downloads\\automl_hw2-master\\.venv\\lib\\site-packages\\autogluon\\core\\hpo\\executors.py\", line 489, in initialize\n",
      "    hyperparameter_tune_kwargs = scheduler_factory(hyperparameter_tune_kwargs, num_trials=num_trials, nthreads_per_trial=\"auto\", ngpus_per_trial=\"auto\")\n",
      "  File \"c:\\Users\\YanPC\\Downloads\\automl_hw2-master\\.venv\\lib\\site-packages\\autogluon\\core\\scheduler\\scheduler_factory.py\", line 191, in scheduler_factory\n",
      "    raise ValueError(f\"Required key 'scheduler' is not present in hyperparameter_tune_kwargs: {hyperparameter_tune_kwargs}\")\n",
      "ValueError: Required key 'scheduler' is not present in hyperparameter_tune_kwargs: {'searcher': 'auto', 'time_out': 1200, 'num_trials': 30}\n",
      "Required key 'scheduler' is not present in hyperparameter_tune_kwargs: {'searcher': 'auto', 'time_out': 1200, 'num_trials': 30}\n",
      "Hyperparameter tuning model: ExtraTreesGini_BAG_L1 ... Tuning model for up to 207.64s of the 8999.82s of remaining time.\n",
      "Warning: Exception caused ExtraTreesGini_BAG_L1 to fail during hyperparameter tuning... Skipping this model.\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\YanPC\\Downloads\\automl_hw2-master\\.venv\\lib\\site-packages\\autogluon\\core\\trainer\\abstract_trainer.py\", line 2135, in _train_single_full\n",
      "    hpo_models, hpo_results = model.hyperparameter_tune(\n",
      "  File \"c:\\Users\\YanPC\\Downloads\\automl_hw2-master\\.venv\\lib\\site-packages\\autogluon\\core\\models\\abstract\\abstract_model.py\", line 1466, in hyperparameter_tune\n",
      "    hpo_executor.initialize(hyperparameter_tune_kwargs, default_num_trials=default_num_trials, time_limit=time_limit)\n",
      "  File \"c:\\Users\\YanPC\\Downloads\\automl_hw2-master\\.venv\\lib\\site-packages\\autogluon\\core\\hpo\\executors.py\", line 489, in initialize\n",
      "    hyperparameter_tune_kwargs = scheduler_factory(hyperparameter_tune_kwargs, num_trials=num_trials, nthreads_per_trial=\"auto\", ngpus_per_trial=\"auto\")\n",
      "  File \"c:\\Users\\YanPC\\Downloads\\automl_hw2-master\\.venv\\lib\\site-packages\\autogluon\\core\\scheduler\\scheduler_factory.py\", line 191, in scheduler_factory\n",
      "    raise ValueError(f\"Required key 'scheduler' is not present in hyperparameter_tune_kwargs: {hyperparameter_tune_kwargs}\")\n",
      "ValueError: Required key 'scheduler' is not present in hyperparameter_tune_kwargs: {'searcher': 'auto', 'time_out': 1200, 'num_trials': 30}\n",
      "Required key 'scheduler' is not present in hyperparameter_tune_kwargs: {'searcher': 'auto', 'time_out': 1200, 'num_trials': 30}\n",
      "Hyperparameter tuning model: ExtraTreesEntr_BAG_L1 ... Tuning model for up to 207.64s of the 8999.82s of remaining time.\n",
      "Warning: Exception caused ExtraTreesEntr_BAG_L1 to fail during hyperparameter tuning... Skipping this model.\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\YanPC\\Downloads\\automl_hw2-master\\.venv\\lib\\site-packages\\autogluon\\core\\trainer\\abstract_trainer.py\", line 2135, in _train_single_full\n",
      "    hpo_models, hpo_results = model.hyperparameter_tune(\n",
      "  File \"c:\\Users\\YanPC\\Downloads\\automl_hw2-master\\.venv\\lib\\site-packages\\autogluon\\core\\models\\abstract\\abstract_model.py\", line 1466, in hyperparameter_tune\n",
      "    hpo_executor.initialize(hyperparameter_tune_kwargs, default_num_trials=default_num_trials, time_limit=time_limit)\n",
      "  File \"c:\\Users\\YanPC\\Downloads\\automl_hw2-master\\.venv\\lib\\site-packages\\autogluon\\core\\hpo\\executors.py\", line 489, in initialize\n",
      "    hyperparameter_tune_kwargs = scheduler_factory(hyperparameter_tune_kwargs, num_trials=num_trials, nthreads_per_trial=\"auto\", ngpus_per_trial=\"auto\")\n",
      "  File \"c:\\Users\\YanPC\\Downloads\\automl_hw2-master\\.venv\\lib\\site-packages\\autogluon\\core\\scheduler\\scheduler_factory.py\", line 191, in scheduler_factory\n",
      "    raise ValueError(f\"Required key 'scheduler' is not present in hyperparameter_tune_kwargs: {hyperparameter_tune_kwargs}\")\n",
      "ValueError: Required key 'scheduler' is not present in hyperparameter_tune_kwargs: {'searcher': 'auto', 'time_out': 1200, 'num_trials': 30}\n",
      "Required key 'scheduler' is not present in hyperparameter_tune_kwargs: {'searcher': 'auto', 'time_out': 1200, 'num_trials': 30}\n",
      "Hyperparameter tuning model: NeuralNetFastAI_BAG_L1 ... Tuning model for up to 207.64s of the 8999.81s of remaining time.\n",
      "Warning: Exception caused NeuralNetFastAI_BAG_L1 to fail during hyperparameter tuning... Skipping this model.\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\YanPC\\Downloads\\automl_hw2-master\\.venv\\lib\\site-packages\\autogluon\\core\\trainer\\abstract_trainer.py\", line 2135, in _train_single_full\n",
      "    hpo_models, hpo_results = model.hyperparameter_tune(\n",
      "  File \"c:\\Users\\YanPC\\Downloads\\automl_hw2-master\\.venv\\lib\\site-packages\\autogluon\\core\\models\\abstract\\abstract_model.py\", line 1466, in hyperparameter_tune\n",
      "    hpo_executor.initialize(hyperparameter_tune_kwargs, default_num_trials=default_num_trials, time_limit=time_limit)\n",
      "  File \"c:\\Users\\YanPC\\Downloads\\automl_hw2-master\\.venv\\lib\\site-packages\\autogluon\\core\\hpo\\executors.py\", line 336, in initialize\n",
      "    hyperparameter_tune_kwargs[\"scheduler\"], hyperparameter_tune_kwargs[\"scheduler\"]\n",
      "KeyError: 'scheduler'\n",
      "'scheduler'\n",
      "Hyperparameter tuning model: XGBoost_BAG_L1 ... Tuning model for up to 207.64s of the 8999.8s of remaining time.\n",
      "Warning: Exception caused XGBoost_BAG_L1 to fail during hyperparameter tuning... Skipping this model.\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\YanPC\\Downloads\\automl_hw2-master\\.venv\\lib\\site-packages\\autogluon\\core\\trainer\\abstract_trainer.py\", line 2135, in _train_single_full\n",
      "    hpo_models, hpo_results = model.hyperparameter_tune(\n",
      "  File \"c:\\Users\\YanPC\\Downloads\\automl_hw2-master\\.venv\\lib\\site-packages\\autogluon\\core\\models\\abstract\\abstract_model.py\", line 1466, in hyperparameter_tune\n",
      "    hpo_executor.initialize(hyperparameter_tune_kwargs, default_num_trials=default_num_trials, time_limit=time_limit)\n",
      "  File \"c:\\Users\\YanPC\\Downloads\\automl_hw2-master\\.venv\\lib\\site-packages\\autogluon\\core\\hpo\\executors.py\", line 489, in initialize\n",
      "    hyperparameter_tune_kwargs = scheduler_factory(hyperparameter_tune_kwargs, num_trials=num_trials, nthreads_per_trial=\"auto\", ngpus_per_trial=\"auto\")\n",
      "  File \"c:\\Users\\YanPC\\Downloads\\automl_hw2-master\\.venv\\lib\\site-packages\\autogluon\\core\\scheduler\\scheduler_factory.py\", line 191, in scheduler_factory\n",
      "    raise ValueError(f\"Required key 'scheduler' is not present in hyperparameter_tune_kwargs: {hyperparameter_tune_kwargs}\")\n",
      "ValueError: Required key 'scheduler' is not present in hyperparameter_tune_kwargs: {'searcher': 'auto', 'time_out': 1200, 'num_trials': 30}\n",
      "Required key 'scheduler' is not present in hyperparameter_tune_kwargs: {'searcher': 'auto', 'time_out': 1200, 'num_trials': 30}\n",
      "Hyperparameter tuning model: NeuralNetTorch_BAG_L1 ... Tuning model for up to 207.64s of the 8999.8s of remaining time.\n",
      "Warning: Exception caused NeuralNetTorch_BAG_L1 to fail during hyperparameter tuning... Skipping this model.\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\YanPC\\Downloads\\automl_hw2-master\\.venv\\lib\\site-packages\\autogluon\\core\\trainer\\abstract_trainer.py\", line 2135, in _train_single_full\n",
      "    hpo_models, hpo_results = model.hyperparameter_tune(\n",
      "  File \"c:\\Users\\YanPC\\Downloads\\automl_hw2-master\\.venv\\lib\\site-packages\\autogluon\\core\\models\\abstract\\abstract_model.py\", line 1466, in hyperparameter_tune\n",
      "    hpo_executor.initialize(hyperparameter_tune_kwargs, default_num_trials=default_num_trials, time_limit=time_limit)\n",
      "  File \"c:\\Users\\YanPC\\Downloads\\automl_hw2-master\\.venv\\lib\\site-packages\\autogluon\\core\\hpo\\executors.py\", line 336, in initialize\n",
      "    hyperparameter_tune_kwargs[\"scheduler\"], hyperparameter_tune_kwargs[\"scheduler\"]\n",
      "KeyError: 'scheduler'\n",
      "'scheduler'\n",
      "Fitting model: LightGBMLarge_BAG_L1 ... Training model for up to 207.64s of the 8999.79s of remaining time.\n",
      "\tFitting 15 child models (S1F1 - S1F15) | Fitting with ParallelLocalFoldFittingStrategy (8 workers, per: cpus=1, gpus=0, memory=0.21%)\n",
      "\t0.8418\t = Validation score   (balanced_accuracy)\n",
      "\t19.4s\t = Training   runtime\n",
      "\t0.06s\t = Validation runtime\n",
      "Repeating k-fold bagging: 2/25\n",
      "Fitting model: LightGBMLarge_BAG_L1 ... Training model for up to 2963.92s of the 8964.6s of remaining time.\n",
      "\tFitting 15 child models (S2F1 - S2F15) | Fitting with ParallelLocalFoldFittingStrategy (8 workers, per: cpus=1, gpus=0, memory=0.22%)\n",
      "\t0.8214\t = Validation score   (balanced_accuracy)\n",
      "\t42.07s\t = Training   runtime\n",
      "\t0.16s\t = Validation runtime\n",
      "Repeating k-fold bagging: 3/25\n",
      "Fitting model: LightGBMLarge_BAG_L1 ... Training model for up to 2932.39s of the 8933.07s of remaining time.\n",
      "\tFitting 15 child models (S3F1 - S3F15) | Fitting with ParallelLocalFoldFittingStrategy (8 workers, per: cpus=1, gpus=0, memory=0.22%)\n",
      "\t0.8228\t = Validation score   (balanced_accuracy)\n",
      "\t61.72s\t = Training   runtime\n",
      "\t0.22s\t = Validation runtime\n",
      "Repeating k-fold bagging: 4/25\n",
      "Fitting model: LightGBMLarge_BAG_L1 ... Training model for up to 2905.23s of the 8905.91s of remaining time.\n",
      "\tFitting 15 child models (S4F1 - S4F15) | Fitting with ParallelLocalFoldFittingStrategy (8 workers, per: cpus=1, gpus=0, memory=0.22%)\n",
      "\t0.8257\t = Validation score   (balanced_accuracy)\n",
      "\t83.87s\t = Training   runtime\n",
      "\t0.3s\t = Validation runtime\n",
      "Repeating k-fold bagging: 5/25\n",
      "Fitting model: LightGBMLarge_BAG_L1 ... Training model for up to 2874.83s of the 8875.51s of remaining time.\n",
      "\tFitting 15 child models (S5F1 - S5F15) | Fitting with ParallelLocalFoldFittingStrategy (8 workers, per: cpus=1, gpus=0, memory=0.21%)\n",
      "\t0.8264\t = Validation score   (balanced_accuracy)\n",
      "\t105.38s\t = Training   runtime\n",
      "\t0.4s\t = Validation runtime\n",
      "Repeating k-fold bagging: 6/25\n",
      "Fitting model: LightGBMLarge_BAG_L1 ... Training model for up to 2845.76s of the 8846.44s of remaining time.\n",
      "\tFitting 15 child models (S6F1 - S6F15) | Fitting with ParallelLocalFoldFittingStrategy (8 workers, per: cpus=1, gpus=0, memory=0.22%)\n",
      "\t0.8278\t = Validation score   (balanced_accuracy)\n",
      "\t127.39s\t = Training   runtime\n",
      "\t0.48s\t = Validation runtime\n",
      "Repeating k-fold bagging: 7/25\n",
      "Fitting model: LightGBMLarge_BAG_L1 ... Training model for up to 2815.61s of the 8816.29s of remaining time.\n",
      "\tFitting 15 child models (S7F1 - S7F15) | Fitting with ParallelLocalFoldFittingStrategy (8 workers, per: cpus=1, gpus=0, memory=0.21%)\n",
      "\t0.8355\t = Validation score   (balanced_accuracy)\n",
      "\t148.8s\t = Training   runtime\n",
      "\t0.56s\t = Validation runtime\n",
      "Repeating k-fold bagging: 8/25\n",
      "Fitting model: LightGBMLarge_BAG_L1 ... Training model for up to 2786.37s of the 8787.04s of remaining time.\n",
      "\tFitting 15 child models (S8F1 - S8F15) | Fitting with ParallelLocalFoldFittingStrategy (8 workers, per: cpus=1, gpus=0, memory=0.22%)\n",
      "\t0.8348\t = Validation score   (balanced_accuracy)\n",
      "\t169.5s\t = Training   runtime\n",
      "\t0.62s\t = Validation runtime\n",
      "Repeating k-fold bagging: 9/25\n",
      "Fitting model: LightGBMLarge_BAG_L1 ... Training model for up to 2757.91s of the 8758.59s of remaining time.\n",
      "\tFitting 15 child models (S9F1 - S9F15) | Fitting with ParallelLocalFoldFittingStrategy (8 workers, per: cpus=1, gpus=0, memory=0.22%)\n",
      "\t0.8341\t = Validation score   (balanced_accuracy)\n",
      "\t191.57s\t = Training   runtime\n",
      "\t0.7s\t = Validation runtime\n",
      "Repeating k-fold bagging: 10/25\n",
      "Fitting model: LightGBMLarge_BAG_L1 ... Training model for up to 2727.67s of the 8728.35s of remaining time.\n",
      "\tFitting 15 child models (S10F1 - S10F15) | Fitting with ParallelLocalFoldFittingStrategy (8 workers, per: cpus=1, gpus=0, memory=0.21%)\n",
      "\t0.8348\t = Validation score   (balanced_accuracy)\n",
      "\t213.36s\t = Training   runtime\n",
      "\t0.77s\t = Validation runtime\n",
      "Repeating k-fold bagging: 11/25\n",
      "Fitting model: LightGBMLarge_BAG_L1 ... Training model for up to 2697.97s of the 8698.64s of remaining time.\n",
      "\tFitting 15 child models (S11F1 - S11F15) | Fitting with ParallelLocalFoldFittingStrategy (8 workers, per: cpus=1, gpus=0, memory=0.21%)\n",
      "\t0.8334\t = Validation score   (balanced_accuracy)\n",
      "\t236.16s\t = Training   runtime\n",
      "\t0.88s\t = Validation runtime\n",
      "Repeating k-fold bagging: 12/25\n",
      "Fitting model: LightGBMLarge_BAG_L1 ... Training model for up to 2667.16s of the 8667.84s of remaining time.\n",
      "\tFitting 15 child models (S12F1 - S12F15) | Fitting with ParallelLocalFoldFittingStrategy (8 workers, per: cpus=1, gpus=0, memory=0.21%)\n",
      "\t0.8341\t = Validation score   (balanced_accuracy)\n",
      "\t257.95s\t = Training   runtime\n",
      "\t0.96s\t = Validation runtime\n",
      "Repeating k-fold bagging: 13/25\n",
      "Fitting model: LightGBMLarge_BAG_L1 ... Training model for up to 2637.77s of the 8638.45s of remaining time.\n",
      "\tFitting 15 child models (S13F1 - S13F15) | Fitting with ParallelLocalFoldFittingStrategy (8 workers, per: cpus=1, gpus=0, memory=0.22%)\n",
      "\t0.8348\t = Validation score   (balanced_accuracy)\n",
      "\t279.96s\t = Training   runtime\n",
      "\t1.04s\t = Validation runtime\n",
      "Repeating k-fold bagging: 14/25\n",
      "Fitting model: LightGBMLarge_BAG_L1 ... Training model for up to 2607.63s of the 8608.3s of remaining time.\n",
      "\tFitting 15 child models (S14F1 - S14F15) | Fitting with ParallelLocalFoldFittingStrategy (8 workers, per: cpus=1, gpus=0, memory=0.22%)\n",
      "\t0.8369\t = Validation score   (balanced_accuracy)\n",
      "\t301.16s\t = Training   runtime\n",
      "\t1.11s\t = Validation runtime\n",
      "Repeating k-fold bagging: 15/25\n",
      "Fitting model: LightGBMLarge_BAG_L1 ... Training model for up to 2578.45s of the 8579.12s of remaining time.\n",
      "\tFitting 15 child models (S15F1 - S15F15) | Fitting with ParallelLocalFoldFittingStrategy (8 workers, per: cpus=1, gpus=0, memory=0.22%)\n",
      "\t0.8355\t = Validation score   (balanced_accuracy)\n",
      "\t323.54s\t = Training   runtime\n",
      "\t1.21s\t = Validation runtime\n",
      "Repeating k-fold bagging: 16/25\n",
      "Fitting model: LightGBMLarge_BAG_L1 ... Training model for up to 2548.36s of the 8549.04s of remaining time.\n",
      "\tFitting 15 child models (S16F1 - S16F15) | Fitting with ParallelLocalFoldFittingStrategy (8 workers, per: cpus=1, gpus=0, memory=0.21%)\n",
      "\t0.8355\t = Validation score   (balanced_accuracy)\n",
      "\t346.14s\t = Training   runtime\n",
      "\t1.3s\t = Validation runtime\n",
      "Repeating k-fold bagging: 17/25\n",
      "Fitting model: LightGBMLarge_BAG_L1 ... Training model for up to 2517.71s of the 8518.38s of remaining time.\n",
      "\tFitting 15 child models (S17F1 - S17F15) | Fitting with ParallelLocalFoldFittingStrategy (8 workers, per: cpus=1, gpus=0, memory=0.22%)\n",
      "\t0.8355\t = Validation score   (balanced_accuracy)\n",
      "\t368.8s\t = Training   runtime\n",
      "\t1.37s\t = Validation runtime\n",
      "Repeating k-fold bagging: 18/25\n",
      "Fitting model: LightGBMLarge_BAG_L1 ... Training model for up to 2486.91s of the 8487.59s of remaining time.\n",
      "\tFitting 15 child models (S18F1 - S18F15) | Fitting with ParallelLocalFoldFittingStrategy (8 workers, per: cpus=1, gpus=0, memory=0.21%)\n",
      "\t0.8355\t = Validation score   (balanced_accuracy)\n",
      "\t390.95s\t = Training   runtime\n",
      "\t1.44s\t = Validation runtime\n",
      "Repeating k-fold bagging: 19/25\n",
      "Fitting model: LightGBMLarge_BAG_L1 ... Training model for up to 2457.03s of the 8457.71s of remaining time.\n",
      "\tFitting 15 child models (S19F1 - S19F15) | Fitting with ParallelLocalFoldFittingStrategy (8 workers, per: cpus=1, gpus=0, memory=0.22%)\n",
      "\t0.8355\t = Validation score   (balanced_accuracy)\n",
      "\t413.7s\t = Training   runtime\n",
      "\t1.51s\t = Validation runtime\n",
      "Repeating k-fold bagging: 20/25\n",
      "Fitting model: LightGBMLarge_BAG_L1 ... Training model for up to 2426.07s of the 8426.75s of remaining time.\n",
      "\tFitting 15 child models (S20F1 - S20F15) | Fitting with ParallelLocalFoldFittingStrategy (8 workers, per: cpus=1, gpus=0, memory=0.21%)\n",
      "\t0.8362\t = Validation score   (balanced_accuracy)\n",
      "\t435.49s\t = Training   runtime\n",
      "\t1.59s\t = Validation runtime\n",
      "Repeating k-fold bagging: 21/25\n",
      "Fitting model: LightGBMLarge_BAG_L1 ... Training model for up to 2396.29s of the 8396.96s of remaining time.\n",
      "\tFitting 15 child models (S21F1 - S21F15) | Fitting with ParallelLocalFoldFittingStrategy (8 workers, per: cpus=1, gpus=0, memory=0.22%)\n",
      "\t0.8376\t = Validation score   (balanced_accuracy)\n",
      "\t456.31s\t = Training   runtime\n",
      "\t1.65s\t = Validation runtime\n",
      "Repeating k-fold bagging: 22/25\n",
      "Fitting model: LightGBMLarge_BAG_L1 ... Training model for up to 2367.37s of the 8368.05s of remaining time.\n",
      "\tFitting 15 child models (S22F1 - S22F15) | Fitting with ParallelLocalFoldFittingStrategy (8 workers, per: cpus=1, gpus=0, memory=0.22%)\n",
      "\t0.8369\t = Validation score   (balanced_accuracy)\n",
      "\t479.62s\t = Training   runtime\n",
      "\t1.74s\t = Validation runtime\n",
      "Repeating k-fold bagging: 23/25\n",
      "Fitting model: LightGBMLarge_BAG_L1 ... Training model for up to 2336.08s of the 8336.76s of remaining time.\n",
      "\tFitting 15 child models (S23F1 - S23F15) | Fitting with ParallelLocalFoldFittingStrategy (8 workers, per: cpus=1, gpus=0, memory=0.22%)\n",
      "\t0.8369\t = Validation score   (balanced_accuracy)\n",
      "\t500.71s\t = Training   runtime\n",
      "\t1.8s\t = Validation runtime\n",
      "Repeating k-fold bagging: 24/25\n",
      "Fitting model: LightGBMLarge_BAG_L1 ... Training model for up to 2307.18s of the 8307.86s of remaining time.\n",
      "\tFitting 15 child models (S24F1 - S24F15) | Fitting with ParallelLocalFoldFittingStrategy (8 workers, per: cpus=1, gpus=0, memory=0.21%)\n",
      "\t0.8383\t = Validation score   (balanced_accuracy)\n",
      "\t522.55s\t = Training   runtime\n",
      "\t1.89s\t = Validation runtime\n",
      "Repeating k-fold bagging: 25/25\n",
      "Fitting model: LightGBMLarge_BAG_L1 ... Training model for up to 2277.37s of the 8278.04s of remaining time.\n",
      "\tFitting 15 child models (S25F1 - S25F15) | Fitting with ParallelLocalFoldFittingStrategy (8 workers, per: cpus=1, gpus=0, memory=0.22%)\n",
      "\t0.839\t = Validation score   (balanced_accuracy)\n",
      "\t544.9s\t = Training   runtime\n",
      "\t1.95s\t = Validation runtime\n",
      "Completed 25/25 k-fold bagging repeats ...\n",
      "Fitting model: WeightedEnsemble_L2 ... Training model for up to 360.0s of the 8247.62s of remaining time.\n",
      "\tEnsemble Weights: {'LightGBMLarge_BAG_L1': 1.0}\n",
      "\t0.839\t = Validation score   (balanced_accuracy)\n",
      "\t0.01s\t = Training   runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "Fitting 11 L2 models ...\n",
      "Hyperparameter tuning model: LightGBMXT_BAG_L2 ... Tuning model for up to 299.84s of the 8247.58s of remaining time.\n",
      "Warning: Exception caused LightGBMXT_BAG_L2 to fail during hyperparameter tuning... Skipping this model.\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\YanPC\\Downloads\\automl_hw2-master\\.venv\\lib\\site-packages\\autogluon\\core\\trainer\\abstract_trainer.py\", line 2135, in _train_single_full\n",
      "    hpo_models, hpo_results = model.hyperparameter_tune(\n",
      "  File \"c:\\Users\\YanPC\\Downloads\\automl_hw2-master\\.venv\\lib\\site-packages\\autogluon\\core\\models\\abstract\\abstract_model.py\", line 1466, in hyperparameter_tune\n",
      "    hpo_executor.initialize(hyperparameter_tune_kwargs, default_num_trials=default_num_trials, time_limit=time_limit)\n",
      "  File \"c:\\Users\\YanPC\\Downloads\\automl_hw2-master\\.venv\\lib\\site-packages\\autogluon\\core\\hpo\\executors.py\", line 489, in initialize\n",
      "    hyperparameter_tune_kwargs = scheduler_factory(hyperparameter_tune_kwargs, num_trials=num_trials, nthreads_per_trial=\"auto\", ngpus_per_trial=\"auto\")\n",
      "  File \"c:\\Users\\YanPC\\Downloads\\automl_hw2-master\\.venv\\lib\\site-packages\\autogluon\\core\\scheduler\\scheduler_factory.py\", line 191, in scheduler_factory\n",
      "    raise ValueError(f\"Required key 'scheduler' is not present in hyperparameter_tune_kwargs: {hyperparameter_tune_kwargs}\")\n",
      "ValueError: Required key 'scheduler' is not present in hyperparameter_tune_kwargs: {'searcher': 'auto', 'time_out': 1200, 'num_trials': 30}\n",
      "Required key 'scheduler' is not present in hyperparameter_tune_kwargs: {'searcher': 'auto', 'time_out': 1200, 'num_trials': 30}\n",
      "Hyperparameter tuning model: LightGBM_BAG_L2 ... Tuning model for up to 299.84s of the 8247.57s of remaining time.\n",
      "Warning: Exception caused LightGBM_BAG_L2 to fail during hyperparameter tuning... Skipping this model.\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\YanPC\\Downloads\\automl_hw2-master\\.venv\\lib\\site-packages\\autogluon\\core\\trainer\\abstract_trainer.py\", line 2135, in _train_single_full\n",
      "    hpo_models, hpo_results = model.hyperparameter_tune(\n",
      "  File \"c:\\Users\\YanPC\\Downloads\\automl_hw2-master\\.venv\\lib\\site-packages\\autogluon\\core\\models\\abstract\\abstract_model.py\", line 1466, in hyperparameter_tune\n",
      "    hpo_executor.initialize(hyperparameter_tune_kwargs, default_num_trials=default_num_trials, time_limit=time_limit)\n",
      "  File \"c:\\Users\\YanPC\\Downloads\\automl_hw2-master\\.venv\\lib\\site-packages\\autogluon\\core\\hpo\\executors.py\", line 489, in initialize\n",
      "    hyperparameter_tune_kwargs = scheduler_factory(hyperparameter_tune_kwargs, num_trials=num_trials, nthreads_per_trial=\"auto\", ngpus_per_trial=\"auto\")\n",
      "  File \"c:\\Users\\YanPC\\Downloads\\automl_hw2-master\\.venv\\lib\\site-packages\\autogluon\\core\\scheduler\\scheduler_factory.py\", line 191, in scheduler_factory\n",
      "    raise ValueError(f\"Required key 'scheduler' is not present in hyperparameter_tune_kwargs: {hyperparameter_tune_kwargs}\")\n",
      "ValueError: Required key 'scheduler' is not present in hyperparameter_tune_kwargs: {'searcher': 'auto', 'time_out': 1200, 'num_trials': 30}\n",
      "Required key 'scheduler' is not present in hyperparameter_tune_kwargs: {'searcher': 'auto', 'time_out': 1200, 'num_trials': 30}\n",
      "Hyperparameter tuning model: RandomForestGini_BAG_L2 ... Tuning model for up to 299.84s of the 8247.56s of remaining time.\n",
      "Warning: Exception caused RandomForestGini_BAG_L2 to fail during hyperparameter tuning... Skipping this model.\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\YanPC\\Downloads\\automl_hw2-master\\.venv\\lib\\site-packages\\autogluon\\core\\trainer\\abstract_trainer.py\", line 2135, in _train_single_full\n",
      "    hpo_models, hpo_results = model.hyperparameter_tune(\n",
      "  File \"c:\\Users\\YanPC\\Downloads\\automl_hw2-master\\.venv\\lib\\site-packages\\autogluon\\core\\models\\abstract\\abstract_model.py\", line 1466, in hyperparameter_tune\n",
      "    hpo_executor.initialize(hyperparameter_tune_kwargs, default_num_trials=default_num_trials, time_limit=time_limit)\n",
      "  File \"c:\\Users\\YanPC\\Downloads\\automl_hw2-master\\.venv\\lib\\site-packages\\autogluon\\core\\hpo\\executors.py\", line 489, in initialize\n",
      "    hyperparameter_tune_kwargs = scheduler_factory(hyperparameter_tune_kwargs, num_trials=num_trials, nthreads_per_trial=\"auto\", ngpus_per_trial=\"auto\")\n",
      "  File \"c:\\Users\\YanPC\\Downloads\\automl_hw2-master\\.venv\\lib\\site-packages\\autogluon\\core\\scheduler\\scheduler_factory.py\", line 191, in scheduler_factory\n",
      "    raise ValueError(f\"Required key 'scheduler' is not present in hyperparameter_tune_kwargs: {hyperparameter_tune_kwargs}\")\n",
      "ValueError: Required key 'scheduler' is not present in hyperparameter_tune_kwargs: {'searcher': 'auto', 'time_out': 1200, 'num_trials': 30}\n",
      "Required key 'scheduler' is not present in hyperparameter_tune_kwargs: {'searcher': 'auto', 'time_out': 1200, 'num_trials': 30}\n",
      "Hyperparameter tuning model: RandomForestEntr_BAG_L2 ... Tuning model for up to 299.84s of the 8247.55s of remaining time.\n",
      "Warning: Exception caused RandomForestEntr_BAG_L2 to fail during hyperparameter tuning... Skipping this model.\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\YanPC\\Downloads\\automl_hw2-master\\.venv\\lib\\site-packages\\autogluon\\core\\trainer\\abstract_trainer.py\", line 2135, in _train_single_full\n",
      "    hpo_models, hpo_results = model.hyperparameter_tune(\n",
      "  File \"c:\\Users\\YanPC\\Downloads\\automl_hw2-master\\.venv\\lib\\site-packages\\autogluon\\core\\models\\abstract\\abstract_model.py\", line 1466, in hyperparameter_tune\n",
      "    hpo_executor.initialize(hyperparameter_tune_kwargs, default_num_trials=default_num_trials, time_limit=time_limit)\n",
      "  File \"c:\\Users\\YanPC\\Downloads\\automl_hw2-master\\.venv\\lib\\site-packages\\autogluon\\core\\hpo\\executors.py\", line 489, in initialize\n",
      "    hyperparameter_tune_kwargs = scheduler_factory(hyperparameter_tune_kwargs, num_trials=num_trials, nthreads_per_trial=\"auto\", ngpus_per_trial=\"auto\")\n",
      "  File \"c:\\Users\\YanPC\\Downloads\\automl_hw2-master\\.venv\\lib\\site-packages\\autogluon\\core\\scheduler\\scheduler_factory.py\", line 191, in scheduler_factory\n",
      "    raise ValueError(f\"Required key 'scheduler' is not present in hyperparameter_tune_kwargs: {hyperparameter_tune_kwargs}\")\n",
      "ValueError: Required key 'scheduler' is not present in hyperparameter_tune_kwargs: {'searcher': 'auto', 'time_out': 1200, 'num_trials': 30}\n",
      "Required key 'scheduler' is not present in hyperparameter_tune_kwargs: {'searcher': 'auto', 'time_out': 1200, 'num_trials': 30}\n",
      "Hyperparameter tuning model: CatBoost_BAG_L2 ... Tuning model for up to 299.84s of the 8247.51s of remaining time.\n",
      "Warning: Exception caused CatBoost_BAG_L2 to fail during hyperparameter tuning... Skipping this model.\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\YanPC\\Downloads\\automl_hw2-master\\.venv\\lib\\site-packages\\autogluon\\core\\trainer\\abstract_trainer.py\", line 2135, in _train_single_full\n",
      "    hpo_models, hpo_results = model.hyperparameter_tune(\n",
      "  File \"c:\\Users\\YanPC\\Downloads\\automl_hw2-master\\.venv\\lib\\site-packages\\autogluon\\core\\models\\abstract\\abstract_model.py\", line 1466, in hyperparameter_tune\n",
      "    hpo_executor.initialize(hyperparameter_tune_kwargs, default_num_trials=default_num_trials, time_limit=time_limit)\n",
      "  File \"c:\\Users\\YanPC\\Downloads\\automl_hw2-master\\.venv\\lib\\site-packages\\autogluon\\core\\hpo\\executors.py\", line 489, in initialize\n",
      "    hyperparameter_tune_kwargs = scheduler_factory(hyperparameter_tune_kwargs, num_trials=num_trials, nthreads_per_trial=\"auto\", ngpus_per_trial=\"auto\")\n",
      "  File \"c:\\Users\\YanPC\\Downloads\\automl_hw2-master\\.venv\\lib\\site-packages\\autogluon\\core\\scheduler\\scheduler_factory.py\", line 191, in scheduler_factory\n",
      "    raise ValueError(f\"Required key 'scheduler' is not present in hyperparameter_tune_kwargs: {hyperparameter_tune_kwargs}\")\n",
      "ValueError: Required key 'scheduler' is not present in hyperparameter_tune_kwargs: {'searcher': 'auto', 'time_out': 1200, 'num_trials': 30}\n",
      "Required key 'scheduler' is not present in hyperparameter_tune_kwargs: {'searcher': 'auto', 'time_out': 1200, 'num_trials': 30}\n",
      "Hyperparameter tuning model: ExtraTreesGini_BAG_L2 ... Tuning model for up to 299.84s of the 8247.51s of remaining time.\n",
      "Warning: Exception caused ExtraTreesGini_BAG_L2 to fail during hyperparameter tuning... Skipping this model.\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\YanPC\\Downloads\\automl_hw2-master\\.venv\\lib\\site-packages\\autogluon\\core\\trainer\\abstract_trainer.py\", line 2135, in _train_single_full\n",
      "    hpo_models, hpo_results = model.hyperparameter_tune(\n",
      "  File \"c:\\Users\\YanPC\\Downloads\\automl_hw2-master\\.venv\\lib\\site-packages\\autogluon\\core\\models\\abstract\\abstract_model.py\", line 1466, in hyperparameter_tune\n",
      "    hpo_executor.initialize(hyperparameter_tune_kwargs, default_num_trials=default_num_trials, time_limit=time_limit)\n",
      "  File \"c:\\Users\\YanPC\\Downloads\\automl_hw2-master\\.venv\\lib\\site-packages\\autogluon\\core\\hpo\\executors.py\", line 489, in initialize\n",
      "    hyperparameter_tune_kwargs = scheduler_factory(hyperparameter_tune_kwargs, num_trials=num_trials, nthreads_per_trial=\"auto\", ngpus_per_trial=\"auto\")\n",
      "  File \"c:\\Users\\YanPC\\Downloads\\automl_hw2-master\\.venv\\lib\\site-packages\\autogluon\\core\\scheduler\\scheduler_factory.py\", line 191, in scheduler_factory\n",
      "    raise ValueError(f\"Required key 'scheduler' is not present in hyperparameter_tune_kwargs: {hyperparameter_tune_kwargs}\")\n",
      "ValueError: Required key 'scheduler' is not present in hyperparameter_tune_kwargs: {'searcher': 'auto', 'time_out': 1200, 'num_trials': 30}\n",
      "Required key 'scheduler' is not present in hyperparameter_tune_kwargs: {'searcher': 'auto', 'time_out': 1200, 'num_trials': 30}\n",
      "Hyperparameter tuning model: ExtraTreesEntr_BAG_L2 ... Tuning model for up to 299.84s of the 8247.5s of remaining time.\n",
      "Warning: Exception caused ExtraTreesEntr_BAG_L2 to fail during hyperparameter tuning... Skipping this model.\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\YanPC\\Downloads\\automl_hw2-master\\.venv\\lib\\site-packages\\autogluon\\core\\trainer\\abstract_trainer.py\", line 2135, in _train_single_full\n",
      "    hpo_models, hpo_results = model.hyperparameter_tune(\n",
      "  File \"c:\\Users\\YanPC\\Downloads\\automl_hw2-master\\.venv\\lib\\site-packages\\autogluon\\core\\models\\abstract\\abstract_model.py\", line 1466, in hyperparameter_tune\n",
      "    hpo_executor.initialize(hyperparameter_tune_kwargs, default_num_trials=default_num_trials, time_limit=time_limit)\n",
      "  File \"c:\\Users\\YanPC\\Downloads\\automl_hw2-master\\.venv\\lib\\site-packages\\autogluon\\core\\hpo\\executors.py\", line 489, in initialize\n",
      "    hyperparameter_tune_kwargs = scheduler_factory(hyperparameter_tune_kwargs, num_trials=num_trials, nthreads_per_trial=\"auto\", ngpus_per_trial=\"auto\")\n",
      "  File \"c:\\Users\\YanPC\\Downloads\\automl_hw2-master\\.venv\\lib\\site-packages\\autogluon\\core\\scheduler\\scheduler_factory.py\", line 191, in scheduler_factory\n",
      "    raise ValueError(f\"Required key 'scheduler' is not present in hyperparameter_tune_kwargs: {hyperparameter_tune_kwargs}\")\n",
      "ValueError: Required key 'scheduler' is not present in hyperparameter_tune_kwargs: {'searcher': 'auto', 'time_out': 1200, 'num_trials': 30}\n",
      "Required key 'scheduler' is not present in hyperparameter_tune_kwargs: {'searcher': 'auto', 'time_out': 1200, 'num_trials': 30}\n",
      "Hyperparameter tuning model: NeuralNetFastAI_BAG_L2 ... Tuning model for up to 299.84s of the 8247.49s of remaining time.\n",
      "Warning: Exception caused NeuralNetFastAI_BAG_L2 to fail during hyperparameter tuning... Skipping this model.\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\YanPC\\Downloads\\automl_hw2-master\\.venv\\lib\\site-packages\\autogluon\\core\\trainer\\abstract_trainer.py\", line 2135, in _train_single_full\n",
      "    hpo_models, hpo_results = model.hyperparameter_tune(\n",
      "  File \"c:\\Users\\YanPC\\Downloads\\automl_hw2-master\\.venv\\lib\\site-packages\\autogluon\\core\\models\\abstract\\abstract_model.py\", line 1466, in hyperparameter_tune\n",
      "    hpo_executor.initialize(hyperparameter_tune_kwargs, default_num_trials=default_num_trials, time_limit=time_limit)\n",
      "  File \"c:\\Users\\YanPC\\Downloads\\automl_hw2-master\\.venv\\lib\\site-packages\\autogluon\\core\\hpo\\executors.py\", line 336, in initialize\n",
      "    hyperparameter_tune_kwargs[\"scheduler\"], hyperparameter_tune_kwargs[\"scheduler\"]\n",
      "KeyError: 'scheduler'\n",
      "'scheduler'\n",
      "Hyperparameter tuning model: XGBoost_BAG_L2 ... Tuning model for up to 299.84s of the 8247.48s of remaining time.\n",
      "Warning: Exception caused XGBoost_BAG_L2 to fail during hyperparameter tuning... Skipping this model.\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\YanPC\\Downloads\\automl_hw2-master\\.venv\\lib\\site-packages\\autogluon\\core\\trainer\\abstract_trainer.py\", line 2135, in _train_single_full\n",
      "    hpo_models, hpo_results = model.hyperparameter_tune(\n",
      "  File \"c:\\Users\\YanPC\\Downloads\\automl_hw2-master\\.venv\\lib\\site-packages\\autogluon\\core\\models\\abstract\\abstract_model.py\", line 1466, in hyperparameter_tune\n",
      "    hpo_executor.initialize(hyperparameter_tune_kwargs, default_num_trials=default_num_trials, time_limit=time_limit)\n",
      "  File \"c:\\Users\\YanPC\\Downloads\\automl_hw2-master\\.venv\\lib\\site-packages\\autogluon\\core\\hpo\\executors.py\", line 489, in initialize\n",
      "    hyperparameter_tune_kwargs = scheduler_factory(hyperparameter_tune_kwargs, num_trials=num_trials, nthreads_per_trial=\"auto\", ngpus_per_trial=\"auto\")\n",
      "  File \"c:\\Users\\YanPC\\Downloads\\automl_hw2-master\\.venv\\lib\\site-packages\\autogluon\\core\\scheduler\\scheduler_factory.py\", line 191, in scheduler_factory\n",
      "    raise ValueError(f\"Required key 'scheduler' is not present in hyperparameter_tune_kwargs: {hyperparameter_tune_kwargs}\")\n",
      "ValueError: Required key 'scheduler' is not present in hyperparameter_tune_kwargs: {'searcher': 'auto', 'time_out': 1200, 'num_trials': 30}\n",
      "Required key 'scheduler' is not present in hyperparameter_tune_kwargs: {'searcher': 'auto', 'time_out': 1200, 'num_trials': 30}\n",
      "Hyperparameter tuning model: NeuralNetTorch_BAG_L2 ... Tuning model for up to 299.84s of the 8247.47s of remaining time.\n",
      "Warning: Exception caused NeuralNetTorch_BAG_L2 to fail during hyperparameter tuning... Skipping this model.\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\YanPC\\Downloads\\automl_hw2-master\\.venv\\lib\\site-packages\\autogluon\\core\\trainer\\abstract_trainer.py\", line 2135, in _train_single_full\n",
      "    hpo_models, hpo_results = model.hyperparameter_tune(\n",
      "  File \"c:\\Users\\YanPC\\Downloads\\automl_hw2-master\\.venv\\lib\\site-packages\\autogluon\\core\\models\\abstract\\abstract_model.py\", line 1466, in hyperparameter_tune\n",
      "    hpo_executor.initialize(hyperparameter_tune_kwargs, default_num_trials=default_num_trials, time_limit=time_limit)\n",
      "  File \"c:\\Users\\YanPC\\Downloads\\automl_hw2-master\\.venv\\lib\\site-packages\\autogluon\\core\\hpo\\executors.py\", line 336, in initialize\n",
      "    hyperparameter_tune_kwargs[\"scheduler\"], hyperparameter_tune_kwargs[\"scheduler\"]\n",
      "KeyError: 'scheduler'\n",
      "'scheduler'\n",
      "Fitting model: LightGBMLarge_BAG_L2 ... Training model for up to 299.84s of the 8247.46s of remaining time.\n",
      "\tFitting 15 child models (S1F1 - S1F15) | Fitting with ParallelLocalFoldFittingStrategy (8 workers, per: cpus=1, gpus=0, memory=0.22%)\n",
      "\t0.8397\t = Validation score   (balanced_accuracy)\n",
      "\t19.33s\t = Training   runtime\n",
      "\t0.04s\t = Validation runtime\n",
      "Repeating k-fold bagging: 2/25\n",
      "Fitting model: LightGBMLarge_BAG_L2 ... Training model for up to 3636.57s of the 8219.47s of remaining time.\n",
      "\tFitting 15 child models (S2F1 - S2F15) | Fitting with ParallelLocalFoldFittingStrategy (8 workers, per: cpus=1, gpus=0, memory=0.22%)\n",
      "\t0.8263\t = Validation score   (balanced_accuracy)\n",
      "\t39.98s\t = Training   runtime\n",
      "\t0.1s\t = Validation runtime\n",
      "Repeating k-fold bagging: 3/25\n",
      "Fitting model: LightGBMLarge_BAG_L2 ... Training model for up to 3607.63s of the 8190.53s of remaining time.\n",
      "\tFitting 15 child models (S3F1 - S3F15) | Fitting with ParallelLocalFoldFittingStrategy (8 workers, per: cpus=1, gpus=0, memory=0.22%)\n",
      "\t0.8277\t = Validation score   (balanced_accuracy)\n",
      "\t60.76s\t = Training   runtime\n",
      "\t0.15s\t = Validation runtime\n",
      "Repeating k-fold bagging: 4/25\n",
      "Fitting model: LightGBMLarge_BAG_L2 ... Training model for up to 3578.77s of the 8161.67s of remaining time.\n",
      "\tFitting 15 child models (S4F1 - S4F15) | Fitting with ParallelLocalFoldFittingStrategy (8 workers, per: cpus=1, gpus=0, memory=0.22%)\n",
      "\t0.8277\t = Validation score   (balanced_accuracy)\n",
      "\t85.81s\t = Training   runtime\n",
      "\t0.22s\t = Validation runtime\n",
      "Repeating k-fold bagging: 5/25\n",
      "Fitting model: LightGBMLarge_BAG_L2 ... Training model for up to 3545.25s of the 8128.15s of remaining time.\n",
      "\tFitting 15 child models (S5F1 - S5F15) | Fitting with ParallelLocalFoldFittingStrategy (8 workers, per: cpus=1, gpus=0, memory=0.23%)\n",
      "\t0.8277\t = Validation score   (balanced_accuracy)\n",
      "\t113.7s\t = Training   runtime\n",
      "\t0.29s\t = Validation runtime\n",
      "Repeating k-fold bagging: 6/25\n",
      "Fitting model: LightGBMLarge_BAG_L2 ... Training model for up to 3505.3s of the 8088.2s of remaining time.\n",
      "\tFitting 15 child models (S6F1 - S6F15) | Fitting with ParallelLocalFoldFittingStrategy (8 workers, per: cpus=1, gpus=0, memory=0.23%)\n",
      "\t0.8284\t = Validation score   (balanced_accuracy)\n",
      "\t140.22s\t = Training   runtime\n",
      "\t0.37s\t = Validation runtime\n",
      "Repeating k-fold bagging: 7/25\n",
      "Fitting model: LightGBMLarge_BAG_L2 ... Training model for up to 3468.41s of the 8051.31s of remaining time.\n",
      "\tFitting 15 child models (S7F1 - S7F15) | Fitting with ParallelLocalFoldFittingStrategy (8 workers, per: cpus=1, gpus=0, memory=0.23%)\n",
      "\t0.8291\t = Validation score   (balanced_accuracy)\n",
      "\t166.64s\t = Training   runtime\n",
      "\t0.43s\t = Validation runtime\n",
      "Repeating k-fold bagging: 8/25\n",
      "Fitting model: LightGBMLarge_BAG_L2 ... Training model for up to 3431.4s of the 8014.3s of remaining time.\n",
      "\tFitting 15 child models (S8F1 - S8F15) | Fitting with ParallelLocalFoldFittingStrategy (8 workers, per: cpus=1, gpus=0, memory=0.23%)\n",
      "\t0.8312\t = Validation score   (balanced_accuracy)\n",
      "\t192.06s\t = Training   runtime\n",
      "\t0.49s\t = Validation runtime\n",
      "Repeating k-fold bagging: 9/25\n",
      "Fitting model: LightGBMLarge_BAG_L2 ... Training model for up to 3396.08s of the 7978.98s of remaining time.\n",
      "\tFitting 15 child models (S9F1 - S9F15) | Fitting with ParallelLocalFoldFittingStrategy (8 workers, per: cpus=1, gpus=0, memory=0.23%)\n",
      "\t0.8298\t = Validation score   (balanced_accuracy)\n",
      "\t215.72s\t = Training   runtime\n",
      "\t0.54s\t = Validation runtime\n",
      "Repeating k-fold bagging: 10/25\n",
      "Fitting model: LightGBMLarge_BAG_L2 ... Training model for up to 3362.43s of the 7945.33s of remaining time.\n",
      "\tFitting 15 child models (S10F1 - S10F15) | Fitting with ParallelLocalFoldFittingStrategy (8 workers, per: cpus=1, gpus=0, memory=0.23%)\n",
      "\t0.8326\t = Validation score   (balanced_accuracy)\n",
      "\t240.19s\t = Training   runtime\n",
      "\t0.58s\t = Validation runtime\n",
      "Repeating k-fold bagging: 11/25\n",
      "Fitting model: LightGBMLarge_BAG_L2 ... Training model for up to 3327.84s of the 7910.74s of remaining time.\n",
      "\tFitting 15 child models (S11F1 - S11F15) | Fitting with ParallelLocalFoldFittingStrategy (8 workers, per: cpus=1, gpus=0, memory=0.23%)\n",
      "\t0.8319\t = Validation score   (balanced_accuracy)\n",
      "\t263.89s\t = Training   runtime\n",
      "\t0.64s\t = Validation runtime\n",
      "Repeating k-fold bagging: 12/25\n",
      "Fitting model: LightGBMLarge_BAG_L2 ... Training model for up to 3294.23s of the 7877.13s of remaining time.\n",
      "\tFitting 15 child models (S12F1 - S12F15) | Fitting with ParallelLocalFoldFittingStrategy (8 workers, per: cpus=1, gpus=0, memory=0.23%)\n",
      "\t0.8312\t = Validation score   (balanced_accuracy)\n",
      "\t288.13s\t = Training   runtime\n",
      "\t0.69s\t = Validation runtime\n",
      "Repeating k-fold bagging: 13/25\n",
      "Fitting model: LightGBMLarge_BAG_L2 ... Training model for up to 3260.09s of the 7842.99s of remaining time.\n",
      "\tFitting 15 child models (S13F1 - S13F15) | Fitting with ParallelLocalFoldFittingStrategy (8 workers, per: cpus=1, gpus=0, memory=0.23%)\n",
      "\t0.8312\t = Validation score   (balanced_accuracy)\n",
      "\t313.09s\t = Training   runtime\n",
      "\t0.78s\t = Validation runtime\n",
      "Repeating k-fold bagging: 14/25\n",
      "Fitting model: LightGBMLarge_BAG_L2 ... Training model for up to 3225.28s of the 7808.18s of remaining time.\n",
      "\tFitting 15 child models (S14F1 - S14F15) | Fitting with ParallelLocalFoldFittingStrategy (8 workers, per: cpus=1, gpus=0, memory=0.23%)\n",
      "\t0.8312\t = Validation score   (balanced_accuracy)\n",
      "\t338.7s\t = Training   runtime\n",
      "\t0.83s\t = Validation runtime\n",
      "Repeating k-fold bagging: 15/25\n",
      "Fitting model: LightGBMLarge_BAG_L2 ... Training model for up to 3189.23s of the 7772.13s of remaining time.\n",
      "\tFitting 15 child models (S15F1 - S15F15) | Fitting with ParallelLocalFoldFittingStrategy (8 workers, per: cpus=1, gpus=0, memory=0.23%)\n",
      "\t0.8298\t = Validation score   (balanced_accuracy)\n",
      "\t361.64s\t = Training   runtime\n",
      "\t0.87s\t = Validation runtime\n",
      "Repeating k-fold bagging: 16/25\n",
      "Fitting model: LightGBMLarge_BAG_L2 ... Training model for up to 3156.71s of the 7739.61s of remaining time.\n",
      "\tFitting 15 child models (S16F1 - S16F15) | Fitting with ParallelLocalFoldFittingStrategy (8 workers, per: cpus=1, gpus=0, memory=0.24%)\n",
      "\t0.8291\t = Validation score   (balanced_accuracy)\n",
      "\t385.55s\t = Training   runtime\n",
      "\t0.93s\t = Validation runtime\n",
      "Repeating k-fold bagging: 17/25\n",
      "Fitting model: LightGBMLarge_BAG_L2 ... Training model for up to 3122.72s of the 7705.62s of remaining time.\n",
      "\tFitting 15 child models (S17F1 - S17F15) | Fitting with ParallelLocalFoldFittingStrategy (8 workers, per: cpus=1, gpus=0, memory=0.24%)\n",
      "\t0.8298\t = Validation score   (balanced_accuracy)\n",
      "\t411.66s\t = Training   runtime\n",
      "\t0.99s\t = Validation runtime\n",
      "Repeating k-fold bagging: 18/25\n",
      "Fitting model: LightGBMLarge_BAG_L2 ... Training model for up to 3086.7s of the 7669.6s of remaining time.\n",
      "\tFitting 15 child models (S18F1 - S18F15) | Fitting with ParallelLocalFoldFittingStrategy (8 workers, per: cpus=1, gpus=0, memory=0.24%)\n",
      "\t0.8319\t = Validation score   (balanced_accuracy)\n",
      "\t437.28s\t = Training   runtime\n",
      "\t1.05s\t = Validation runtime\n",
      "Repeating k-fold bagging: 19/25\n",
      "Fitting model: LightGBMLarge_BAG_L2 ... Training model for up to 3050.65s of the 7633.55s of remaining time.\n",
      "\tFitting 15 child models (S19F1 - S19F15) | Fitting with ParallelLocalFoldFittingStrategy (8 workers, per: cpus=1, gpus=0, memory=0.23%)\n",
      "\t0.8284\t = Validation score   (balanced_accuracy)\n",
      "\t463.75s\t = Training   runtime\n",
      "\t1.12s\t = Validation runtime\n",
      "Repeating k-fold bagging: 20/25\n",
      "Fitting model: LightGBMLarge_BAG_L2 ... Training model for up to 3014.19s of the 7597.09s of remaining time.\n",
      "\tFitting 15 child models (S20F1 - S20F15) | Fitting with ParallelLocalFoldFittingStrategy (8 workers, per: cpus=1, gpus=0, memory=0.23%)\n",
      "\t0.827\t = Validation score   (balanced_accuracy)\n",
      "\t489.92s\t = Training   runtime\n",
      "\t1.18s\t = Validation runtime\n",
      "Repeating k-fold bagging: 21/25\n",
      "Fitting model: LightGBMLarge_BAG_L2 ... Training model for up to 2977.27s of the 7560.17s of remaining time.\n",
      "\tFitting 15 child models (S21F1 - S21F15) | Fitting with ParallelLocalFoldFittingStrategy (8 workers, per: cpus=1, gpus=0, memory=0.23%)\n",
      "\t0.8284\t = Validation score   (balanced_accuracy)\n",
      "\t513.47s\t = Training   runtime\n",
      "\t1.23s\t = Validation runtime\n",
      "Repeating k-fold bagging: 22/25\n",
      "Fitting model: LightGBMLarge_BAG_L2 ... Training model for up to 2943.6s of the 7526.5s of remaining time.\n",
      "\tFitting 15 child models (S22F1 - S22F15) | Fitting with ParallelLocalFoldFittingStrategy (8 workers, per: cpus=1, gpus=0, memory=0.23%)\n",
      "\t0.827\t = Validation score   (balanced_accuracy)\n",
      "\t538.71s\t = Training   runtime\n",
      "\t1.28s\t = Validation runtime\n",
      "Repeating k-fold bagging: 23/25\n",
      "Fitting model: LightGBMLarge_BAG_L2 ... Training model for up to 2907.66s of the 7490.56s of remaining time.\n",
      "\tFitting 15 child models (S23F1 - S23F15) | Fitting with ParallelLocalFoldFittingStrategy (8 workers, per: cpus=1, gpus=0, memory=0.23%)\n",
      "\t0.8291\t = Validation score   (balanced_accuracy)\n",
      "\t563.87s\t = Training   runtime\n",
      "\t1.35s\t = Validation runtime\n",
      "Repeating k-fold bagging: 24/25\n",
      "Fitting model: LightGBMLarge_BAG_L2 ... Training model for up to 2872.27s of the 7455.17s of remaining time.\n",
      "\tFitting 15 child models (S24F1 - S24F15) | Fitting with ParallelLocalFoldFittingStrategy (8 workers, per: cpus=1, gpus=0, memory=0.23%)\n",
      "\t0.8291\t = Validation score   (balanced_accuracy)\n",
      "\t591.08s\t = Training   runtime\n",
      "\t1.41s\t = Validation runtime\n",
      "Repeating k-fold bagging: 25/25\n",
      "Fitting model: LightGBMLarge_BAG_L2 ... Training model for up to 2834.9s of the 7417.8s of remaining time.\n",
      "\tFitting 15 child models (S25F1 - S25F15) | Fitting with ParallelLocalFoldFittingStrategy (8 workers, per: cpus=1, gpus=0, memory=0.23%)\n",
      "\t0.8291\t = Validation score   (balanced_accuracy)\n",
      "\t614.98s\t = Training   runtime\n",
      "\t1.46s\t = Validation runtime\n",
      "Completed 25/25 k-fold bagging repeats ...\n",
      "Fitting model: WeightedEnsemble_L3 ... Training model for up to 366.47s of the 7384.13s of remaining time.\n",
      "\tEnsemble Weights: {'LightGBMLarge_BAG_L2': 1.0}\n",
      "\t0.8291\t = Validation score   (balanced_accuracy)\n",
      "\t0.01s\t = Training   runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "Fitting 11 L3 models ...\n",
      "Hyperparameter tuning model: LightGBMXT_BAG_L3 ... Tuning model for up to 402.67s of the 7384.07s of remaining time.\n",
      "Warning: Exception caused LightGBMXT_BAG_L3 to fail during hyperparameter tuning... Skipping this model.\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\YanPC\\Downloads\\automl_hw2-master\\.venv\\lib\\site-packages\\autogluon\\core\\trainer\\abstract_trainer.py\", line 2135, in _train_single_full\n",
      "    hpo_models, hpo_results = model.hyperparameter_tune(\n",
      "  File \"c:\\Users\\YanPC\\Downloads\\automl_hw2-master\\.venv\\lib\\site-packages\\autogluon\\core\\models\\abstract\\abstract_model.py\", line 1466, in hyperparameter_tune\n",
      "    hpo_executor.initialize(hyperparameter_tune_kwargs, default_num_trials=default_num_trials, time_limit=time_limit)\n",
      "  File \"c:\\Users\\YanPC\\Downloads\\automl_hw2-master\\.venv\\lib\\site-packages\\autogluon\\core\\hpo\\executors.py\", line 489, in initialize\n",
      "    hyperparameter_tune_kwargs = scheduler_factory(hyperparameter_tune_kwargs, num_trials=num_trials, nthreads_per_trial=\"auto\", ngpus_per_trial=\"auto\")\n",
      "  File \"c:\\Users\\YanPC\\Downloads\\automl_hw2-master\\.venv\\lib\\site-packages\\autogluon\\core\\scheduler\\scheduler_factory.py\", line 191, in scheduler_factory\n",
      "    raise ValueError(f\"Required key 'scheduler' is not present in hyperparameter_tune_kwargs: {hyperparameter_tune_kwargs}\")\n",
      "ValueError: Required key 'scheduler' is not present in hyperparameter_tune_kwargs: {'searcher': 'auto', 'time_out': 1200, 'num_trials': 30}\n",
      "Required key 'scheduler' is not present in hyperparameter_tune_kwargs: {'searcher': 'auto', 'time_out': 1200, 'num_trials': 30}\n",
      "Hyperparameter tuning model: LightGBM_BAG_L3 ... Tuning model for up to 402.67s of the 7384.06s of remaining time.\n",
      "Warning: Exception caused LightGBM_BAG_L3 to fail during hyperparameter tuning... Skipping this model.\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\YanPC\\Downloads\\automl_hw2-master\\.venv\\lib\\site-packages\\autogluon\\core\\trainer\\abstract_trainer.py\", line 2135, in _train_single_full\n",
      "    hpo_models, hpo_results = model.hyperparameter_tune(\n",
      "  File \"c:\\Users\\YanPC\\Downloads\\automl_hw2-master\\.venv\\lib\\site-packages\\autogluon\\core\\models\\abstract\\abstract_model.py\", line 1466, in hyperparameter_tune\n",
      "    hpo_executor.initialize(hyperparameter_tune_kwargs, default_num_trials=default_num_trials, time_limit=time_limit)\n",
      "  File \"c:\\Users\\YanPC\\Downloads\\automl_hw2-master\\.venv\\lib\\site-packages\\autogluon\\core\\hpo\\executors.py\", line 489, in initialize\n",
      "    hyperparameter_tune_kwargs = scheduler_factory(hyperparameter_tune_kwargs, num_trials=num_trials, nthreads_per_trial=\"auto\", ngpus_per_trial=\"auto\")\n",
      "  File \"c:\\Users\\YanPC\\Downloads\\automl_hw2-master\\.venv\\lib\\site-packages\\autogluon\\core\\scheduler\\scheduler_factory.py\", line 191, in scheduler_factory\n",
      "    raise ValueError(f\"Required key 'scheduler' is not present in hyperparameter_tune_kwargs: {hyperparameter_tune_kwargs}\")\n",
      "ValueError: Required key 'scheduler' is not present in hyperparameter_tune_kwargs: {'searcher': 'auto', 'time_out': 1200, 'num_trials': 30}\n",
      "Required key 'scheduler' is not present in hyperparameter_tune_kwargs: {'searcher': 'auto', 'time_out': 1200, 'num_trials': 30}\n",
      "Hyperparameter tuning model: RandomForestGini_BAG_L3 ... Tuning model for up to 402.67s of the 7384.05s of remaining time.\n",
      "Warning: Exception caused RandomForestGini_BAG_L3 to fail during hyperparameter tuning... Skipping this model.\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\YanPC\\Downloads\\automl_hw2-master\\.venv\\lib\\site-packages\\autogluon\\core\\trainer\\abstract_trainer.py\", line 2135, in _train_single_full\n",
      "    hpo_models, hpo_results = model.hyperparameter_tune(\n",
      "  File \"c:\\Users\\YanPC\\Downloads\\automl_hw2-master\\.venv\\lib\\site-packages\\autogluon\\core\\models\\abstract\\abstract_model.py\", line 1466, in hyperparameter_tune\n",
      "    hpo_executor.initialize(hyperparameter_tune_kwargs, default_num_trials=default_num_trials, time_limit=time_limit)\n",
      "  File \"c:\\Users\\YanPC\\Downloads\\automl_hw2-master\\.venv\\lib\\site-packages\\autogluon\\core\\hpo\\executors.py\", line 489, in initialize\n",
      "    hyperparameter_tune_kwargs = scheduler_factory(hyperparameter_tune_kwargs, num_trials=num_trials, nthreads_per_trial=\"auto\", ngpus_per_trial=\"auto\")\n",
      "  File \"c:\\Users\\YanPC\\Downloads\\automl_hw2-master\\.venv\\lib\\site-packages\\autogluon\\core\\scheduler\\scheduler_factory.py\", line 191, in scheduler_factory\n",
      "    raise ValueError(f\"Required key 'scheduler' is not present in hyperparameter_tune_kwargs: {hyperparameter_tune_kwargs}\")\n",
      "ValueError: Required key 'scheduler' is not present in hyperparameter_tune_kwargs: {'searcher': 'auto', 'time_out': 1200, 'num_trials': 30}\n",
      "Required key 'scheduler' is not present in hyperparameter_tune_kwargs: {'searcher': 'auto', 'time_out': 1200, 'num_trials': 30}\n",
      "Hyperparameter tuning model: RandomForestEntr_BAG_L3 ... Tuning model for up to 402.67s of the 7384.04s of remaining time.\n",
      "Warning: Exception caused RandomForestEntr_BAG_L3 to fail during hyperparameter tuning... Skipping this model.\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\YanPC\\Downloads\\automl_hw2-master\\.venv\\lib\\site-packages\\autogluon\\core\\trainer\\abstract_trainer.py\", line 2135, in _train_single_full\n",
      "    hpo_models, hpo_results = model.hyperparameter_tune(\n",
      "  File \"c:\\Users\\YanPC\\Downloads\\automl_hw2-master\\.venv\\lib\\site-packages\\autogluon\\core\\models\\abstract\\abstract_model.py\", line 1466, in hyperparameter_tune\n",
      "    hpo_executor.initialize(hyperparameter_tune_kwargs, default_num_trials=default_num_trials, time_limit=time_limit)\n",
      "  File \"c:\\Users\\YanPC\\Downloads\\automl_hw2-master\\.venv\\lib\\site-packages\\autogluon\\core\\hpo\\executors.py\", line 489, in initialize\n",
      "    hyperparameter_tune_kwargs = scheduler_factory(hyperparameter_tune_kwargs, num_trials=num_trials, nthreads_per_trial=\"auto\", ngpus_per_trial=\"auto\")\n",
      "  File \"c:\\Users\\YanPC\\Downloads\\automl_hw2-master\\.venv\\lib\\site-packages\\autogluon\\core\\scheduler\\scheduler_factory.py\", line 191, in scheduler_factory\n",
      "    raise ValueError(f\"Required key 'scheduler' is not present in hyperparameter_tune_kwargs: {hyperparameter_tune_kwargs}\")\n",
      "ValueError: Required key 'scheduler' is not present in hyperparameter_tune_kwargs: {'searcher': 'auto', 'time_out': 1200, 'num_trials': 30}\n",
      "Required key 'scheduler' is not present in hyperparameter_tune_kwargs: {'searcher': 'auto', 'time_out': 1200, 'num_trials': 30}\n",
      "Hyperparameter tuning model: CatBoost_BAG_L3 ... Tuning model for up to 402.67s of the 7384.02s of remaining time.\n",
      "Warning: Exception caused CatBoost_BAG_L3 to fail during hyperparameter tuning... Skipping this model.\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\YanPC\\Downloads\\automl_hw2-master\\.venv\\lib\\site-packages\\autogluon\\core\\trainer\\abstract_trainer.py\", line 2135, in _train_single_full\n",
      "    hpo_models, hpo_results = model.hyperparameter_tune(\n",
      "  File \"c:\\Users\\YanPC\\Downloads\\automl_hw2-master\\.venv\\lib\\site-packages\\autogluon\\core\\models\\abstract\\abstract_model.py\", line 1466, in hyperparameter_tune\n",
      "    hpo_executor.initialize(hyperparameter_tune_kwargs, default_num_trials=default_num_trials, time_limit=time_limit)\n",
      "  File \"c:\\Users\\YanPC\\Downloads\\automl_hw2-master\\.venv\\lib\\site-packages\\autogluon\\core\\hpo\\executors.py\", line 489, in initialize\n",
      "    hyperparameter_tune_kwargs = scheduler_factory(hyperparameter_tune_kwargs, num_trials=num_trials, nthreads_per_trial=\"auto\", ngpus_per_trial=\"auto\")\n",
      "  File \"c:\\Users\\YanPC\\Downloads\\automl_hw2-master\\.venv\\lib\\site-packages\\autogluon\\core\\scheduler\\scheduler_factory.py\", line 191, in scheduler_factory\n",
      "    raise ValueError(f\"Required key 'scheduler' is not present in hyperparameter_tune_kwargs: {hyperparameter_tune_kwargs}\")\n",
      "ValueError: Required key 'scheduler' is not present in hyperparameter_tune_kwargs: {'searcher': 'auto', 'time_out': 1200, 'num_trials': 30}\n",
      "Required key 'scheduler' is not present in hyperparameter_tune_kwargs: {'searcher': 'auto', 'time_out': 1200, 'num_trials': 30}\n",
      "Hyperparameter tuning model: ExtraTreesGini_BAG_L3 ... Tuning model for up to 402.67s of the 7383.99s of remaining time.\n",
      "Warning: Exception caused ExtraTreesGini_BAG_L3 to fail during hyperparameter tuning... Skipping this model.\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\YanPC\\Downloads\\automl_hw2-master\\.venv\\lib\\site-packages\\autogluon\\core\\trainer\\abstract_trainer.py\", line 2135, in _train_single_full\n",
      "    hpo_models, hpo_results = model.hyperparameter_tune(\n",
      "  File \"c:\\Users\\YanPC\\Downloads\\automl_hw2-master\\.venv\\lib\\site-packages\\autogluon\\core\\models\\abstract\\abstract_model.py\", line 1466, in hyperparameter_tune\n",
      "    hpo_executor.initialize(hyperparameter_tune_kwargs, default_num_trials=default_num_trials, time_limit=time_limit)\n",
      "  File \"c:\\Users\\YanPC\\Downloads\\automl_hw2-master\\.venv\\lib\\site-packages\\autogluon\\core\\hpo\\executors.py\", line 489, in initialize\n",
      "    hyperparameter_tune_kwargs = scheduler_factory(hyperparameter_tune_kwargs, num_trials=num_trials, nthreads_per_trial=\"auto\", ngpus_per_trial=\"auto\")\n",
      "  File \"c:\\Users\\YanPC\\Downloads\\automl_hw2-master\\.venv\\lib\\site-packages\\autogluon\\core\\scheduler\\scheduler_factory.py\", line 191, in scheduler_factory\n",
      "    raise ValueError(f\"Required key 'scheduler' is not present in hyperparameter_tune_kwargs: {hyperparameter_tune_kwargs}\")\n",
      "ValueError: Required key 'scheduler' is not present in hyperparameter_tune_kwargs: {'searcher': 'auto', 'time_out': 1200, 'num_trials': 30}\n",
      "Required key 'scheduler' is not present in hyperparameter_tune_kwargs: {'searcher': 'auto', 'time_out': 1200, 'num_trials': 30}\n",
      "Hyperparameter tuning model: ExtraTreesEntr_BAG_L3 ... Tuning model for up to 402.67s of the 7383.97s of remaining time.\n",
      "Warning: Exception caused ExtraTreesEntr_BAG_L3 to fail during hyperparameter tuning... Skipping this model.\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\YanPC\\Downloads\\automl_hw2-master\\.venv\\lib\\site-packages\\autogluon\\core\\trainer\\abstract_trainer.py\", line 2135, in _train_single_full\n",
      "    hpo_models, hpo_results = model.hyperparameter_tune(\n",
      "  File \"c:\\Users\\YanPC\\Downloads\\automl_hw2-master\\.venv\\lib\\site-packages\\autogluon\\core\\models\\abstract\\abstract_model.py\", line 1466, in hyperparameter_tune\n",
      "    hpo_executor.initialize(hyperparameter_tune_kwargs, default_num_trials=default_num_trials, time_limit=time_limit)\n",
      "  File \"c:\\Users\\YanPC\\Downloads\\automl_hw2-master\\.venv\\lib\\site-packages\\autogluon\\core\\hpo\\executors.py\", line 489, in initialize\n",
      "    hyperparameter_tune_kwargs = scheduler_factory(hyperparameter_tune_kwargs, num_trials=num_trials, nthreads_per_trial=\"auto\", ngpus_per_trial=\"auto\")\n",
      "  File \"c:\\Users\\YanPC\\Downloads\\automl_hw2-master\\.venv\\lib\\site-packages\\autogluon\\core\\scheduler\\scheduler_factory.py\", line 191, in scheduler_factory\n",
      "    raise ValueError(f\"Required key 'scheduler' is not present in hyperparameter_tune_kwargs: {hyperparameter_tune_kwargs}\")\n",
      "ValueError: Required key 'scheduler' is not present in hyperparameter_tune_kwargs: {'searcher': 'auto', 'time_out': 1200, 'num_trials': 30}\n",
      "Required key 'scheduler' is not present in hyperparameter_tune_kwargs: {'searcher': 'auto', 'time_out': 1200, 'num_trials': 30}\n",
      "Hyperparameter tuning model: NeuralNetFastAI_BAG_L3 ... Tuning model for up to 402.67s of the 7383.96s of remaining time.\n",
      "Warning: Exception caused NeuralNetFastAI_BAG_L3 to fail during hyperparameter tuning... Skipping this model.\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\YanPC\\Downloads\\automl_hw2-master\\.venv\\lib\\site-packages\\autogluon\\core\\trainer\\abstract_trainer.py\", line 2135, in _train_single_full\n",
      "    hpo_models, hpo_results = model.hyperparameter_tune(\n",
      "  File \"c:\\Users\\YanPC\\Downloads\\automl_hw2-master\\.venv\\lib\\site-packages\\autogluon\\core\\models\\abstract\\abstract_model.py\", line 1466, in hyperparameter_tune\n",
      "    hpo_executor.initialize(hyperparameter_tune_kwargs, default_num_trials=default_num_trials, time_limit=time_limit)\n",
      "  File \"c:\\Users\\YanPC\\Downloads\\automl_hw2-master\\.venv\\lib\\site-packages\\autogluon\\core\\hpo\\executors.py\", line 336, in initialize\n",
      "    hyperparameter_tune_kwargs[\"scheduler\"], hyperparameter_tune_kwargs[\"scheduler\"]\n",
      "KeyError: 'scheduler'\n",
      "'scheduler'\n",
      "Hyperparameter tuning model: XGBoost_BAG_L3 ... Tuning model for up to 402.67s of the 7383.95s of remaining time.\n",
      "Warning: Exception caused XGBoost_BAG_L3 to fail during hyperparameter tuning... Skipping this model.\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\YanPC\\Downloads\\automl_hw2-master\\.venv\\lib\\site-packages\\autogluon\\core\\trainer\\abstract_trainer.py\", line 2135, in _train_single_full\n",
      "    hpo_models, hpo_results = model.hyperparameter_tune(\n",
      "  File \"c:\\Users\\YanPC\\Downloads\\automl_hw2-master\\.venv\\lib\\site-packages\\autogluon\\core\\models\\abstract\\abstract_model.py\", line 1466, in hyperparameter_tune\n",
      "    hpo_executor.initialize(hyperparameter_tune_kwargs, default_num_trials=default_num_trials, time_limit=time_limit)\n",
      "  File \"c:\\Users\\YanPC\\Downloads\\automl_hw2-master\\.venv\\lib\\site-packages\\autogluon\\core\\hpo\\executors.py\", line 489, in initialize\n",
      "    hyperparameter_tune_kwargs = scheduler_factory(hyperparameter_tune_kwargs, num_trials=num_trials, nthreads_per_trial=\"auto\", ngpus_per_trial=\"auto\")\n",
      "  File \"c:\\Users\\YanPC\\Downloads\\automl_hw2-master\\.venv\\lib\\site-packages\\autogluon\\core\\scheduler\\scheduler_factory.py\", line 191, in scheduler_factory\n",
      "    raise ValueError(f\"Required key 'scheduler' is not present in hyperparameter_tune_kwargs: {hyperparameter_tune_kwargs}\")\n",
      "ValueError: Required key 'scheduler' is not present in hyperparameter_tune_kwargs: {'searcher': 'auto', 'time_out': 1200, 'num_trials': 30}\n",
      "Required key 'scheduler' is not present in hyperparameter_tune_kwargs: {'searcher': 'auto', 'time_out': 1200, 'num_trials': 30}\n",
      "Hyperparameter tuning model: NeuralNetTorch_BAG_L3 ... Tuning model for up to 402.67s of the 7383.94s of remaining time.\n",
      "Warning: Exception caused NeuralNetTorch_BAG_L3 to fail during hyperparameter tuning... Skipping this model.\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\YanPC\\Downloads\\automl_hw2-master\\.venv\\lib\\site-packages\\autogluon\\core\\trainer\\abstract_trainer.py\", line 2135, in _train_single_full\n",
      "    hpo_models, hpo_results = model.hyperparameter_tune(\n",
      "  File \"c:\\Users\\YanPC\\Downloads\\automl_hw2-master\\.venv\\lib\\site-packages\\autogluon\\core\\models\\abstract\\abstract_model.py\", line 1466, in hyperparameter_tune\n",
      "    hpo_executor.initialize(hyperparameter_tune_kwargs, default_num_trials=default_num_trials, time_limit=time_limit)\n",
      "  File \"c:\\Users\\YanPC\\Downloads\\automl_hw2-master\\.venv\\lib\\site-packages\\autogluon\\core\\hpo\\executors.py\", line 336, in initialize\n",
      "    hyperparameter_tune_kwargs[\"scheduler\"], hyperparameter_tune_kwargs[\"scheduler\"]\n",
      "KeyError: 'scheduler'\n",
      "'scheduler'\n",
      "Fitting model: LightGBMLarge_BAG_L3 ... Training model for up to 402.67s of the 7383.93s of remaining time.\n",
      "\tFitting 15 child models (S1F1 - S1F15) | Fitting with ParallelLocalFoldFittingStrategy (8 workers, per: cpus=1, gpus=0, memory=0.23%)\n",
      "\t0.841\t = Validation score   (balanced_accuracy)\n",
      "\t26.38s\t = Training   runtime\n",
      "\t0.07s\t = Validation runtime\n",
      "Repeating k-fold bagging: 2/25\n",
      "Fitting model: LightGBMLarge_BAG_L3 ... Training model for up to 4883.74s of the 7346.32s of remaining time.\n",
      "\tFitting 15 child models (S2F1 - S2F15) | Fitting with ParallelLocalFoldFittingStrategy (8 workers, per: cpus=1, gpus=0, memory=0.23%)\n",
      "\t0.8269\t = Validation score   (balanced_accuracy)\n",
      "\t51.28s\t = Training   runtime\n",
      "\t0.12s\t = Validation runtime\n",
      "Repeating k-fold bagging: 3/25\n",
      "Fitting model: LightGBMLarge_BAG_L3 ... Training model for up to 4848.33s of the 7310.91s of remaining time.\n",
      "\tFitting 15 child models (S3F1 - S3F15) | Fitting with ParallelLocalFoldFittingStrategy (8 workers, per: cpus=1, gpus=0, memory=0.23%)\n",
      "\t0.829\t = Validation score   (balanced_accuracy)\n",
      "\t77.26s\t = Training   runtime\n",
      "\t0.19s\t = Validation runtime\n",
      "Repeating k-fold bagging: 4/25\n",
      "Fitting model: LightGBMLarge_BAG_L3 ... Training model for up to 4812.58s of the 7275.16s of remaining time.\n",
      "\tFitting 15 child models (S4F1 - S4F15) | Fitting with ParallelLocalFoldFittingStrategy (8 workers, per: cpus=1, gpus=0, memory=0.23%)\n",
      "\t0.8311\t = Validation score   (balanced_accuracy)\n",
      "\t101.78s\t = Training   runtime\n",
      "\t0.23s\t = Validation runtime\n",
      "Repeating k-fold bagging: 5/25\n",
      "Fitting model: LightGBMLarge_BAG_L3 ... Training model for up to 4777.46s of the 7240.04s of remaining time.\n",
      "\tFitting 15 child models (S5F1 - S5F15) | Fitting with ParallelLocalFoldFittingStrategy (8 workers, per: cpus=1, gpus=0, memory=0.23%)\n",
      "\t0.8332\t = Validation score   (balanced_accuracy)\n",
      "\t126.85s\t = Training   runtime\n",
      "\t0.28s\t = Validation runtime\n",
      "Repeating k-fold bagging: 6/25\n",
      "Fitting model: LightGBMLarge_BAG_L3 ... Training model for up to 4741.99s of the 7204.57s of remaining time.\n",
      "\tFitting 15 child models (S6F1 - S6F15) | Fitting with ParallelLocalFoldFittingStrategy (8 workers, per: cpus=1, gpus=0, memory=0.23%)\n",
      "\t0.836\t = Validation score   (balanced_accuracy)\n",
      "\t153.0s\t = Training   runtime\n",
      "\t0.35s\t = Validation runtime\n",
      "Repeating k-fold bagging: 7/25\n",
      "Fitting model: LightGBMLarge_BAG_L3 ... Training model for up to 4705.38s of the 7167.96s of remaining time.\n",
      "\tFitting 15 child models (S7F1 - S7F15) | Fitting with ParallelLocalFoldFittingStrategy (8 workers, per: cpus=1, gpus=0, memory=0.22%)\n",
      "\t0.8367\t = Validation score   (balanced_accuracy)\n",
      "\t178.17s\t = Training   runtime\n",
      "\t0.4s\t = Validation runtime\n",
      "Repeating k-fold bagging: 8/25\n",
      "Fitting model: LightGBMLarge_BAG_L3 ... Training model for up to 4669.88s of the 7132.46s of remaining time.\n",
      "\tFitting 15 child models (S8F1 - S8F15) | Fitting with ParallelLocalFoldFittingStrategy (8 workers, per: cpus=1, gpus=0, memory=0.22%)\n",
      "\t0.8346\t = Validation score   (balanced_accuracy)\n",
      "\t204.38s\t = Training   runtime\n",
      "\t0.48s\t = Validation runtime\n",
      "Repeating k-fold bagging: 9/25\n",
      "Fitting model: LightGBMLarge_BAG_L3 ... Training model for up to 4633.65s of the 7096.23s of remaining time.\n",
      "\tFitting 15 child models (S9F1 - S9F15) | Fitting with ParallelLocalFoldFittingStrategy (8 workers, per: cpus=1, gpus=0, memory=0.23%)\n",
      "\t0.8339\t = Validation score   (balanced_accuracy)\n",
      "\t229.24s\t = Training   runtime\n",
      "\t0.52s\t = Validation runtime\n",
      "Repeating k-fold bagging: 10/25\n",
      "Fitting model: LightGBMLarge_BAG_L3 ... Training model for up to 4598.43s of the 7061.02s of remaining time.\n",
      "\tFitting 15 child models (S10F1 - S10F15) | Fitting with ParallelLocalFoldFittingStrategy (8 workers, per: cpus=1, gpus=0, memory=0.23%)\n",
      "\t0.8325\t = Validation score   (balanced_accuracy)\n",
      "\t254.25s\t = Training   runtime\n",
      "\t0.57s\t = Validation runtime\n",
      "Repeating k-fold bagging: 11/25\n",
      "Fitting model: LightGBMLarge_BAG_L3 ... Training model for up to 4562.67s of the 7025.25s of remaining time.\n",
      "\tFitting 15 child models (S11F1 - S11F15) | Fitting with ParallelLocalFoldFittingStrategy (8 workers, per: cpus=1, gpus=0, memory=0.23%)\n",
      "\t0.8325\t = Validation score   (balanced_accuracy)\n",
      "\t280.63s\t = Training   runtime\n",
      "\t0.63s\t = Validation runtime\n",
      "Repeating k-fold bagging: 12/25\n",
      "Fitting model: LightGBMLarge_BAG_L3 ... Training model for up to 4526.58s of the 6989.16s of remaining time.\n",
      "\tFitting 15 child models (S12F1 - S12F15) | Fitting with ParallelLocalFoldFittingStrategy (8 workers, per: cpus=1, gpus=0, memory=0.23%)\n",
      "\t0.8339\t = Validation score   (balanced_accuracy)\n",
      "\t305.27s\t = Training   runtime\n",
      "\t0.68s\t = Validation runtime\n",
      "Repeating k-fold bagging: 13/25\n",
      "Fitting model: LightGBMLarge_BAG_L3 ... Training model for up to 4491.66s of the 6954.24s of remaining time.\n",
      "\tFitting 15 child models (S13F1 - S13F15) | Fitting with ParallelLocalFoldFittingStrategy (8 workers, per: cpus=1, gpus=0, memory=0.23%)\n",
      "\t0.8332\t = Validation score   (balanced_accuracy)\n",
      "\t330.83s\t = Training   runtime\n",
      "\t0.74s\t = Validation runtime\n",
      "Repeating k-fold bagging: 14/25\n",
      "Fitting model: LightGBMLarge_BAG_L3 ... Training model for up to 4456.17s of the 6918.76s of remaining time.\n",
      "\tFitting 15 child models (S14F1 - S14F15) | Fitting with ParallelLocalFoldFittingStrategy (8 workers, per: cpus=1, gpus=0, memory=0.23%)\n",
      "\t0.8339\t = Validation score   (balanced_accuracy)\n",
      "\t355.7s\t = Training   runtime\n",
      "\t0.79s\t = Validation runtime\n",
      "Repeating k-fold bagging: 15/25\n",
      "Fitting model: LightGBMLarge_BAG_L3 ... Training model for up to 4420.67s of the 6883.26s of remaining time.\n",
      "\tFitting 15 child models (S15F1 - S15F15) | Fitting with ParallelLocalFoldFittingStrategy (8 workers, per: cpus=1, gpus=0, memory=0.23%)\n",
      "\t0.8353\t = Validation score   (balanced_accuracy)\n",
      "\t380.53s\t = Training   runtime\n",
      "\t0.84s\t = Validation runtime\n",
      "Repeating k-fold bagging: 16/25\n",
      "Fitting model: LightGBMLarge_BAG_L3 ... Training model for up to 4385.89s of the 6848.47s of remaining time.\n",
      "\tFitting 15 child models (S16F1 - S16F15) | Fitting with ParallelLocalFoldFittingStrategy (8 workers, per: cpus=1, gpus=0, memory=0.23%)\n",
      "\t0.8353\t = Validation score   (balanced_accuracy)\n",
      "\t405.69s\t = Training   runtime\n",
      "\t0.9s\t = Validation runtime\n",
      "Repeating k-fold bagging: 17/25\n",
      "Fitting model: LightGBMLarge_BAG_L3 ... Training model for up to 4350.67s of the 6813.25s of remaining time.\n",
      "\tFitting 15 child models (S17F1 - S17F15) | Fitting with ParallelLocalFoldFittingStrategy (8 workers, per: cpus=1, gpus=0, memory=0.23%)\n",
      "\t0.8346\t = Validation score   (balanced_accuracy)\n",
      "\t429.79s\t = Training   runtime\n",
      "\t0.95s\t = Validation runtime\n",
      "Repeating k-fold bagging: 18/25\n",
      "Fitting model: LightGBMLarge_BAG_L3 ... Training model for up to 4316.57s of the 6779.15s of remaining time.\n",
      "\tFitting 15 child models (S18F1 - S18F15) | Fitting with ParallelLocalFoldFittingStrategy (8 workers, per: cpus=1, gpus=0, memory=0.23%)\n",
      "\t0.8332\t = Validation score   (balanced_accuracy)\n",
      "\t457.83s\t = Training   runtime\n",
      "\t1.02s\t = Validation runtime\n",
      "Repeating k-fold bagging: 19/25\n",
      "Fitting model: LightGBMLarge_BAG_L3 ... Training model for up to 4278.07s of the 6740.65s of remaining time.\n",
      "\tFitting 15 child models (S19F1 - S19F15) | Fitting with ParallelLocalFoldFittingStrategy (8 workers, per: cpus=1, gpus=0, memory=0.23%)\n",
      "\t0.8353\t = Validation score   (balanced_accuracy)\n",
      "\t484.01s\t = Training   runtime\n",
      "\t1.09s\t = Validation runtime\n",
      "Repeating k-fold bagging: 20/25\n",
      "Fitting model: LightGBMLarge_BAG_L3 ... Training model for up to 4241.72s of the 6704.3s of remaining time.\n",
      "\tFitting 15 child models (S20F1 - S20F15) | Fitting with ParallelLocalFoldFittingStrategy (8 workers, per: cpus=1, gpus=0, memory=0.23%)\n",
      "\t0.8332\t = Validation score   (balanced_accuracy)\n",
      "\t512.67s\t = Training   runtime\n",
      "\t1.17s\t = Validation runtime\n",
      "Repeating k-fold bagging: 21/25\n",
      "Fitting model: LightGBMLarge_BAG_L3 ... Training model for up to 4202.72s of the 6665.3s of remaining time.\n",
      "\tFitting 15 child models (S21F1 - S21F15) | Fitting with ParallelLocalFoldFittingStrategy (8 workers, per: cpus=1, gpus=0, memory=0.23%)\n",
      "\t0.8325\t = Validation score   (balanced_accuracy)\n",
      "\t538.29s\t = Training   runtime\n",
      "\t1.22s\t = Validation runtime\n",
      "Repeating k-fold bagging: 22/25\n",
      "Fitting model: LightGBMLarge_BAG_L3 ... Training model for up to 4167.47s of the 6630.05s of remaining time.\n",
      "\tFitting 15 child models (S22F1 - S22F15) | Fitting with ParallelLocalFoldFittingStrategy (8 workers, per: cpus=1, gpus=0, memory=0.23%)\n",
      "\t0.8318\t = Validation score   (balanced_accuracy)\n",
      "\t563.77s\t = Training   runtime\n",
      "\t1.29s\t = Validation runtime\n",
      "Repeating k-fold bagging: 23/25\n",
      "Fitting model: LightGBMLarge_BAG_L3 ... Training model for up to 4132.13s of the 6594.71s of remaining time.\n",
      "\tFitting 15 child models (S23F1 - S23F15) | Fitting with ParallelLocalFoldFittingStrategy (8 workers, per: cpus=1, gpus=0, memory=0.23%)\n",
      "\t0.8325\t = Validation score   (balanced_accuracy)\n",
      "\t589.45s\t = Training   runtime\n",
      "\t1.35s\t = Validation runtime\n",
      "Repeating k-fold bagging: 24/25\n",
      "Fitting model: LightGBMLarge_BAG_L3 ... Training model for up to 4096.42s of the 6559.0s of remaining time.\n",
      "\tFitting 15 child models (S24F1 - S24F15) | Fitting with ParallelLocalFoldFittingStrategy (8 workers, per: cpus=1, gpus=0, memory=0.23%)\n",
      "\t0.8332\t = Validation score   (balanced_accuracy)\n",
      "\t616.73s\t = Training   runtime\n",
      "\t1.41s\t = Validation runtime\n",
      "Repeating k-fold bagging: 25/25\n",
      "Fitting model: LightGBMLarge_BAG_L3 ... Training model for up to 4058.66s of the 6521.24s of remaining time.\n",
      "\tFitting 15 child models (S25F1 - S25F15) | Fitting with ParallelLocalFoldFittingStrategy (8 workers, per: cpus=1, gpus=0, memory=0.23%)\n",
      "\t0.8325\t = Validation score   (balanced_accuracy)\n",
      "\t641.98s\t = Training   runtime\n",
      "\t1.46s\t = Validation runtime\n",
      "Completed 25/25 k-fold bagging repeats ...\n",
      "Fitting model: WeightedEnsemble_L4 ... Training model for up to 492.15s of the 6486.1s of remaining time.\n",
      "\tEnsemble Weights: {'LightGBMLarge_BAG_L3': 1.0}\n",
      "\t0.8325\t = Validation score   (balanced_accuracy)\n",
      "\t0.01s\t = Training   runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "Fitting 11 L4 models ...\n",
      "Hyperparameter tuning model: LightGBMXT_BAG_L4 ... Tuning model for up to 530.68s of the 6486.05s of remaining time.\n",
      "Warning: Exception caused LightGBMXT_BAG_L4 to fail during hyperparameter tuning... Skipping this model.\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\YanPC\\Downloads\\automl_hw2-master\\.venv\\lib\\site-packages\\autogluon\\core\\trainer\\abstract_trainer.py\", line 2135, in _train_single_full\n",
      "    hpo_models, hpo_results = model.hyperparameter_tune(\n",
      "  File \"c:\\Users\\YanPC\\Downloads\\automl_hw2-master\\.venv\\lib\\site-packages\\autogluon\\core\\models\\abstract\\abstract_model.py\", line 1466, in hyperparameter_tune\n",
      "    hpo_executor.initialize(hyperparameter_tune_kwargs, default_num_trials=default_num_trials, time_limit=time_limit)\n",
      "  File \"c:\\Users\\YanPC\\Downloads\\automl_hw2-master\\.venv\\lib\\site-packages\\autogluon\\core\\hpo\\executors.py\", line 489, in initialize\n",
      "    hyperparameter_tune_kwargs = scheduler_factory(hyperparameter_tune_kwargs, num_trials=num_trials, nthreads_per_trial=\"auto\", ngpus_per_trial=\"auto\")\n",
      "  File \"c:\\Users\\YanPC\\Downloads\\automl_hw2-master\\.venv\\lib\\site-packages\\autogluon\\core\\scheduler\\scheduler_factory.py\", line 191, in scheduler_factory\n",
      "    raise ValueError(f\"Required key 'scheduler' is not present in hyperparameter_tune_kwargs: {hyperparameter_tune_kwargs}\")\n",
      "ValueError: Required key 'scheduler' is not present in hyperparameter_tune_kwargs: {'searcher': 'auto', 'time_out': 1200, 'num_trials': 30}\n",
      "Required key 'scheduler' is not present in hyperparameter_tune_kwargs: {'searcher': 'auto', 'time_out': 1200, 'num_trials': 30}\n",
      "Hyperparameter tuning model: LightGBM_BAG_L4 ... Tuning model for up to 530.68s of the 6486.04s of remaining time.\n",
      "Warning: Exception caused LightGBM_BAG_L4 to fail during hyperparameter tuning... Skipping this model.\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\YanPC\\Downloads\\automl_hw2-master\\.venv\\lib\\site-packages\\autogluon\\core\\trainer\\abstract_trainer.py\", line 2135, in _train_single_full\n",
      "    hpo_models, hpo_results = model.hyperparameter_tune(\n",
      "  File \"c:\\Users\\YanPC\\Downloads\\automl_hw2-master\\.venv\\lib\\site-packages\\autogluon\\core\\models\\abstract\\abstract_model.py\", line 1466, in hyperparameter_tune\n",
      "    hpo_executor.initialize(hyperparameter_tune_kwargs, default_num_trials=default_num_trials, time_limit=time_limit)\n",
      "  File \"c:\\Users\\YanPC\\Downloads\\automl_hw2-master\\.venv\\lib\\site-packages\\autogluon\\core\\hpo\\executors.py\", line 489, in initialize\n",
      "    hyperparameter_tune_kwargs = scheduler_factory(hyperparameter_tune_kwargs, num_trials=num_trials, nthreads_per_trial=\"auto\", ngpus_per_trial=\"auto\")\n",
      "  File \"c:\\Users\\YanPC\\Downloads\\automl_hw2-master\\.venv\\lib\\site-packages\\autogluon\\core\\scheduler\\scheduler_factory.py\", line 191, in scheduler_factory\n",
      "    raise ValueError(f\"Required key 'scheduler' is not present in hyperparameter_tune_kwargs: {hyperparameter_tune_kwargs}\")\n",
      "ValueError: Required key 'scheduler' is not present in hyperparameter_tune_kwargs: {'searcher': 'auto', 'time_out': 1200, 'num_trials': 30}\n",
      "Required key 'scheduler' is not present in hyperparameter_tune_kwargs: {'searcher': 'auto', 'time_out': 1200, 'num_trials': 30}\n",
      "Hyperparameter tuning model: RandomForestGini_BAG_L4 ... Tuning model for up to 530.68s of the 6486.03s of remaining time.\n",
      "Warning: Exception caused RandomForestGini_BAG_L4 to fail during hyperparameter tuning... Skipping this model.\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\YanPC\\Downloads\\automl_hw2-master\\.venv\\lib\\site-packages\\autogluon\\core\\trainer\\abstract_trainer.py\", line 2135, in _train_single_full\n",
      "    hpo_models, hpo_results = model.hyperparameter_tune(\n",
      "  File \"c:\\Users\\YanPC\\Downloads\\automl_hw2-master\\.venv\\lib\\site-packages\\autogluon\\core\\models\\abstract\\abstract_model.py\", line 1466, in hyperparameter_tune\n",
      "    hpo_executor.initialize(hyperparameter_tune_kwargs, default_num_trials=default_num_trials, time_limit=time_limit)\n",
      "  File \"c:\\Users\\YanPC\\Downloads\\automl_hw2-master\\.venv\\lib\\site-packages\\autogluon\\core\\hpo\\executors.py\", line 489, in initialize\n",
      "    hyperparameter_tune_kwargs = scheduler_factory(hyperparameter_tune_kwargs, num_trials=num_trials, nthreads_per_trial=\"auto\", ngpus_per_trial=\"auto\")\n",
      "  File \"c:\\Users\\YanPC\\Downloads\\automl_hw2-master\\.venv\\lib\\site-packages\\autogluon\\core\\scheduler\\scheduler_factory.py\", line 191, in scheduler_factory\n",
      "    raise ValueError(f\"Required key 'scheduler' is not present in hyperparameter_tune_kwargs: {hyperparameter_tune_kwargs}\")\n",
      "ValueError: Required key 'scheduler' is not present in hyperparameter_tune_kwargs: {'searcher': 'auto', 'time_out': 1200, 'num_trials': 30}\n",
      "Required key 'scheduler' is not present in hyperparameter_tune_kwargs: {'searcher': 'auto', 'time_out': 1200, 'num_trials': 30}\n",
      "Hyperparameter tuning model: RandomForestEntr_BAG_L4 ... Tuning model for up to 530.68s of the 6486.02s of remaining time.\n",
      "Warning: Exception caused RandomForestEntr_BAG_L4 to fail during hyperparameter tuning... Skipping this model.\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\YanPC\\Downloads\\automl_hw2-master\\.venv\\lib\\site-packages\\autogluon\\core\\trainer\\abstract_trainer.py\", line 2135, in _train_single_full\n",
      "    hpo_models, hpo_results = model.hyperparameter_tune(\n",
      "  File \"c:\\Users\\YanPC\\Downloads\\automl_hw2-master\\.venv\\lib\\site-packages\\autogluon\\core\\models\\abstract\\abstract_model.py\", line 1466, in hyperparameter_tune\n",
      "    hpo_executor.initialize(hyperparameter_tune_kwargs, default_num_trials=default_num_trials, time_limit=time_limit)\n",
      "  File \"c:\\Users\\YanPC\\Downloads\\automl_hw2-master\\.venv\\lib\\site-packages\\autogluon\\core\\hpo\\executors.py\", line 489, in initialize\n",
      "    hyperparameter_tune_kwargs = scheduler_factory(hyperparameter_tune_kwargs, num_trials=num_trials, nthreads_per_trial=\"auto\", ngpus_per_trial=\"auto\")\n",
      "  File \"c:\\Users\\YanPC\\Downloads\\automl_hw2-master\\.venv\\lib\\site-packages\\autogluon\\core\\scheduler\\scheduler_factory.py\", line 191, in scheduler_factory\n",
      "    raise ValueError(f\"Required key 'scheduler' is not present in hyperparameter_tune_kwargs: {hyperparameter_tune_kwargs}\")\n",
      "ValueError: Required key 'scheduler' is not present in hyperparameter_tune_kwargs: {'searcher': 'auto', 'time_out': 1200, 'num_trials': 30}\n",
      "Required key 'scheduler' is not present in hyperparameter_tune_kwargs: {'searcher': 'auto', 'time_out': 1200, 'num_trials': 30}\n",
      "Hyperparameter tuning model: CatBoost_BAG_L4 ... Tuning model for up to 530.68s of the 6486.01s of remaining time.\n",
      "Warning: Exception caused CatBoost_BAG_L4 to fail during hyperparameter tuning... Skipping this model.\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\YanPC\\Downloads\\automl_hw2-master\\.venv\\lib\\site-packages\\autogluon\\core\\trainer\\abstract_trainer.py\", line 2135, in _train_single_full\n",
      "    hpo_models, hpo_results = model.hyperparameter_tune(\n",
      "  File \"c:\\Users\\YanPC\\Downloads\\automl_hw2-master\\.venv\\lib\\site-packages\\autogluon\\core\\models\\abstract\\abstract_model.py\", line 1466, in hyperparameter_tune\n",
      "    hpo_executor.initialize(hyperparameter_tune_kwargs, default_num_trials=default_num_trials, time_limit=time_limit)\n",
      "  File \"c:\\Users\\YanPC\\Downloads\\automl_hw2-master\\.venv\\lib\\site-packages\\autogluon\\core\\hpo\\executors.py\", line 489, in initialize\n",
      "    hyperparameter_tune_kwargs = scheduler_factory(hyperparameter_tune_kwargs, num_trials=num_trials, nthreads_per_trial=\"auto\", ngpus_per_trial=\"auto\")\n",
      "  File \"c:\\Users\\YanPC\\Downloads\\automl_hw2-master\\.venv\\lib\\site-packages\\autogluon\\core\\scheduler\\scheduler_factory.py\", line 191, in scheduler_factory\n",
      "    raise ValueError(f\"Required key 'scheduler' is not present in hyperparameter_tune_kwargs: {hyperparameter_tune_kwargs}\")\n",
      "ValueError: Required key 'scheduler' is not present in hyperparameter_tune_kwargs: {'searcher': 'auto', 'time_out': 1200, 'num_trials': 30}\n",
      "Required key 'scheduler' is not present in hyperparameter_tune_kwargs: {'searcher': 'auto', 'time_out': 1200, 'num_trials': 30}\n",
      "Hyperparameter tuning model: ExtraTreesGini_BAG_L4 ... Tuning model for up to 530.68s of the 6486.0s of remaining time.\n",
      "Warning: Exception caused ExtraTreesGini_BAG_L4 to fail during hyperparameter tuning... Skipping this model.\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\YanPC\\Downloads\\automl_hw2-master\\.venv\\lib\\site-packages\\autogluon\\core\\trainer\\abstract_trainer.py\", line 2135, in _train_single_full\n",
      "    hpo_models, hpo_results = model.hyperparameter_tune(\n",
      "  File \"c:\\Users\\YanPC\\Downloads\\automl_hw2-master\\.venv\\lib\\site-packages\\autogluon\\core\\models\\abstract\\abstract_model.py\", line 1466, in hyperparameter_tune\n",
      "    hpo_executor.initialize(hyperparameter_tune_kwargs, default_num_trials=default_num_trials, time_limit=time_limit)\n",
      "  File \"c:\\Users\\YanPC\\Downloads\\automl_hw2-master\\.venv\\lib\\site-packages\\autogluon\\core\\hpo\\executors.py\", line 489, in initialize\n",
      "    hyperparameter_tune_kwargs = scheduler_factory(hyperparameter_tune_kwargs, num_trials=num_trials, nthreads_per_trial=\"auto\", ngpus_per_trial=\"auto\")\n",
      "  File \"c:\\Users\\YanPC\\Downloads\\automl_hw2-master\\.venv\\lib\\site-packages\\autogluon\\core\\scheduler\\scheduler_factory.py\", line 191, in scheduler_factory\n",
      "    raise ValueError(f\"Required key 'scheduler' is not present in hyperparameter_tune_kwargs: {hyperparameter_tune_kwargs}\")\n",
      "ValueError: Required key 'scheduler' is not present in hyperparameter_tune_kwargs: {'searcher': 'auto', 'time_out': 1200, 'num_trials': 30}\n",
      "Required key 'scheduler' is not present in hyperparameter_tune_kwargs: {'searcher': 'auto', 'time_out': 1200, 'num_trials': 30}\n",
      "Hyperparameter tuning model: ExtraTreesEntr_BAG_L4 ... Tuning model for up to 530.68s of the 6485.96s of remaining time.\n",
      "Warning: Exception caused ExtraTreesEntr_BAG_L4 to fail during hyperparameter tuning... Skipping this model.\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\YanPC\\Downloads\\automl_hw2-master\\.venv\\lib\\site-packages\\autogluon\\core\\trainer\\abstract_trainer.py\", line 2135, in _train_single_full\n",
      "    hpo_models, hpo_results = model.hyperparameter_tune(\n",
      "  File \"c:\\Users\\YanPC\\Downloads\\automl_hw2-master\\.venv\\lib\\site-packages\\autogluon\\core\\models\\abstract\\abstract_model.py\", line 1466, in hyperparameter_tune\n",
      "    hpo_executor.initialize(hyperparameter_tune_kwargs, default_num_trials=default_num_trials, time_limit=time_limit)\n",
      "  File \"c:\\Users\\YanPC\\Downloads\\automl_hw2-master\\.venv\\lib\\site-packages\\autogluon\\core\\hpo\\executors.py\", line 489, in initialize\n",
      "    hyperparameter_tune_kwargs = scheduler_factory(hyperparameter_tune_kwargs, num_trials=num_trials, nthreads_per_trial=\"auto\", ngpus_per_trial=\"auto\")\n",
      "  File \"c:\\Users\\YanPC\\Downloads\\automl_hw2-master\\.venv\\lib\\site-packages\\autogluon\\core\\scheduler\\scheduler_factory.py\", line 191, in scheduler_factory\n",
      "    raise ValueError(f\"Required key 'scheduler' is not present in hyperparameter_tune_kwargs: {hyperparameter_tune_kwargs}\")\n",
      "ValueError: Required key 'scheduler' is not present in hyperparameter_tune_kwargs: {'searcher': 'auto', 'time_out': 1200, 'num_trials': 30}\n",
      "Required key 'scheduler' is not present in hyperparameter_tune_kwargs: {'searcher': 'auto', 'time_out': 1200, 'num_trials': 30}\n",
      "Hyperparameter tuning model: NeuralNetFastAI_BAG_L4 ... Tuning model for up to 530.68s of the 6485.95s of remaining time.\n",
      "Warning: Exception caused NeuralNetFastAI_BAG_L4 to fail during hyperparameter tuning... Skipping this model.\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\YanPC\\Downloads\\automl_hw2-master\\.venv\\lib\\site-packages\\autogluon\\core\\trainer\\abstract_trainer.py\", line 2135, in _train_single_full\n",
      "    hpo_models, hpo_results = model.hyperparameter_tune(\n",
      "  File \"c:\\Users\\YanPC\\Downloads\\automl_hw2-master\\.venv\\lib\\site-packages\\autogluon\\core\\models\\abstract\\abstract_model.py\", line 1466, in hyperparameter_tune\n",
      "    hpo_executor.initialize(hyperparameter_tune_kwargs, default_num_trials=default_num_trials, time_limit=time_limit)\n",
      "  File \"c:\\Users\\YanPC\\Downloads\\automl_hw2-master\\.venv\\lib\\site-packages\\autogluon\\core\\hpo\\executors.py\", line 336, in initialize\n",
      "    hyperparameter_tune_kwargs[\"scheduler\"], hyperparameter_tune_kwargs[\"scheduler\"]\n",
      "KeyError: 'scheduler'\n",
      "'scheduler'\n",
      "Hyperparameter tuning model: XGBoost_BAG_L4 ... Tuning model for up to 530.68s of the 6485.94s of remaining time.\n",
      "Warning: Exception caused XGBoost_BAG_L4 to fail during hyperparameter tuning... Skipping this model.\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\YanPC\\Downloads\\automl_hw2-master\\.venv\\lib\\site-packages\\autogluon\\core\\trainer\\abstract_trainer.py\", line 2135, in _train_single_full\n",
      "    hpo_models, hpo_results = model.hyperparameter_tune(\n",
      "  File \"c:\\Users\\YanPC\\Downloads\\automl_hw2-master\\.venv\\lib\\site-packages\\autogluon\\core\\models\\abstract\\abstract_model.py\", line 1466, in hyperparameter_tune\n",
      "    hpo_executor.initialize(hyperparameter_tune_kwargs, default_num_trials=default_num_trials, time_limit=time_limit)\n",
      "  File \"c:\\Users\\YanPC\\Downloads\\automl_hw2-master\\.venv\\lib\\site-packages\\autogluon\\core\\hpo\\executors.py\", line 489, in initialize\n",
      "    hyperparameter_tune_kwargs = scheduler_factory(hyperparameter_tune_kwargs, num_trials=num_trials, nthreads_per_trial=\"auto\", ngpus_per_trial=\"auto\")\n",
      "  File \"c:\\Users\\YanPC\\Downloads\\automl_hw2-master\\.venv\\lib\\site-packages\\autogluon\\core\\scheduler\\scheduler_factory.py\", line 191, in scheduler_factory\n",
      "    raise ValueError(f\"Required key 'scheduler' is not present in hyperparameter_tune_kwargs: {hyperparameter_tune_kwargs}\")\n",
      "ValueError: Required key 'scheduler' is not present in hyperparameter_tune_kwargs: {'searcher': 'auto', 'time_out': 1200, 'num_trials': 30}\n",
      "Required key 'scheduler' is not present in hyperparameter_tune_kwargs: {'searcher': 'auto', 'time_out': 1200, 'num_trials': 30}\n",
      "Hyperparameter tuning model: NeuralNetTorch_BAG_L4 ... Tuning model for up to 530.68s of the 6485.91s of remaining time.\n",
      "Warning: Exception caused NeuralNetTorch_BAG_L4 to fail during hyperparameter tuning... Skipping this model.\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\YanPC\\Downloads\\automl_hw2-master\\.venv\\lib\\site-packages\\autogluon\\core\\trainer\\abstract_trainer.py\", line 2135, in _train_single_full\n",
      "    hpo_models, hpo_results = model.hyperparameter_tune(\n",
      "  File \"c:\\Users\\YanPC\\Downloads\\automl_hw2-master\\.venv\\lib\\site-packages\\autogluon\\core\\models\\abstract\\abstract_model.py\", line 1466, in hyperparameter_tune\n",
      "    hpo_executor.initialize(hyperparameter_tune_kwargs, default_num_trials=default_num_trials, time_limit=time_limit)\n",
      "  File \"c:\\Users\\YanPC\\Downloads\\automl_hw2-master\\.venv\\lib\\site-packages\\autogluon\\core\\hpo\\executors.py\", line 336, in initialize\n",
      "    hyperparameter_tune_kwargs[\"scheduler\"], hyperparameter_tune_kwargs[\"scheduler\"]\n",
      "KeyError: 'scheduler'\n",
      "'scheduler'\n",
      "Fitting model: LightGBMLarge_BAG_L4 ... Training model for up to 530.68s of the 6485.89s of remaining time.\n",
      "\tFitting 15 child models (S1F1 - S1F15) | Fitting with ParallelLocalFoldFittingStrategy (8 workers, per: cpus=1, gpus=0, memory=0.23%)\n",
      "\t0.8389\t = Validation score   (balanced_accuracy)\n",
      "\t28.51s\t = Training   runtime\n",
      "\t0.06s\t = Validation runtime\n",
      "Repeating k-fold bagging: 2/25\n",
      "Fitting model: LightGBMLarge_BAG_L4 ... Training model for up to 6446.79s of the 6446.78s of remaining time.\n",
      "\tFitting 15 child models (S2F1 - S2F15) | Fitting with ParallelLocalFoldFittingStrategy (8 workers, per: cpus=1, gpus=0, memory=0.23%)\n",
      "\t0.8248\t = Validation score   (balanced_accuracy)\n",
      "\t54.41s\t = Training   runtime\n",
      "\t0.12s\t = Validation runtime\n",
      "Repeating k-fold bagging: 3/25\n",
      "Fitting model: LightGBMLarge_BAG_L4 ... Training model for up to 6408.78s of the 6408.77s of remaining time.\n",
      "\tFitting 15 child models (S3F1 - S3F15) | Fitting with ParallelLocalFoldFittingStrategy (8 workers, per: cpus=1, gpus=0, memory=0.23%)\n",
      "\t0.8297\t = Validation score   (balanced_accuracy)\n",
      "\t79.79s\t = Training   runtime\n",
      "\t0.17s\t = Validation runtime\n",
      "Repeating k-fold bagging: 4/25\n",
      "Fitting model: LightGBMLarge_BAG_L4 ... Training model for up to 6372.58s of the 6372.57s of remaining time.\n",
      "\tFitting 15 child models (S4F1 - S4F15) | Fitting with ParallelLocalFoldFittingStrategy (8 workers, per: cpus=1, gpus=0, memory=0.23%)\n",
      "\t0.8304\t = Validation score   (balanced_accuracy)\n",
      "\t104.91s\t = Training   runtime\n",
      "\t0.25s\t = Validation runtime\n",
      "Repeating k-fold bagging: 5/25\n",
      "Fitting model: LightGBMLarge_BAG_L4 ... Training model for up to 6337.19s of the 6337.18s of remaining time.\n",
      "\tFitting 15 child models (S5F1 - S5F15) | Fitting with ParallelLocalFoldFittingStrategy (8 workers, per: cpus=1, gpus=0, memory=0.23%)\n",
      "\t0.8297\t = Validation score   (balanced_accuracy)\n",
      "\t131.56s\t = Training   runtime\n",
      "\t0.31s\t = Validation runtime\n",
      "Repeating k-fold bagging: 6/25\n",
      "Fitting model: LightGBMLarge_BAG_L4 ... Training model for up to 6299.93s of the 6299.92s of remaining time.\n",
      "\tFitting 15 child models (S6F1 - S6F15) | Fitting with ParallelLocalFoldFittingStrategy (8 workers, per: cpus=1, gpus=0, memory=0.23%)\n",
      "\t0.8304\t = Validation score   (balanced_accuracy)\n",
      "\t155.59s\t = Training   runtime\n",
      "\t0.35s\t = Validation runtime\n",
      "Repeating k-fold bagging: 7/25\n",
      "Fitting model: LightGBMLarge_BAG_L4 ... Training model for up to 6265.97s of the 6265.96s of remaining time.\n",
      "\tFitting 15 child models (S7F1 - S7F15) | Fitting with ParallelLocalFoldFittingStrategy (8 workers, per: cpus=1, gpus=0, memory=0.23%)\n",
      "\t0.8304\t = Validation score   (balanced_accuracy)\n",
      "\t180.83s\t = Training   runtime\n",
      "\t0.41s\t = Validation runtime\n",
      "Repeating k-fold bagging: 8/25\n",
      "Fitting model: LightGBMLarge_BAG_L4 ... Training model for up to 6230.29s of the 6230.28s of remaining time.\n",
      "\tFitting 15 child models (S8F1 - S8F15) | Fitting with ParallelLocalFoldFittingStrategy (8 workers, per: cpus=1, gpus=0, memory=0.23%)\n",
      "\t0.8318\t = Validation score   (balanced_accuracy)\n",
      "\t204.99s\t = Training   runtime\n",
      "\t0.46s\t = Validation runtime\n",
      "Repeating k-fold bagging: 9/25\n",
      "Fitting model: LightGBMLarge_BAG_L4 ... Training model for up to 6195.15s of the 6195.15s of remaining time.\n",
      "\tFitting 15 child models (S9F1 - S9F15) | Fitting with ParallelLocalFoldFittingStrategy (8 workers, per: cpus=1, gpus=0, memory=0.23%)\n",
      "\t0.8339\t = Validation score   (balanced_accuracy)\n",
      "\t231.32s\t = Training   runtime\n",
      "\t0.51s\t = Validation runtime\n",
      "Repeating k-fold bagging: 10/25\n",
      "Fitting model: LightGBMLarge_BAG_L4 ... Training model for up to 6157.99s of the 6157.98s of remaining time.\n",
      "\tFitting 15 child models (S10F1 - S10F15) | Fitting with ParallelLocalFoldFittingStrategy (8 workers, per: cpus=1, gpus=0, memory=0.22%)\n",
      "\t0.8339\t = Validation score   (balanced_accuracy)\n",
      "\t255.51s\t = Training   runtime\n",
      "\t0.56s\t = Validation runtime\n",
      "Repeating k-fold bagging: 11/25\n",
      "Fitting model: LightGBMLarge_BAG_L4 ... Training model for up to 6123.72s of the 6123.71s of remaining time.\n",
      "\tFitting 15 child models (S11F1 - S11F15) | Fitting with ParallelLocalFoldFittingStrategy (8 workers, per: cpus=1, gpus=0, memory=0.23%)\n",
      "\t0.8332\t = Validation score   (balanced_accuracy)\n",
      "\t281.76s\t = Training   runtime\n",
      "\t0.61s\t = Validation runtime\n",
      "Repeating k-fold bagging: 12/25\n",
      "Fitting model: LightGBMLarge_BAG_L4 ... Training model for up to 6086.77s of the 6086.76s of remaining time.\n",
      "\tFitting 15 child models (S12F1 - S12F15) | Fitting with ParallelLocalFoldFittingStrategy (8 workers, per: cpus=1, gpus=0, memory=0.23%)\n",
      "\t0.8318\t = Validation score   (balanced_accuracy)\n",
      "\t309.0s\t = Training   runtime\n",
      "\t0.66s\t = Validation runtime\n",
      "Repeating k-fold bagging: 13/25\n",
      "Fitting model: LightGBMLarge_BAG_L4 ... Training model for up to 6049.29s of the 6049.28s of remaining time.\n",
      "\tFitting 15 child models (S13F1 - S13F15) | Fitting with ParallelLocalFoldFittingStrategy (8 workers, per: cpus=1, gpus=0, memory=0.23%)\n",
      "\t0.8311\t = Validation score   (balanced_accuracy)\n",
      "\t334.49s\t = Training   runtime\n",
      "\t0.72s\t = Validation runtime\n",
      "Repeating k-fold bagging: 14/25\n",
      "Fitting model: LightGBMLarge_BAG_L4 ... Training model for up to 6013.39s of the 6013.38s of remaining time.\n",
      "\tFitting 15 child models (S14F1 - S14F15) | Fitting with ParallelLocalFoldFittingStrategy (8 workers, per: cpus=1, gpus=0, memory=0.23%)\n",
      "\t0.829\t = Validation score   (balanced_accuracy)\n",
      "\t358.61s\t = Training   runtime\n",
      "\t0.77s\t = Validation runtime\n",
      "Repeating k-fold bagging: 15/25\n",
      "Fitting model: LightGBMLarge_BAG_L4 ... Training model for up to 5978.13s of the 5978.13s of remaining time.\n",
      "\tFitting 15 child models (S15F1 - S15F15) | Fitting with ParallelLocalFoldFittingStrategy (8 workers, per: cpus=1, gpus=0, memory=0.24%)\n",
      "\t0.8311\t = Validation score   (balanced_accuracy)\n",
      "\t382.96s\t = Training   runtime\n",
      "\t0.82s\t = Validation runtime\n",
      "Repeating k-fold bagging: 16/25\n",
      "Fitting model: LightGBMLarge_BAG_L4 ... Training model for up to 5942.76s of the 5942.75s of remaining time.\n",
      "\tFitting 15 child models (S16F1 - S16F15) | Fitting with ParallelLocalFoldFittingStrategy (8 workers, per: cpus=1, gpus=0, memory=0.23%)\n",
      "\t0.8297\t = Validation score   (balanced_accuracy)\n",
      "\t409.31s\t = Training   runtime\n",
      "\t0.88s\t = Validation runtime\n",
      "Repeating k-fold bagging: 17/25\n",
      "Fitting model: LightGBMLarge_BAG_L4 ... Training model for up to 5906.15s of the 5906.15s of remaining time.\n",
      "\tFitting 15 child models (S17F1 - S17F15) | Fitting with ParallelLocalFoldFittingStrategy (8 workers, per: cpus=1, gpus=0, memory=0.23%)\n",
      "\t0.8318\t = Validation score   (balanced_accuracy)\n",
      "\t433.56s\t = Training   runtime\n",
      "\t0.94s\t = Validation runtime\n",
      "Repeating k-fold bagging: 18/25\n",
      "Fitting model: LightGBMLarge_BAG_L4 ... Training model for up to 5871.89s of the 5871.88s of remaining time.\n",
      "\tFitting 15 child models (S18F1 - S18F15) | Fitting with ParallelLocalFoldFittingStrategy (8 workers, per: cpus=1, gpus=0, memory=0.23%)\n",
      "\t0.8332\t = Validation score   (balanced_accuracy)\n",
      "\t458.53s\t = Training   runtime\n",
      "\t0.98s\t = Validation runtime\n",
      "Repeating k-fold bagging: 19/25\n",
      "Fitting model: LightGBMLarge_BAG_L4 ... Training model for up to 5836.62s of the 5836.61s of remaining time.\n",
      "\tFitting 15 child models (S19F1 - S19F15) | Fitting with ParallelLocalFoldFittingStrategy (8 workers, per: cpus=1, gpus=0, memory=0.23%)\n",
      "\t0.8353\t = Validation score   (balanced_accuracy)\n",
      "\t484.48s\t = Training   runtime\n",
      "\t1.02s\t = Validation runtime\n",
      "Repeating k-fold bagging: 20/25\n",
      "Fitting model: LightGBMLarge_BAG_L4 ... Training model for up to 5800.36s of the 5800.35s of remaining time.\n",
      "\tFitting 15 child models (S20F1 - S20F15) | Fitting with ParallelLocalFoldFittingStrategy (8 workers, per: cpus=1, gpus=0, memory=0.23%)\n",
      "\t0.8339\t = Validation score   (balanced_accuracy)\n",
      "\t510.85s\t = Training   runtime\n",
      "\t1.06s\t = Validation runtime\n",
      "Repeating k-fold bagging: 21/25\n",
      "Fitting model: LightGBMLarge_BAG_L4 ... Training model for up to 5763.8s of the 5763.79s of remaining time.\n",
      "\tFitting 15 child models (S21F1 - S21F15) | Fitting with ParallelLocalFoldFittingStrategy (8 workers, per: cpus=1, gpus=0, memory=0.23%)\n",
      "\t0.8353\t = Validation score   (balanced_accuracy)\n",
      "\t538.21s\t = Training   runtime\n",
      "\t1.11s\t = Validation runtime\n",
      "Repeating k-fold bagging: 22/25\n",
      "Fitting model: LightGBMLarge_BAG_L4 ... Training model for up to 5726.14s of the 5726.13s of remaining time.\n",
      "\tFitting 15 child models (S22F1 - S22F15) | Fitting with ParallelLocalFoldFittingStrategy (8 workers, per: cpus=1, gpus=0, memory=0.23%)\n",
      "\t0.8339\t = Validation score   (balanced_accuracy)\n",
      "\t564.02s\t = Training   runtime\n",
      "\t1.16s\t = Validation runtime\n",
      "Repeating k-fold bagging: 23/25\n",
      "Fitting model: LightGBMLarge_BAG_L4 ... Training model for up to 5690.31s of the 5690.3s of remaining time.\n",
      "\tFitting 15 child models (S23F1 - S23F15) | Fitting with ParallelLocalFoldFittingStrategy (8 workers, per: cpus=1, gpus=0, memory=0.23%)\n",
      "\t0.8346\t = Validation score   (balanced_accuracy)\n",
      "\t589.37s\t = Training   runtime\n",
      "\t1.22s\t = Validation runtime\n",
      "Repeating k-fold bagging: 24/25\n",
      "Fitting model: LightGBMLarge_BAG_L4 ... Training model for up to 5654.24s of the 5654.23s of remaining time.\n",
      "\tFitting 15 child models (S24F1 - S24F15) | Fitting with ParallelLocalFoldFittingStrategy (8 workers, per: cpus=1, gpus=0, memory=0.23%)\n",
      "\t0.8346\t = Validation score   (balanced_accuracy)\n",
      "\t615.72s\t = Training   runtime\n",
      "\t1.27s\t = Validation runtime\n",
      "Repeating k-fold bagging: 25/25\n",
      "Fitting model: LightGBMLarge_BAG_L4 ... Training model for up to 5617.02s of the 5617.01s of remaining time.\n",
      "\tFitting 15 child models (S25F1 - S25F15) | Fitting with ParallelLocalFoldFittingStrategy (8 workers, per: cpus=1, gpus=0, memory=0.23%)\n",
      "\t0.8339\t = Validation score   (balanced_accuracy)\n",
      "\t641.08s\t = Training   runtime\n",
      "\t1.31s\t = Validation runtime\n",
      "Completed 25/25 k-fold bagging repeats ...\n",
      "Fitting model: WeightedEnsemble_ALL_L5 ... Training model for up to 648.61s of the 5581.2s of remaining time.\n",
      "\tEnsemble Weights: {'LightGBMLarge_BAG_L4': 0.571, 'LightGBMLarge_BAG_L1': 0.429}\n",
      "\t0.8424\t = Validation score   (balanced_accuracy)\n",
      "\t0.69s\t = Training   runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "Fitting model: WeightedEnsemble_L5 ... Training model for up to 648.61s of the 5580.48s of remaining time.\n",
      "\tEnsemble Weights: {'LightGBMLarge_BAG_L4': 1.0}\n",
      "\t0.8339\t = Validation score   (balanced_accuracy)\n",
      "\t0.01s\t = Training   runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "AutoGluon training complete, total runtime = 3419.58s ... Best model: \"WeightedEnsemble_ALL_L5\"\n",
      "TabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"output\\autogluon\\20240114_114214/ds_sub_fit/sub_fit_ho\")\n",
      "Leaderboard on holdout data from dynamic stacking:\n",
      "                     model  holdout_score  score_val        eval_metric  pred_time_test  pred_time_val     fit_time  pred_time_test_marginal  pred_time_val_marginal  fit_time_marginal  stack_level  can_infer  fit_order\n",
      "0     LightGBMLarge_BAG_L1       0.848315   0.839013  balanced_accuracy       16.519792       1.952962   544.902808                16.519792                1.952962         544.902808            1       True          1\n",
      "1      WeightedEnsemble_L2       0.848315   0.839013  balanced_accuracy       16.534793       1.958962   544.910784                 0.015002                0.006000           0.007976            2       True          2\n",
      "2     LightGBMLarge_BAG_L2       0.831461   0.829114  balanced_accuracy       28.866857       3.412018  1159.882526                12.347065                1.459056         614.979717            2       True          3\n",
      "3      WeightedEnsemble_L3       0.831461   0.829114  balanced_accuracy       28.887858       3.419015  1159.894518                 0.021001                0.006996           0.011992            3       True          4\n",
      "4  WeightedEnsemble_ALL_L5       0.831461   0.842389  balanced_accuracy       51.828773       6.184476  2443.636038                 0.016001                0.005999           0.689914            5       True          8\n",
      "5     LightGBMLarge_BAG_L3       0.825843   0.832508  balanced_accuracy       40.754718       4.869616  1801.862386                11.887861                1.457597         641.979860            3       True          5\n",
      "6      WeightedEnsemble_L4       0.825843   0.832508  balanced_accuracy       40.770663       4.875615  1801.877274                 0.015945                0.005999           0.014888            4       True          6\n",
      "7     LightGBMLarge_BAG_L4       0.825843   0.833891  balanced_accuracy       51.812771       6.178477  2442.946124                11.058053                1.308861         641.083738            4       True          7\n",
      "8      WeightedEnsemble_L5       0.825843   0.833891  balanced_accuracy       51.827768       6.186494  2442.957130                 0.014997                0.008017           0.011006            5       True          9\n",
      "Stacked overfitting occurred: True.\n",
      "Spend 3475 seconds for the sub-fit(s) during dynamic stacking.\n",
      "Time left for full fit of AutoGluon: 32525 seconds.\n",
      "Starting full fit now with num_stack_levels 0.\n",
      "Beginning AutoGluon training ... Time limit = 32525s\n",
      "AutoGluon will save models to \"output\\autogluon\\20240114_114214\"\n",
      "=================== System Info ===================\n",
      "AutoGluon Version:  1.0.0\n",
      "Python Version:     3.9.11\n",
      "Operating System:   Windows\n",
      "Platform Machine:   AMD64\n",
      "Platform Version:   10.0.19041\n",
      "CPU Count:          8\n",
      "Memory Avail:       9.27 GB / 15.95 GB (58.1%)\n",
      "Disk Space Avail:   16.19 GB / 222.52 GB (7.3%)\n",
      "===================================================\n",
      "Train Data Rows:    1600\n",
      "Train Data Columns: 25\n",
      "Label Column:       class\n",
      "Problem Type:       binary\n",
      "Preprocessing data ...\n",
      "Selected class <--> label mapping:  class 1 = 1.0, class 0 = -1.0\n",
      "\tNote: For your binary classification, AutoGluon arbitrarily selected which label-value represents positive (1.0) vs negative (-1.0) class.\n",
      "\tTo explicitly set the positive_class, either rename classes to 1 and 0, or specify positive_class in Predictor init.\n",
      "Using Feature Generators to preprocess the data ...\n",
      "Fitting AutoMLPipelineFeatureGenerator...\n",
      "\tAvailable Memory:                    9494.68 MB\n",
      "\tTrain Data (Original)  Memory Usage: 0.31 MB (0.0% of available memory)\n",
      "\tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n",
      "\tStage 1 Generators:\n",
      "\t\tFitting AsTypeFeatureGenerator...\n",
      "\tStage 2 Generators:\n",
      "\t\tFitting FillNaFeatureGenerator...\n",
      "\tStage 3 Generators:\n",
      "\t\tFitting IdentityFeatureGenerator...\n",
      "\tStage 4 Generators:\n",
      "\t\tFitting DropUniqueFeatureGenerator...\n",
      "\tStage 5 Generators:\n",
      "\t\tFitting DropDuplicatesFeatureGenerator...\n",
      "\tTypes of features in original data (raw dtype, special dtypes):\n",
      "\t\t('float', []) : 25 | ['0', '1', '2', '3', '4', ...]\n",
      "\tTypes of features in processed data (raw dtype, special dtypes):\n",
      "\t\t('float', []) : 25 | ['0', '1', '2', '3', '4', ...]\n",
      "\t0.1s = Fit runtime\n",
      "\t25 features in original data used to generate 25 features in processed data.\n",
      "\tTrain Data (Processed) Memory Usage: 0.31 MB (0.0% of available memory)\n",
      "Data preprocessing and feature engineering runtime = 0.14s ...\n",
      "AutoGluon will gauge predictive performance using evaluation metric: 'balanced_accuracy'\n",
      "\tTo change this, specify the eval_metric parameter of Predictor()\n",
      "User-specified model hyperparameters to be fit:\n",
      "{\n",
      "\t'NN_TORCH': {},\n",
      "\t'GBM': [{'extra_trees': True, 'ag_args': {'name_suffix': 'XT'}}, {}, 'GBMLarge'],\n",
      "\t'CAT': {},\n",
      "\t'XGB': {},\n",
      "\t'FASTAI': {},\n",
      "\t'RF': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
      "\t'XT': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
      "\t'KNN': [{'weights': 'uniform', 'ag_args': {'name_suffix': 'Unif'}}, {'weights': 'distance', 'ag_args': {'name_suffix': 'Dist'}}],\n",
      "}\n",
      "Fitting 13 L1 models ...\n",
      "Hyperparameter tuning model: KNeighborsUnif_BAG_L1 ... Tuning model for up to 2251.72s of the 32524.85s of remaining time.\n",
      "Warning: Exception caused KNeighborsUnif_BAG_L1 to fail during hyperparameter tuning... Skipping this model.\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\YanPC\\Downloads\\automl_hw2-master\\.venv\\lib\\site-packages\\autogluon\\core\\trainer\\abstract_trainer.py\", line 2135, in _train_single_full\n",
      "    hpo_models, hpo_results = model.hyperparameter_tune(\n",
      "  File \"c:\\Users\\YanPC\\Downloads\\automl_hw2-master\\.venv\\lib\\site-packages\\autogluon\\core\\models\\abstract\\abstract_model.py\", line 1466, in hyperparameter_tune\n",
      "    hpo_executor.initialize(hyperparameter_tune_kwargs, default_num_trials=default_num_trials, time_limit=time_limit)\n",
      "  File \"c:\\Users\\YanPC\\Downloads\\automl_hw2-master\\.venv\\lib\\site-packages\\autogluon\\core\\hpo\\executors.py\", line 489, in initialize\n",
      "    hyperparameter_tune_kwargs = scheduler_factory(hyperparameter_tune_kwargs, num_trials=num_trials, nthreads_per_trial=\"auto\", ngpus_per_trial=\"auto\")\n",
      "  File \"c:\\Users\\YanPC\\Downloads\\automl_hw2-master\\.venv\\lib\\site-packages\\autogluon\\core\\scheduler\\scheduler_factory.py\", line 191, in scheduler_factory\n",
      "    raise ValueError(f\"Required key 'scheduler' is not present in hyperparameter_tune_kwargs: {hyperparameter_tune_kwargs}\")\n",
      "ValueError: Required key 'scheduler' is not present in hyperparameter_tune_kwargs: {'searcher': 'auto', 'time_out': 1200, 'num_trials': 30}\n",
      "Required key 'scheduler' is not present in hyperparameter_tune_kwargs: {'searcher': 'auto', 'time_out': 1200, 'num_trials': 30}\n",
      "Hyperparameter tuning model: KNeighborsDist_BAG_L1 ... Tuning model for up to 2251.72s of the 32524.84s of remaining time.\n",
      "Warning: Exception caused KNeighborsDist_BAG_L1 to fail during hyperparameter tuning... Skipping this model.\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\YanPC\\Downloads\\automl_hw2-master\\.venv\\lib\\site-packages\\autogluon\\core\\trainer\\abstract_trainer.py\", line 2135, in _train_single_full\n",
      "    hpo_models, hpo_results = model.hyperparameter_tune(\n",
      "  File \"c:\\Users\\YanPC\\Downloads\\automl_hw2-master\\.venv\\lib\\site-packages\\autogluon\\core\\models\\abstract\\abstract_model.py\", line 1466, in hyperparameter_tune\n",
      "    hpo_executor.initialize(hyperparameter_tune_kwargs, default_num_trials=default_num_trials, time_limit=time_limit)\n",
      "  File \"c:\\Users\\YanPC\\Downloads\\automl_hw2-master\\.venv\\lib\\site-packages\\autogluon\\core\\hpo\\executors.py\", line 489, in initialize\n",
      "    hyperparameter_tune_kwargs = scheduler_factory(hyperparameter_tune_kwargs, num_trials=num_trials, nthreads_per_trial=\"auto\", ngpus_per_trial=\"auto\")\n",
      "  File \"c:\\Users\\YanPC\\Downloads\\automl_hw2-master\\.venv\\lib\\site-packages\\autogluon\\core\\scheduler\\scheduler_factory.py\", line 191, in scheduler_factory\n",
      "    raise ValueError(f\"Required key 'scheduler' is not present in hyperparameter_tune_kwargs: {hyperparameter_tune_kwargs}\")\n",
      "ValueError: Required key 'scheduler' is not present in hyperparameter_tune_kwargs: {'searcher': 'auto', 'time_out': 1200, 'num_trials': 30}\n",
      "Required key 'scheduler' is not present in hyperparameter_tune_kwargs: {'searcher': 'auto', 'time_out': 1200, 'num_trials': 30}\n",
      "Hyperparameter tuning model: LightGBMXT_BAG_L1 ... Tuning model for up to 2251.72s of the 32524.83s of remaining time.\n",
      "Warning: Exception caused LightGBMXT_BAG_L1 to fail during hyperparameter tuning... Skipping this model.\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\YanPC\\Downloads\\automl_hw2-master\\.venv\\lib\\site-packages\\autogluon\\core\\trainer\\abstract_trainer.py\", line 2135, in _train_single_full\n",
      "    hpo_models, hpo_results = model.hyperparameter_tune(\n",
      "  File \"c:\\Users\\YanPC\\Downloads\\automl_hw2-master\\.venv\\lib\\site-packages\\autogluon\\core\\models\\abstract\\abstract_model.py\", line 1466, in hyperparameter_tune\n",
      "    hpo_executor.initialize(hyperparameter_tune_kwargs, default_num_trials=default_num_trials, time_limit=time_limit)\n",
      "  File \"c:\\Users\\YanPC\\Downloads\\automl_hw2-master\\.venv\\lib\\site-packages\\autogluon\\core\\hpo\\executors.py\", line 489, in initialize\n",
      "    hyperparameter_tune_kwargs = scheduler_factory(hyperparameter_tune_kwargs, num_trials=num_trials, nthreads_per_trial=\"auto\", ngpus_per_trial=\"auto\")\n",
      "  File \"c:\\Users\\YanPC\\Downloads\\automl_hw2-master\\.venv\\lib\\site-packages\\autogluon\\core\\scheduler\\scheduler_factory.py\", line 191, in scheduler_factory\n",
      "    raise ValueError(f\"Required key 'scheduler' is not present in hyperparameter_tune_kwargs: {hyperparameter_tune_kwargs}\")\n",
      "ValueError: Required key 'scheduler' is not present in hyperparameter_tune_kwargs: {'searcher': 'auto', 'time_out': 1200, 'num_trials': 30}\n",
      "Required key 'scheduler' is not present in hyperparameter_tune_kwargs: {'searcher': 'auto', 'time_out': 1200, 'num_trials': 30}\n",
      "Hyperparameter tuning model: LightGBM_BAG_L1 ... Tuning model for up to 2251.72s of the 32524.83s of remaining time.\n",
      "Warning: Exception caused LightGBM_BAG_L1 to fail during hyperparameter tuning... Skipping this model.\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\YanPC\\Downloads\\automl_hw2-master\\.venv\\lib\\site-packages\\autogluon\\core\\trainer\\abstract_trainer.py\", line 2135, in _train_single_full\n",
      "    hpo_models, hpo_results = model.hyperparameter_tune(\n",
      "  File \"c:\\Users\\YanPC\\Downloads\\automl_hw2-master\\.venv\\lib\\site-packages\\autogluon\\core\\models\\abstract\\abstract_model.py\", line 1466, in hyperparameter_tune\n",
      "    hpo_executor.initialize(hyperparameter_tune_kwargs, default_num_trials=default_num_trials, time_limit=time_limit)\n",
      "  File \"c:\\Users\\YanPC\\Downloads\\automl_hw2-master\\.venv\\lib\\site-packages\\autogluon\\core\\hpo\\executors.py\", line 489, in initialize\n",
      "    hyperparameter_tune_kwargs = scheduler_factory(hyperparameter_tune_kwargs, num_trials=num_trials, nthreads_per_trial=\"auto\", ngpus_per_trial=\"auto\")\n",
      "  File \"c:\\Users\\YanPC\\Downloads\\automl_hw2-master\\.venv\\lib\\site-packages\\autogluon\\core\\scheduler\\scheduler_factory.py\", line 191, in scheduler_factory\n",
      "    raise ValueError(f\"Required key 'scheduler' is not present in hyperparameter_tune_kwargs: {hyperparameter_tune_kwargs}\")\n",
      "ValueError: Required key 'scheduler' is not present in hyperparameter_tune_kwargs: {'searcher': 'auto', 'time_out': 1200, 'num_trials': 30}\n",
      "Required key 'scheduler' is not present in hyperparameter_tune_kwargs: {'searcher': 'auto', 'time_out': 1200, 'num_trials': 30}\n",
      "Hyperparameter tuning model: RandomForestGini_BAG_L1 ... Tuning model for up to 2251.72s of the 32524.82s of remaining time.\n",
      "Warning: Exception caused RandomForestGini_BAG_L1 to fail during hyperparameter tuning... Skipping this model.\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\YanPC\\Downloads\\automl_hw2-master\\.venv\\lib\\site-packages\\autogluon\\core\\trainer\\abstract_trainer.py\", line 2135, in _train_single_full\n",
      "    hpo_models, hpo_results = model.hyperparameter_tune(\n",
      "  File \"c:\\Users\\YanPC\\Downloads\\automl_hw2-master\\.venv\\lib\\site-packages\\autogluon\\core\\models\\abstract\\abstract_model.py\", line 1466, in hyperparameter_tune\n",
      "    hpo_executor.initialize(hyperparameter_tune_kwargs, default_num_trials=default_num_trials, time_limit=time_limit)\n",
      "  File \"c:\\Users\\YanPC\\Downloads\\automl_hw2-master\\.venv\\lib\\site-packages\\autogluon\\core\\hpo\\executors.py\", line 489, in initialize\n",
      "    hyperparameter_tune_kwargs = scheduler_factory(hyperparameter_tune_kwargs, num_trials=num_trials, nthreads_per_trial=\"auto\", ngpus_per_trial=\"auto\")\n",
      "  File \"c:\\Users\\YanPC\\Downloads\\automl_hw2-master\\.venv\\lib\\site-packages\\autogluon\\core\\scheduler\\scheduler_factory.py\", line 191, in scheduler_factory\n",
      "    raise ValueError(f\"Required key 'scheduler' is not present in hyperparameter_tune_kwargs: {hyperparameter_tune_kwargs}\")\n",
      "ValueError: Required key 'scheduler' is not present in hyperparameter_tune_kwargs: {'searcher': 'auto', 'time_out': 1200, 'num_trials': 30}\n",
      "Required key 'scheduler' is not present in hyperparameter_tune_kwargs: {'searcher': 'auto', 'time_out': 1200, 'num_trials': 30}\n",
      "Hyperparameter tuning model: RandomForestEntr_BAG_L1 ... Tuning model for up to 2251.72s of the 32524.81s of remaining time.\n",
      "Warning: Exception caused RandomForestEntr_BAG_L1 to fail during hyperparameter tuning... Skipping this model.\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\YanPC\\Downloads\\automl_hw2-master\\.venv\\lib\\site-packages\\autogluon\\core\\trainer\\abstract_trainer.py\", line 2135, in _train_single_full\n",
      "    hpo_models, hpo_results = model.hyperparameter_tune(\n",
      "  File \"c:\\Users\\YanPC\\Downloads\\automl_hw2-master\\.venv\\lib\\site-packages\\autogluon\\core\\models\\abstract\\abstract_model.py\", line 1466, in hyperparameter_tune\n",
      "    hpo_executor.initialize(hyperparameter_tune_kwargs, default_num_trials=default_num_trials, time_limit=time_limit)\n",
      "  File \"c:\\Users\\YanPC\\Downloads\\automl_hw2-master\\.venv\\lib\\site-packages\\autogluon\\core\\hpo\\executors.py\", line 489, in initialize\n",
      "    hyperparameter_tune_kwargs = scheduler_factory(hyperparameter_tune_kwargs, num_trials=num_trials, nthreads_per_trial=\"auto\", ngpus_per_trial=\"auto\")\n",
      "  File \"c:\\Users\\YanPC\\Downloads\\automl_hw2-master\\.venv\\lib\\site-packages\\autogluon\\core\\scheduler\\scheduler_factory.py\", line 191, in scheduler_factory\n",
      "    raise ValueError(f\"Required key 'scheduler' is not present in hyperparameter_tune_kwargs: {hyperparameter_tune_kwargs}\")\n",
      "ValueError: Required key 'scheduler' is not present in hyperparameter_tune_kwargs: {'searcher': 'auto', 'time_out': 1200, 'num_trials': 30}\n",
      "Required key 'scheduler' is not present in hyperparameter_tune_kwargs: {'searcher': 'auto', 'time_out': 1200, 'num_trials': 30}\n",
      "Hyperparameter tuning model: CatBoost_BAG_L1 ... Tuning model for up to 2251.72s of the 32524.8s of remaining time.\n",
      "Warning: Exception caused CatBoost_BAG_L1 to fail during hyperparameter tuning... Skipping this model.\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\YanPC\\Downloads\\automl_hw2-master\\.venv\\lib\\site-packages\\autogluon\\core\\trainer\\abstract_trainer.py\", line 2135, in _train_single_full\n",
      "    hpo_models, hpo_results = model.hyperparameter_tune(\n",
      "  File \"c:\\Users\\YanPC\\Downloads\\automl_hw2-master\\.venv\\lib\\site-packages\\autogluon\\core\\models\\abstract\\abstract_model.py\", line 1466, in hyperparameter_tune\n",
      "    hpo_executor.initialize(hyperparameter_tune_kwargs, default_num_trials=default_num_trials, time_limit=time_limit)\n",
      "  File \"c:\\Users\\YanPC\\Downloads\\automl_hw2-master\\.venv\\lib\\site-packages\\autogluon\\core\\hpo\\executors.py\", line 489, in initialize\n",
      "    hyperparameter_tune_kwargs = scheduler_factory(hyperparameter_tune_kwargs, num_trials=num_trials, nthreads_per_trial=\"auto\", ngpus_per_trial=\"auto\")\n",
      "  File \"c:\\Users\\YanPC\\Downloads\\automl_hw2-master\\.venv\\lib\\site-packages\\autogluon\\core\\scheduler\\scheduler_factory.py\", line 191, in scheduler_factory\n",
      "    raise ValueError(f\"Required key 'scheduler' is not present in hyperparameter_tune_kwargs: {hyperparameter_tune_kwargs}\")\n",
      "ValueError: Required key 'scheduler' is not present in hyperparameter_tune_kwargs: {'searcher': 'auto', 'time_out': 1200, 'num_trials': 30}\n",
      "Required key 'scheduler' is not present in hyperparameter_tune_kwargs: {'searcher': 'auto', 'time_out': 1200, 'num_trials': 30}\n",
      "Hyperparameter tuning model: ExtraTreesGini_BAG_L1 ... Tuning model for up to 2251.72s of the 32524.79s of remaining time.\n",
      "Warning: Exception caused ExtraTreesGini_BAG_L1 to fail during hyperparameter tuning... Skipping this model.\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\YanPC\\Downloads\\automl_hw2-master\\.venv\\lib\\site-packages\\autogluon\\core\\trainer\\abstract_trainer.py\", line 2135, in _train_single_full\n",
      "    hpo_models, hpo_results = model.hyperparameter_tune(\n",
      "  File \"c:\\Users\\YanPC\\Downloads\\automl_hw2-master\\.venv\\lib\\site-packages\\autogluon\\core\\models\\abstract\\abstract_model.py\", line 1466, in hyperparameter_tune\n",
      "    hpo_executor.initialize(hyperparameter_tune_kwargs, default_num_trials=default_num_trials, time_limit=time_limit)\n",
      "  File \"c:\\Users\\YanPC\\Downloads\\automl_hw2-master\\.venv\\lib\\site-packages\\autogluon\\core\\hpo\\executors.py\", line 489, in initialize\n",
      "    hyperparameter_tune_kwargs = scheduler_factory(hyperparameter_tune_kwargs, num_trials=num_trials, nthreads_per_trial=\"auto\", ngpus_per_trial=\"auto\")\n",
      "  File \"c:\\Users\\YanPC\\Downloads\\automl_hw2-master\\.venv\\lib\\site-packages\\autogluon\\core\\scheduler\\scheduler_factory.py\", line 191, in scheduler_factory\n",
      "    raise ValueError(f\"Required key 'scheduler' is not present in hyperparameter_tune_kwargs: {hyperparameter_tune_kwargs}\")\n",
      "ValueError: Required key 'scheduler' is not present in hyperparameter_tune_kwargs: {'searcher': 'auto', 'time_out': 1200, 'num_trials': 30}\n",
      "Required key 'scheduler' is not present in hyperparameter_tune_kwargs: {'searcher': 'auto', 'time_out': 1200, 'num_trials': 30}\n",
      "Hyperparameter tuning model: ExtraTreesEntr_BAG_L1 ... Tuning model for up to 2251.72s of the 32524.78s of remaining time.\n",
      "Warning: Exception caused ExtraTreesEntr_BAG_L1 to fail during hyperparameter tuning... Skipping this model.\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\YanPC\\Downloads\\automl_hw2-master\\.venv\\lib\\site-packages\\autogluon\\core\\trainer\\abstract_trainer.py\", line 2135, in _train_single_full\n",
      "    hpo_models, hpo_results = model.hyperparameter_tune(\n",
      "  File \"c:\\Users\\YanPC\\Downloads\\automl_hw2-master\\.venv\\lib\\site-packages\\autogluon\\core\\models\\abstract\\abstract_model.py\", line 1466, in hyperparameter_tune\n",
      "    hpo_executor.initialize(hyperparameter_tune_kwargs, default_num_trials=default_num_trials, time_limit=time_limit)\n",
      "  File \"c:\\Users\\YanPC\\Downloads\\automl_hw2-master\\.venv\\lib\\site-packages\\autogluon\\core\\hpo\\executors.py\", line 489, in initialize\n",
      "    hyperparameter_tune_kwargs = scheduler_factory(hyperparameter_tune_kwargs, num_trials=num_trials, nthreads_per_trial=\"auto\", ngpus_per_trial=\"auto\")\n",
      "  File \"c:\\Users\\YanPC\\Downloads\\automl_hw2-master\\.venv\\lib\\site-packages\\autogluon\\core\\scheduler\\scheduler_factory.py\", line 191, in scheduler_factory\n",
      "    raise ValueError(f\"Required key 'scheduler' is not present in hyperparameter_tune_kwargs: {hyperparameter_tune_kwargs}\")\n",
      "ValueError: Required key 'scheduler' is not present in hyperparameter_tune_kwargs: {'searcher': 'auto', 'time_out': 1200, 'num_trials': 30}\n",
      "Required key 'scheduler' is not present in hyperparameter_tune_kwargs: {'searcher': 'auto', 'time_out': 1200, 'num_trials': 30}\n",
      "Hyperparameter tuning model: NeuralNetFastAI_BAG_L1 ... Tuning model for up to 2251.72s of the 32524.77s of remaining time.\n",
      "Warning: Exception caused NeuralNetFastAI_BAG_L1 to fail during hyperparameter tuning... Skipping this model.\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\YanPC\\Downloads\\automl_hw2-master\\.venv\\lib\\site-packages\\autogluon\\core\\trainer\\abstract_trainer.py\", line 2135, in _train_single_full\n",
      "    hpo_models, hpo_results = model.hyperparameter_tune(\n",
      "  File \"c:\\Users\\YanPC\\Downloads\\automl_hw2-master\\.venv\\lib\\site-packages\\autogluon\\core\\models\\abstract\\abstract_model.py\", line 1466, in hyperparameter_tune\n",
      "    hpo_executor.initialize(hyperparameter_tune_kwargs, default_num_trials=default_num_trials, time_limit=time_limit)\n",
      "  File \"c:\\Users\\YanPC\\Downloads\\automl_hw2-master\\.venv\\lib\\site-packages\\autogluon\\core\\hpo\\executors.py\", line 336, in initialize\n",
      "    hyperparameter_tune_kwargs[\"scheduler\"], hyperparameter_tune_kwargs[\"scheduler\"]\n",
      "KeyError: 'scheduler'\n",
      "'scheduler'\n",
      "Hyperparameter tuning model: XGBoost_BAG_L1 ... Tuning model for up to 2251.72s of the 32524.76s of remaining time.\n",
      "Warning: Exception caused XGBoost_BAG_L1 to fail during hyperparameter tuning... Skipping this model.\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\YanPC\\Downloads\\automl_hw2-master\\.venv\\lib\\site-packages\\autogluon\\core\\trainer\\abstract_trainer.py\", line 2135, in _train_single_full\n",
      "    hpo_models, hpo_results = model.hyperparameter_tune(\n",
      "  File \"c:\\Users\\YanPC\\Downloads\\automl_hw2-master\\.venv\\lib\\site-packages\\autogluon\\core\\models\\abstract\\abstract_model.py\", line 1466, in hyperparameter_tune\n",
      "    hpo_executor.initialize(hyperparameter_tune_kwargs, default_num_trials=default_num_trials, time_limit=time_limit)\n",
      "  File \"c:\\Users\\YanPC\\Downloads\\automl_hw2-master\\.venv\\lib\\site-packages\\autogluon\\core\\hpo\\executors.py\", line 489, in initialize\n",
      "    hyperparameter_tune_kwargs = scheduler_factory(hyperparameter_tune_kwargs, num_trials=num_trials, nthreads_per_trial=\"auto\", ngpus_per_trial=\"auto\")\n",
      "  File \"c:\\Users\\YanPC\\Downloads\\automl_hw2-master\\.venv\\lib\\site-packages\\autogluon\\core\\scheduler\\scheduler_factory.py\", line 191, in scheduler_factory\n",
      "    raise ValueError(f\"Required key 'scheduler' is not present in hyperparameter_tune_kwargs: {hyperparameter_tune_kwargs}\")\n",
      "ValueError: Required key 'scheduler' is not present in hyperparameter_tune_kwargs: {'searcher': 'auto', 'time_out': 1200, 'num_trials': 30}\n",
      "Required key 'scheduler' is not present in hyperparameter_tune_kwargs: {'searcher': 'auto', 'time_out': 1200, 'num_trials': 30}\n",
      "Hyperparameter tuning model: NeuralNetTorch_BAG_L1 ... Tuning model for up to 2251.72s of the 32524.76s of remaining time.\n",
      "Warning: Exception caused NeuralNetTorch_BAG_L1 to fail during hyperparameter tuning... Skipping this model.\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\YanPC\\Downloads\\automl_hw2-master\\.venv\\lib\\site-packages\\autogluon\\core\\trainer\\abstract_trainer.py\", line 2135, in _train_single_full\n",
      "    hpo_models, hpo_results = model.hyperparameter_tune(\n",
      "  File \"c:\\Users\\YanPC\\Downloads\\automl_hw2-master\\.venv\\lib\\site-packages\\autogluon\\core\\models\\abstract\\abstract_model.py\", line 1466, in hyperparameter_tune\n",
      "    hpo_executor.initialize(hyperparameter_tune_kwargs, default_num_trials=default_num_trials, time_limit=time_limit)\n",
      "  File \"c:\\Users\\YanPC\\Downloads\\automl_hw2-master\\.venv\\lib\\site-packages\\autogluon\\core\\hpo\\executors.py\", line 336, in initialize\n",
      "    hyperparameter_tune_kwargs[\"scheduler\"], hyperparameter_tune_kwargs[\"scheduler\"]\n",
      "KeyError: 'scheduler'\n",
      "'scheduler'\n",
      "Fitting model: LightGBMLarge_BAG_L1 ... Training model for up to 2251.72s of the 32524.74s of remaining time.\n",
      "\tFitting 15 child models (S1F1 - S1F15) | Fitting with ParallelLocalFoldFittingStrategy (8 workers, per: cpus=1, gpus=0, memory=0.22%)\n",
      "\t0.8475\t = Validation score   (balanced_accuracy)\n",
      "\t28.45s\t = Training   runtime\n",
      "\t0.09s\t = Validation runtime\n",
      "Repeating k-fold bagging: 2/25\n",
      "Fitting model: LightGBMLarge_BAG_L1 ... Training model for up to 32485.42s of the 32485.41s of remaining time.\n",
      "\tFitting 15 child models (S2F1 - S2F15) | Fitting with ParallelLocalFoldFittingStrategy (8 workers, per: cpus=1, gpus=0, memory=0.22%)\n",
      "\t0.8331\t = Validation score   (balanced_accuracy)\n",
      "\t56.55s\t = Training   runtime\n",
      "\t0.16s\t = Validation runtime\n",
      "Repeating k-fold bagging: 3/25\n",
      "Fitting model: LightGBMLarge_BAG_L1 ... Training model for up to 32446.37s of the 32446.36s of remaining time.\n",
      "\tFitting 15 child models (S3F1 - S3F15) | Fitting with ParallelLocalFoldFittingStrategy (8 workers, per: cpus=1, gpus=0, memory=0.22%)\n",
      "\t0.8344\t = Validation score   (balanced_accuracy)\n",
      "\t86.72s\t = Training   runtime\n",
      "\t0.26s\t = Validation runtime\n",
      "Repeating k-fold bagging: 4/25\n",
      "Fitting model: LightGBMLarge_BAG_L1 ... Training model for up to 32404.52s of the 32404.51s of remaining time.\n",
      "\tFitting 15 child models (S4F1 - S4F15) | Fitting with ParallelLocalFoldFittingStrategy (8 workers, per: cpus=1, gpus=0, memory=0.22%)\n",
      "\t0.8344\t = Validation score   (balanced_accuracy)\n",
      "\t115.88s\t = Training   runtime\n",
      "\t0.34s\t = Validation runtime\n",
      "Repeating k-fold bagging: 5/25\n",
      "Fitting model: LightGBMLarge_BAG_L1 ... Training model for up to 32365.19s of the 32365.19s of remaining time.\n",
      "\tFitting 15 child models (S5F1 - S5F15) | Fitting with ParallelLocalFoldFittingStrategy (8 workers, per: cpus=1, gpus=0, memory=0.23%)\n",
      "\t0.8344\t = Validation score   (balanced_accuracy)\n",
      "\t147.51s\t = Training   runtime\n",
      "\t0.43s\t = Validation runtime\n",
      "Repeating k-fold bagging: 6/25\n",
      "Fitting model: LightGBMLarge_BAG_L1 ... Training model for up to 32322.85s of the 32322.85s of remaining time.\n",
      "\tFitting 15 child models (S6F1 - S6F15) | Fitting with ParallelLocalFoldFittingStrategy (8 workers, per: cpus=1, gpus=0, memory=0.22%)\n",
      "\t0.8337\t = Validation score   (balanced_accuracy)\n",
      "\t178.57s\t = Training   runtime\n",
      "\t0.51s\t = Validation runtime\n",
      "Repeating k-fold bagging: 7/25\n",
      "Fitting model: LightGBMLarge_BAG_L1 ... Training model for up to 32280.83s of the 32280.82s of remaining time.\n",
      "\tFitting 15 child models (S7F1 - S7F15) | Fitting with ParallelLocalFoldFittingStrategy (8 workers, per: cpus=1, gpus=0, memory=0.22%)\n",
      "\t0.8356\t = Validation score   (balanced_accuracy)\n",
      "\t206.59s\t = Training   runtime\n",
      "\t0.58s\t = Validation runtime\n",
      "Repeating k-fold bagging: 8/25\n",
      "Fitting model: LightGBMLarge_BAG_L1 ... Training model for up to 32242.33s of the 32242.32s of remaining time.\n",
      "\tFitting 15 child models (S8F1 - S8F15) | Fitting with ParallelLocalFoldFittingStrategy (8 workers, per: cpus=1, gpus=0, memory=0.22%)\n",
      "\t0.8375\t = Validation score   (balanced_accuracy)\n",
      "\t233.74s\t = Training   runtime\n",
      "\t0.64s\t = Validation runtime\n",
      "Repeating k-fold bagging: 9/25\n",
      "Fitting model: LightGBMLarge_BAG_L1 ... Training model for up to 32204.83s of the 32204.82s of remaining time.\n",
      "\tFitting 15 child models (S9F1 - S9F15) | Fitting with ParallelLocalFoldFittingStrategy (8 workers, per: cpus=1, gpus=0, memory=0.23%)\n",
      "\t0.8381\t = Validation score   (balanced_accuracy)\n",
      "\t263.28s\t = Training   runtime\n",
      "\t0.75s\t = Validation runtime\n",
      "Repeating k-fold bagging: 10/25\n",
      "Fitting model: LightGBMLarge_BAG_L1 ... Training model for up to 32164.36s of the 32164.36s of remaining time.\n",
      "\tFitting 15 child models (S10F1 - S10F15) | Fitting with ParallelLocalFoldFittingStrategy (8 workers, per: cpus=1, gpus=0, memory=0.23%)\n",
      "\t0.8394\t = Validation score   (balanced_accuracy)\n",
      "\t293.16s\t = Training   runtime\n",
      "\t0.84s\t = Validation runtime\n",
      "Repeating k-fold bagging: 11/25\n",
      "Fitting model: LightGBMLarge_BAG_L1 ... Training model for up to 32124.22s of the 32124.21s of remaining time.\n",
      "\tFitting 15 child models (S11F1 - S11F15) | Fitting with ParallelLocalFoldFittingStrategy (8 workers, per: cpus=1, gpus=0, memory=0.22%)\n",
      "\t0.835\t = Validation score   (balanced_accuracy)\n",
      "\t320.31s\t = Training   runtime\n",
      "\t0.93s\t = Validation runtime\n",
      "Repeating k-fold bagging: 12/25\n",
      "Fitting model: LightGBMLarge_BAG_L1 ... Training model for up to 32086.6s of the 32086.6s of remaining time.\n",
      "\tFitting 15 child models (S12F1 - S12F15) | Fitting with ParallelLocalFoldFittingStrategy (8 workers, per: cpus=1, gpus=0, memory=0.23%)\n",
      "\t0.8375\t = Validation score   (balanced_accuracy)\n",
      "\t348.83s\t = Training   runtime\n",
      "\t1.01s\t = Validation runtime\n",
      "Repeating k-fold bagging: 13/25\n",
      "Fitting model: LightGBMLarge_BAG_L1 ... Training model for up to 32046.98s of the 32046.98s of remaining time.\n",
      "\tFitting 15 child models (S13F1 - S13F15) | Fitting with ParallelLocalFoldFittingStrategy (8 workers, per: cpus=1, gpus=0, memory=0.22%)\n",
      "\t0.84\t = Validation score   (balanced_accuracy)\n",
      "\t376.77s\t = Training   runtime\n",
      "\t1.1s\t = Validation runtime\n",
      "Repeating k-fold bagging: 14/25\n",
      "Fitting model: LightGBMLarge_BAG_L1 ... Training model for up to 32008.64s of the 32008.63s of remaining time.\n",
      "\tFitting 15 child models (S14F1 - S14F15) | Fitting with ParallelLocalFoldFittingStrategy (8 workers, per: cpus=1, gpus=0, memory=0.24%)\n",
      "\t0.8387\t = Validation score   (balanced_accuracy)\n",
      "\t406.91s\t = Training   runtime\n",
      "\t1.2s\t = Validation runtime\n",
      "Repeating k-fold bagging: 15/25\n",
      "Fitting model: LightGBMLarge_BAG_L1 ... Training model for up to 31967.3s of the 31967.29s of remaining time.\n",
      "\tFitting 15 child models (S15F1 - S15F15) | Fitting with ParallelLocalFoldFittingStrategy (8 workers, per: cpus=1, gpus=0, memory=0.22%)\n",
      "\t0.8394\t = Validation score   (balanced_accuracy)\n",
      "\t436.79s\t = Training   runtime\n",
      "\t1.32s\t = Validation runtime\n",
      "Repeating k-fold bagging: 16/25\n",
      "Fitting model: LightGBMLarge_BAG_L1 ... Training model for up to 31926.81s of the 31926.81s of remaining time.\n",
      "\tFitting 15 child models (S16F1 - S16F15) | Fitting with ParallelLocalFoldFittingStrategy (8 workers, per: cpus=1, gpus=0, memory=0.23%)\n",
      "\t0.8394\t = Validation score   (balanced_accuracy)\n",
      "\t468.91s\t = Training   runtime\n",
      "\t1.44s\t = Validation runtime\n",
      "Repeating k-fold bagging: 17/25\n",
      "Fitting model: LightGBMLarge_BAG_L1 ... Training model for up to 31883.98s of the 31883.97s of remaining time.\n",
      "\tFitting 15 child models (S17F1 - S17F15) | Fitting with ParallelLocalFoldFittingStrategy (8 workers, per: cpus=1, gpus=0, memory=0.22%)\n",
      "\t0.8387\t = Validation score   (balanced_accuracy)\n",
      "\t498.34s\t = Training   runtime\n",
      "\t1.51s\t = Validation runtime\n",
      "Repeating k-fold bagging: 18/25\n",
      "Fitting model: LightGBMLarge_BAG_L1 ... Training model for up to 31843.98s of the 31843.98s of remaining time.\n",
      "\tFitting 15 child models (S18F1 - S18F15) | Fitting with ParallelLocalFoldFittingStrategy (8 workers, per: cpus=1, gpus=0, memory=0.22%)\n",
      "\t0.8362\t = Validation score   (balanced_accuracy)\n",
      "\t528.43s\t = Training   runtime\n",
      "\t1.62s\t = Validation runtime\n",
      "Repeating k-fold bagging: 19/25\n",
      "Fitting model: LightGBMLarge_BAG_L1 ... Training model for up to 31803.15s of the 31803.15s of remaining time.\n",
      "\tFitting 15 child models (S19F1 - S19F15) | Fitting with ParallelLocalFoldFittingStrategy (8 workers, per: cpus=1, gpus=0, memory=0.22%)\n",
      "\t0.8394\t = Validation score   (balanced_accuracy)\n",
      "\t558.45s\t = Training   runtime\n",
      "\t1.69s\t = Validation runtime\n",
      "Repeating k-fold bagging: 20/25\n",
      "Fitting model: LightGBMLarge_BAG_L1 ... Training model for up to 31762.54s of the 31762.53s of remaining time.\n",
      "\tFitting 15 child models (S20F1 - S20F15) | Fitting with ParallelLocalFoldFittingStrategy (8 workers, per: cpus=1, gpus=0, memory=0.22%)\n",
      "\t0.8375\t = Validation score   (balanced_accuracy)\n",
      "\t587.76s\t = Training   runtime\n",
      "\t1.76s\t = Validation runtime\n",
      "Repeating k-fold bagging: 21/25\n",
      "Fitting model: LightGBMLarge_BAG_L1 ... Training model for up to 31722.44s of the 31722.43s of remaining time.\n",
      "\tFitting 15 child models (S21F1 - S21F15) | Fitting with ParallelLocalFoldFittingStrategy (8 workers, per: cpus=1, gpus=0, memory=0.22%)\n",
      "\t0.8381\t = Validation score   (balanced_accuracy)\n",
      "\t617.1s\t = Training   runtime\n",
      "\t1.84s\t = Validation runtime\n",
      "Repeating k-fold bagging: 22/25\n",
      "Fitting model: LightGBMLarge_BAG_L1 ... Training model for up to 31682.36s of the 31682.36s of remaining time.\n",
      "\tFitting 15 child models (S22F1 - S22F15) | Fitting with ParallelLocalFoldFittingStrategy (8 workers, per: cpus=1, gpus=0, memory=0.22%)\n",
      "\t0.8369\t = Validation score   (balanced_accuracy)\n",
      "\t645.87s\t = Training   runtime\n",
      "\t1.94s\t = Validation runtime\n",
      "Repeating k-fold bagging: 23/25\n",
      "Fitting model: LightGBMLarge_BAG_L1 ... Training model for up to 31642.79s of the 31642.79s of remaining time.\n",
      "\tFitting 15 child models (S23F1 - S23F15) | Fitting with ParallelLocalFoldFittingStrategy (8 workers, per: cpus=1, gpus=0, memory=0.22%)\n",
      "\t0.8375\t = Validation score   (balanced_accuracy)\n",
      "\t676.4s\t = Training   runtime\n",
      "\t2.02s\t = Validation runtime\n",
      "Repeating k-fold bagging: 24/25\n",
      "Fitting model: LightGBMLarge_BAG_L1 ... Training model for up to 31601.34s of the 31601.34s of remaining time.\n",
      "\tFitting 15 child models (S24F1 - S24F15) | Fitting with ParallelLocalFoldFittingStrategy (8 workers, per: cpus=1, gpus=0, memory=0.22%)\n",
      "\t0.8394\t = Validation score   (balanced_accuracy)\n",
      "\t705.54s\t = Training   runtime\n",
      "\t2.1s\t = Validation runtime\n",
      "Repeating k-fold bagging: 25/25\n",
      "Fitting model: LightGBMLarge_BAG_L1 ... Training model for up to 31561.94s of the 31561.93s of remaining time.\n",
      "\tFitting 15 child models (S25F1 - S25F15) | Fitting with ParallelLocalFoldFittingStrategy (8 workers, per: cpus=1, gpus=0, memory=0.23%)\n",
      "\t0.8406\t = Validation score   (balanced_accuracy)\n",
      "\t736.32s\t = Training   runtime\n",
      "\t2.18s\t = Validation runtime\n",
      "Completed 25/25 k-fold bagging repeats ...\n",
      "Fitting model: WeightedEnsemble_L2 ... Training model for up to 3252.49s of the 31520.58s of remaining time.\n",
      "\tEnsemble Weights: {'LightGBMLarge_BAG_L1': 1.0}\n",
      "\t0.8406\t = Validation score   (balanced_accuracy)\n",
      "\t0.02s\t = Training   runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "AutoGluon training complete, total runtime = 1004.53s ... Best model: \"WeightedEnsemble_L2\"\n",
      "TabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"output\\autogluon\\20240114_114214\")\n"
     ]
    }
   ],
   "source": [
    "save_path = path.join(OUTPUT_DIR_AUTOGLUON, UNIQUE_ID)\n",
    "predictor = TabularPredictor(\n",
    "    label=\"class\",\n",
    "    path=save_path,\n",
    "    eval_metric=\"balanced_accuracy\",\n",
    "    problem_type=\"binary\",\n",
    ").fit(\n",
    "    train_data=train_data_pd,\n",
    "    # tuning_data=valid_data_pd,\n",
    "    time_limit=TRAIN_TIME_LIMIT_AUTOGLUON,\n",
    "    presets=\"best_quality\",\n",
    "    hyperparameters=\"default\",\n",
    "    fit_weighted_ensemble=True,\n",
    "    fit_full_last_level_weighted_ensemble=True,\n",
    "    full_weighted_ensemble_additionally=True,\n",
    "    num_bag_folds=15,\n",
    "    num_bag_sets=25,\n",
    "    num_stack_levels=3,\n",
    "    auto_stack=True,\n",
    "    dynamic_stacking=True,\n",
    "    feature_generator=\"auto\",\n",
    "    hyperparameter_tune_kwargs={\n",
    "        # \"scheduler\": \"local\",\n",
    "        \"searcher\": \"auto\",\n",
    "        \"time_out\": 1200,\n",
    "        \"num_trials\": 30,\n",
    "        # \"early_stopping\": True,\n",
    "    },\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>model</th>\n",
       "      <th>score_val</th>\n",
       "      <th>eval_metric</th>\n",
       "      <th>pred_time_val</th>\n",
       "      <th>fit_time</th>\n",
       "      <th>pred_time_val_marginal</th>\n",
       "      <th>fit_time_marginal</th>\n",
       "      <th>stack_level</th>\n",
       "      <th>can_infer</th>\n",
       "      <th>fit_order</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>LightGBMLarge_BAG_L1</td>\n",
       "      <td>0.840616</td>\n",
       "      <td>balanced_accuracy</td>\n",
       "      <td>2.180954</td>\n",
       "      <td>736.321142</td>\n",
       "      <td>2.180954</td>\n",
       "      <td>736.321142</td>\n",
       "      <td>1</td>\n",
       "      <td>True</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>WeightedEnsemble_L2</td>\n",
       "      <td>0.840616</td>\n",
       "      <td>balanced_accuracy</td>\n",
       "      <td>2.192951</td>\n",
       "      <td>736.343144</td>\n",
       "      <td>0.011997</td>\n",
       "      <td>0.022002</td>\n",
       "      <td>2</td>\n",
       "      <td>True</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                  model  score_val        eval_metric  pred_time_val  \\\n",
       "0  LightGBMLarge_BAG_L1   0.840616  balanced_accuracy       2.180954   \n",
       "1   WeightedEnsemble_L2   0.840616  balanced_accuracy       2.192951   \n",
       "\n",
       "     fit_time  pred_time_val_marginal  fit_time_marginal  stack_level  \\\n",
       "0  736.321142                2.180954         736.321142            1   \n",
       "1  736.343144                0.011997           0.022002            2   \n",
       "\n",
       "   can_infer  fit_order  \n",
       "0       True          1  \n",
       "1       True          2  "
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictor.leaderboard()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'balanced_accuracy': 0.8299617413918132,\n",
       " 'accuracy': 0.83,\n",
       " 'mcc': 0.6599234827836263,\n",
       " 'roc_auc': 0.8939761446325423,\n",
       " 'f1': 0.8274111675126904,\n",
       " 'precision': 0.8274111675126904,\n",
       " 'recall': 0.8274111675126904}"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictor.evaluate(valid_data_pd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_path_proba = path.join(\n",
    "    OUTPUT_DIR_AUTOGLUON, UNIQUE_ID, \"autogluon_model_proba.txt\"\n",
    ")\n",
    "dump_proba(predictor, pd.DataFrame(test_x), output_path_proba)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MLJar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 1.  1.  1. ...  1. -1. -1.]\n",
      "AutoML directory: output\\mljar\\20240114_114214\\tmp\n",
      "The task is binary_classification with evaluation metric f1\n",
      "AutoML will use algorithms: ['Decision Tree', 'Linear', 'Random Forest', 'Extra Trees', 'LightGBM', 'Xgboost', 'CatBoost', 'Neural Network', 'Nearest Neighbors']\n",
      "AutoML will stack models\n",
      "AutoML will ensemble available models\n",
      "AutoML steps: ['adjust_validation', 'simple_algorithms', 'default_algorithms', 'not_so_random', 'golden_features', 'kmeans_features', 'insert_random_feature', 'features_selection', 'hill_climbing_1', 'hill_climbing_2', 'boost_on_errors', 'ensemble', 'stack', 'ensemble_stacked']\n",
      "* Step adjust_validation will try to check up to 1 model\n",
      "1_DecisionTree f1 0.742515 trained in 3.76 seconds\n",
      "Adjust validation. Remove: 1_DecisionTree\n",
      "Validation strategy: 10-fold CV Shuffle,Stratify\n",
      "* Step simple_algorithms will try to check up to 4 models\n",
      "1_DecisionTree f1 0.743881 trained in 6.52 seconds\n",
      "2_DecisionTree f1 0.660644 trained in 6.99 seconds\n",
      "3_DecisionTree f1 0.638272 trained in 6.24 seconds\n",
      "4_Linear f1 0.63586 trained in 15.42 seconds\n",
      "* Step default_algorithms will try to check up to 7 models\n",
      "5_Default_LightGBM f1 0.83549 trained in 18.9 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\YanPC\\Downloads\\automl_hw2-master\\.venv\\lib\\site-packages\\xgboost\\training.py:38: UserWarning: `feval` is deprecated, use `custom_metric` instead.  They have different behavior when custom objective is also used.See https://xgboost.readthedocs.io/en/latest/tutorials/custom_metric_obj.html for details on the `custom_metric`.\n",
      "c:\\Users\\YanPC\\Downloads\\automl_hw2-master\\.venv\\lib\\site-packages\\xgboost\\training.py:38: UserWarning: `feval` is deprecated, use `custom_metric` instead.  They have different behavior when custom objective is also used.See https://xgboost.readthedocs.io/en/latest/tutorials/custom_metric_obj.html for details on the `custom_metric`.\n",
      "c:\\Users\\YanPC\\Downloads\\automl_hw2-master\\.venv\\lib\\site-packages\\xgboost\\training.py:38: UserWarning: `feval` is deprecated, use `custom_metric` instead.  They have different behavior when custom objective is also used.See https://xgboost.readthedocs.io/en/latest/tutorials/custom_metric_obj.html for details on the `custom_metric`.\n",
      "c:\\Users\\YanPC\\Downloads\\automl_hw2-master\\.venv\\lib\\site-packages\\xgboost\\training.py:38: UserWarning: `feval` is deprecated, use `custom_metric` instead.  They have different behavior when custom objective is also used.See https://xgboost.readthedocs.io/en/latest/tutorials/custom_metric_obj.html for details on the `custom_metric`.\n",
      "c:\\Users\\YanPC\\Downloads\\automl_hw2-master\\.venv\\lib\\site-packages\\xgboost\\training.py:38: UserWarning: `feval` is deprecated, use `custom_metric` instead.  They have different behavior when custom objective is also used.See https://xgboost.readthedocs.io/en/latest/tutorials/custom_metric_obj.html for details on the `custom_metric`.\n",
      "c:\\Users\\YanPC\\Downloads\\automl_hw2-master\\.venv\\lib\\site-packages\\xgboost\\training.py:38: UserWarning: `feval` is deprecated, use `custom_metric` instead.  They have different behavior when custom objective is also used.See https://xgboost.readthedocs.io/en/latest/tutorials/custom_metric_obj.html for details on the `custom_metric`.\n",
      "c:\\Users\\YanPC\\Downloads\\automl_hw2-master\\.venv\\lib\\site-packages\\xgboost\\training.py:38: UserWarning: `feval` is deprecated, use `custom_metric` instead.  They have different behavior when custom objective is also used.See https://xgboost.readthedocs.io/en/latest/tutorials/custom_metric_obj.html for details on the `custom_metric`.\n",
      "c:\\Users\\YanPC\\Downloads\\automl_hw2-master\\.venv\\lib\\site-packages\\xgboost\\training.py:38: UserWarning: `feval` is deprecated, use `custom_metric` instead.  They have different behavior when custom objective is also used.See https://xgboost.readthedocs.io/en/latest/tutorials/custom_metric_obj.html for details on the `custom_metric`.\n",
      "c:\\Users\\YanPC\\Downloads\\automl_hw2-master\\.venv\\lib\\site-packages\\xgboost\\training.py:38: UserWarning: `feval` is deprecated, use `custom_metric` instead.  They have different behavior when custom objective is also used.See https://xgboost.readthedocs.io/en/latest/tutorials/custom_metric_obj.html for details on the `custom_metric`.\n",
      "c:\\Users\\YanPC\\Downloads\\automl_hw2-master\\.venv\\lib\\site-packages\\xgboost\\training.py:38: UserWarning: `feval` is deprecated, use `custom_metric` instead.  They have different behavior when custom objective is also used.See https://xgboost.readthedocs.io/en/latest/tutorials/custom_metric_obj.html for details on the `custom_metric`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6_Default_Xgboost f1 0.815871 trained in 25.23 seconds\n",
      "7_Default_CatBoost f1 0.836836 trained in 16.83 seconds\n",
      "8_Default_NeuralNetwork f1 0.675444 trained in 13.05 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\YanPC\\Downloads\\automl_hw2-master\\.venv\\lib\\site-packages\\supervised\\utils\\learning_curves.py:113: FutureWarning: The behavior of Series.argmax/argmin with skipna=False and NAs, or with all-NAs is deprecated. In a future version this will raise ValueError.\n",
      "c:\\Users\\YanPC\\Downloads\\automl_hw2-master\\.venv\\lib\\site-packages\\supervised\\utils\\learning_curves.py:113: FutureWarning: The behavior of Series.argmax/argmin with skipna=False and NAs, or with all-NAs is deprecated. In a future version this will raise ValueError.\n",
      "c:\\Users\\YanPC\\Downloads\\automl_hw2-master\\.venv\\lib\\site-packages\\supervised\\utils\\learning_curves.py:113: FutureWarning: The behavior of Series.argmax/argmin with skipna=False and NAs, or with all-NAs is deprecated. In a future version this will raise ValueError.\n",
      "c:\\Users\\YanPC\\Downloads\\automl_hw2-master\\.venv\\lib\\site-packages\\supervised\\utils\\learning_curves.py:113: FutureWarning: The behavior of Series.argmax/argmin with skipna=False and NAs, or with all-NAs is deprecated. In a future version this will raise ValueError.\n",
      "c:\\Users\\YanPC\\Downloads\\automl_hw2-master\\.venv\\lib\\site-packages\\supervised\\utils\\learning_curves.py:113: FutureWarning: The behavior of Series.argmax/argmin with skipna=False and NAs, or with all-NAs is deprecated. In a future version this will raise ValueError.\n",
      "c:\\Users\\YanPC\\Downloads\\automl_hw2-master\\.venv\\lib\\site-packages\\supervised\\utils\\learning_curves.py:113: FutureWarning: The behavior of Series.argmax/argmin with skipna=False and NAs, or with all-NAs is deprecated. In a future version this will raise ValueError.\n",
      "c:\\Users\\YanPC\\Downloads\\automl_hw2-master\\.venv\\lib\\site-packages\\supervised\\utils\\learning_curves.py:113: FutureWarning: The behavior of Series.argmax/argmin with skipna=False and NAs, or with all-NAs is deprecated. In a future version this will raise ValueError.\n",
      "c:\\Users\\YanPC\\Downloads\\automl_hw2-master\\.venv\\lib\\site-packages\\supervised\\utils\\learning_curves.py:113: FutureWarning: The behavior of Series.argmax/argmin with skipna=False and NAs, or with all-NAs is deprecated. In a future version this will raise ValueError.\n",
      "c:\\Users\\YanPC\\Downloads\\automl_hw2-master\\.venv\\lib\\site-packages\\supervised\\utils\\learning_curves.py:113: FutureWarning: The behavior of Series.argmax/argmin with skipna=False and NAs, or with all-NAs is deprecated. In a future version this will raise ValueError.\n",
      "c:\\Users\\YanPC\\Downloads\\automl_hw2-master\\.venv\\lib\\site-packages\\supervised\\utils\\learning_curves.py:113: FutureWarning: The behavior of Series.argmax/argmin with skipna=False and NAs, or with all-NAs is deprecated. In a future version this will raise ValueError.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9_Default_RandomForest f1 0.78765 trained in 23.48 seconds\n",
      "10_Default_ExtraTrees f1 0.743227 trained in 16.59 seconds\n",
      "11_Default_NearestNeighbors f1 0.639112 trained in 9.4 seconds\n",
      "* Step not_so_random will try to check up to 61 models\n",
      "21_LightGBM f1 0.815142 trained in 18.61 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\YanPC\\Downloads\\automl_hw2-master\\.venv\\lib\\site-packages\\xgboost\\training.py:38: UserWarning: `feval` is deprecated, use `custom_metric` instead.  They have different behavior when custom objective is also used.See https://xgboost.readthedocs.io/en/latest/tutorials/custom_metric_obj.html for details on the `custom_metric`.\n",
      "c:\\Users\\YanPC\\Downloads\\automl_hw2-master\\.venv\\lib\\site-packages\\xgboost\\training.py:38: UserWarning: `feval` is deprecated, use `custom_metric` instead.  They have different behavior when custom objective is also used.See https://xgboost.readthedocs.io/en/latest/tutorials/custom_metric_obj.html for details on the `custom_metric`.\n",
      "c:\\Users\\YanPC\\Downloads\\automl_hw2-master\\.venv\\lib\\site-packages\\xgboost\\training.py:38: UserWarning: `feval` is deprecated, use `custom_metric` instead.  They have different behavior when custom objective is also used.See https://xgboost.readthedocs.io/en/latest/tutorials/custom_metric_obj.html for details on the `custom_metric`.\n",
      "c:\\Users\\YanPC\\Downloads\\automl_hw2-master\\.venv\\lib\\site-packages\\xgboost\\training.py:38: UserWarning: `feval` is deprecated, use `custom_metric` instead.  They have different behavior when custom objective is also used.See https://xgboost.readthedocs.io/en/latest/tutorials/custom_metric_obj.html for details on the `custom_metric`.\n",
      "c:\\Users\\YanPC\\Downloads\\automl_hw2-master\\.venv\\lib\\site-packages\\xgboost\\training.py:38: UserWarning: `feval` is deprecated, use `custom_metric` instead.  They have different behavior when custom objective is also used.See https://xgboost.readthedocs.io/en/latest/tutorials/custom_metric_obj.html for details on the `custom_metric`.\n",
      "c:\\Users\\YanPC\\Downloads\\automl_hw2-master\\.venv\\lib\\site-packages\\xgboost\\training.py:38: UserWarning: `feval` is deprecated, use `custom_metric` instead.  They have different behavior when custom objective is also used.See https://xgboost.readthedocs.io/en/latest/tutorials/custom_metric_obj.html for details on the `custom_metric`.\n",
      "c:\\Users\\YanPC\\Downloads\\automl_hw2-master\\.venv\\lib\\site-packages\\xgboost\\training.py:38: UserWarning: `feval` is deprecated, use `custom_metric` instead.  They have different behavior when custom objective is also used.See https://xgboost.readthedocs.io/en/latest/tutorials/custom_metric_obj.html for details on the `custom_metric`.\n",
      "c:\\Users\\YanPC\\Downloads\\automl_hw2-master\\.venv\\lib\\site-packages\\xgboost\\training.py:38: UserWarning: `feval` is deprecated, use `custom_metric` instead.  They have different behavior when custom objective is also used.See https://xgboost.readthedocs.io/en/latest/tutorials/custom_metric_obj.html for details on the `custom_metric`.\n",
      "c:\\Users\\YanPC\\Downloads\\automl_hw2-master\\.venv\\lib\\site-packages\\xgboost\\training.py:38: UserWarning: `feval` is deprecated, use `custom_metric` instead.  They have different behavior when custom objective is also used.See https://xgboost.readthedocs.io/en/latest/tutorials/custom_metric_obj.html for details on the `custom_metric`.\n",
      "c:\\Users\\YanPC\\Downloads\\automl_hw2-master\\.venv\\lib\\site-packages\\xgboost\\training.py:38: UserWarning: `feval` is deprecated, use `custom_metric` instead.  They have different behavior when custom objective is also used.See https://xgboost.readthedocs.io/en/latest/tutorials/custom_metric_obj.html for details on the `custom_metric`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12_Xgboost f1 0.794984 trained in 23.32 seconds\n",
      "30_CatBoost f1 0.841133 trained in 20.51 seconds\n",
      "39_RandomForest f1 0.811321 trained in 28.48 seconds\n",
      "48_ExtraTrees f1 0.783558 trained in 20.52 seconds\n",
      "57_NeuralNetwork f1 0.559256 trained in 14.77 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\YanPC\\Downloads\\automl_hw2-master\\.venv\\lib\\site-packages\\supervised\\utils\\learning_curves.py:113: FutureWarning: The behavior of Series.argmax/argmin with skipna=False and NAs, or with all-NAs is deprecated. In a future version this will raise ValueError.\n",
      "c:\\Users\\YanPC\\Downloads\\automl_hw2-master\\.venv\\lib\\site-packages\\supervised\\utils\\learning_curves.py:113: FutureWarning: The behavior of Series.argmax/argmin with skipna=False and NAs, or with all-NAs is deprecated. In a future version this will raise ValueError.\n",
      "c:\\Users\\YanPC\\Downloads\\automl_hw2-master\\.venv\\lib\\site-packages\\supervised\\utils\\learning_curves.py:113: FutureWarning: The behavior of Series.argmax/argmin with skipna=False and NAs, or with all-NAs is deprecated. In a future version this will raise ValueError.\n",
      "c:\\Users\\YanPC\\Downloads\\automl_hw2-master\\.venv\\lib\\site-packages\\supervised\\utils\\learning_curves.py:113: FutureWarning: The behavior of Series.argmax/argmin with skipna=False and NAs, or with all-NAs is deprecated. In a future version this will raise ValueError.\n",
      "c:\\Users\\YanPC\\Downloads\\automl_hw2-master\\.venv\\lib\\site-packages\\supervised\\utils\\learning_curves.py:113: FutureWarning: The behavior of Series.argmax/argmin with skipna=False and NAs, or with all-NAs is deprecated. In a future version this will raise ValueError.\n",
      "c:\\Users\\YanPC\\Downloads\\automl_hw2-master\\.venv\\lib\\site-packages\\supervised\\utils\\learning_curves.py:113: FutureWarning: The behavior of Series.argmax/argmin with skipna=False and NAs, or with all-NAs is deprecated. In a future version this will raise ValueError.\n",
      "c:\\Users\\YanPC\\Downloads\\automl_hw2-master\\.venv\\lib\\site-packages\\supervised\\utils\\learning_curves.py:113: FutureWarning: The behavior of Series.argmax/argmin with skipna=False and NAs, or with all-NAs is deprecated. In a future version this will raise ValueError.\n",
      "c:\\Users\\YanPC\\Downloads\\automl_hw2-master\\.venv\\lib\\site-packages\\supervised\\utils\\learning_curves.py:113: FutureWarning: The behavior of Series.argmax/argmin with skipna=False and NAs, or with all-NAs is deprecated. In a future version this will raise ValueError.\n",
      "c:\\Users\\YanPC\\Downloads\\automl_hw2-master\\.venv\\lib\\site-packages\\supervised\\utils\\learning_curves.py:113: FutureWarning: The behavior of Series.argmax/argmin with skipna=False and NAs, or with all-NAs is deprecated. In a future version this will raise ValueError.\n",
      "c:\\Users\\YanPC\\Downloads\\automl_hw2-master\\.venv\\lib\\site-packages\\supervised\\utils\\learning_curves.py:113: FutureWarning: The behavior of Series.argmax/argmin with skipna=False and NAs, or with all-NAs is deprecated. In a future version this will raise ValueError.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "66_NearestNeighbors f1 0.652527 trained in 10.59 seconds\n",
      "22_LightGBM f1 0.823965 trained in 18.99 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\YanPC\\Downloads\\automl_hw2-master\\.venv\\lib\\site-packages\\xgboost\\training.py:38: UserWarning: `feval` is deprecated, use `custom_metric` instead.  They have different behavior when custom objective is also used.See https://xgboost.readthedocs.io/en/latest/tutorials/custom_metric_obj.html for details on the `custom_metric`.\n",
      "c:\\Users\\YanPC\\Downloads\\automl_hw2-master\\.venv\\lib\\site-packages\\xgboost\\training.py:38: UserWarning: `feval` is deprecated, use `custom_metric` instead.  They have different behavior when custom objective is also used.See https://xgboost.readthedocs.io/en/latest/tutorials/custom_metric_obj.html for details on the `custom_metric`.\n",
      "c:\\Users\\YanPC\\Downloads\\automl_hw2-master\\.venv\\lib\\site-packages\\xgboost\\training.py:38: UserWarning: `feval` is deprecated, use `custom_metric` instead.  They have different behavior when custom objective is also used.See https://xgboost.readthedocs.io/en/latest/tutorials/custom_metric_obj.html for details on the `custom_metric`.\n",
      "c:\\Users\\YanPC\\Downloads\\automl_hw2-master\\.venv\\lib\\site-packages\\xgboost\\training.py:38: UserWarning: `feval` is deprecated, use `custom_metric` instead.  They have different behavior when custom objective is also used.See https://xgboost.readthedocs.io/en/latest/tutorials/custom_metric_obj.html for details on the `custom_metric`.\n",
      "c:\\Users\\YanPC\\Downloads\\automl_hw2-master\\.venv\\lib\\site-packages\\xgboost\\training.py:38: UserWarning: `feval` is deprecated, use `custom_metric` instead.  They have different behavior when custom objective is also used.See https://xgboost.readthedocs.io/en/latest/tutorials/custom_metric_obj.html for details on the `custom_metric`.\n",
      "c:\\Users\\YanPC\\Downloads\\automl_hw2-master\\.venv\\lib\\site-packages\\xgboost\\training.py:38: UserWarning: `feval` is deprecated, use `custom_metric` instead.  They have different behavior when custom objective is also used.See https://xgboost.readthedocs.io/en/latest/tutorials/custom_metric_obj.html for details on the `custom_metric`.\n",
      "c:\\Users\\YanPC\\Downloads\\automl_hw2-master\\.venv\\lib\\site-packages\\xgboost\\training.py:38: UserWarning: `feval` is deprecated, use `custom_metric` instead.  They have different behavior when custom objective is also used.See https://xgboost.readthedocs.io/en/latest/tutorials/custom_metric_obj.html for details on the `custom_metric`.\n",
      "c:\\Users\\YanPC\\Downloads\\automl_hw2-master\\.venv\\lib\\site-packages\\xgboost\\training.py:38: UserWarning: `feval` is deprecated, use `custom_metric` instead.  They have different behavior when custom objective is also used.See https://xgboost.readthedocs.io/en/latest/tutorials/custom_metric_obj.html for details on the `custom_metric`.\n",
      "c:\\Users\\YanPC\\Downloads\\automl_hw2-master\\.venv\\lib\\site-packages\\xgboost\\training.py:38: UserWarning: `feval` is deprecated, use `custom_metric` instead.  They have different behavior when custom objective is also used.See https://xgboost.readthedocs.io/en/latest/tutorials/custom_metric_obj.html for details on the `custom_metric`.\n",
      "c:\\Users\\YanPC\\Downloads\\automl_hw2-master\\.venv\\lib\\site-packages\\xgboost\\training.py:38: UserWarning: `feval` is deprecated, use `custom_metric` instead.  They have different behavior when custom objective is also used.See https://xgboost.readthedocs.io/en/latest/tutorials/custom_metric_obj.html for details on the `custom_metric`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "13_Xgboost f1 0.714286 trained in 27.42 seconds\n",
      "31_CatBoost f1 0.838471 trained in 19.26 seconds\n",
      "40_RandomForest f1 0.753433 trained in 23.1 seconds\n",
      "49_ExtraTrees f1 0.712846 trained in 19.3 seconds\n",
      "58_NeuralNetwork f1 0.708504 trained in 18.35 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\YanPC\\Downloads\\automl_hw2-master\\.venv\\lib\\site-packages\\supervised\\utils\\learning_curves.py:113: FutureWarning: The behavior of Series.argmax/argmin with skipna=False and NAs, or with all-NAs is deprecated. In a future version this will raise ValueError.\n",
      "c:\\Users\\YanPC\\Downloads\\automl_hw2-master\\.venv\\lib\\site-packages\\supervised\\utils\\learning_curves.py:113: FutureWarning: The behavior of Series.argmax/argmin with skipna=False and NAs, or with all-NAs is deprecated. In a future version this will raise ValueError.\n",
      "c:\\Users\\YanPC\\Downloads\\automl_hw2-master\\.venv\\lib\\site-packages\\supervised\\utils\\learning_curves.py:113: FutureWarning: The behavior of Series.argmax/argmin with skipna=False and NAs, or with all-NAs is deprecated. In a future version this will raise ValueError.\n",
      "c:\\Users\\YanPC\\Downloads\\automl_hw2-master\\.venv\\lib\\site-packages\\supervised\\utils\\learning_curves.py:113: FutureWarning: The behavior of Series.argmax/argmin with skipna=False and NAs, or with all-NAs is deprecated. In a future version this will raise ValueError.\n",
      "c:\\Users\\YanPC\\Downloads\\automl_hw2-master\\.venv\\lib\\site-packages\\supervised\\utils\\learning_curves.py:113: FutureWarning: The behavior of Series.argmax/argmin with skipna=False and NAs, or with all-NAs is deprecated. In a future version this will raise ValueError.\n",
      "c:\\Users\\YanPC\\Downloads\\automl_hw2-master\\.venv\\lib\\site-packages\\supervised\\utils\\learning_curves.py:113: FutureWarning: The behavior of Series.argmax/argmin with skipna=False and NAs, or with all-NAs is deprecated. In a future version this will raise ValueError.\n",
      "c:\\Users\\YanPC\\Downloads\\automl_hw2-master\\.venv\\lib\\site-packages\\supervised\\utils\\learning_curves.py:113: FutureWarning: The behavior of Series.argmax/argmin with skipna=False and NAs, or with all-NAs is deprecated. In a future version this will raise ValueError.\n",
      "c:\\Users\\YanPC\\Downloads\\automl_hw2-master\\.venv\\lib\\site-packages\\supervised\\utils\\learning_curves.py:113: FutureWarning: The behavior of Series.argmax/argmin with skipna=False and NAs, or with all-NAs is deprecated. In a future version this will raise ValueError.\n",
      "c:\\Users\\YanPC\\Downloads\\automl_hw2-master\\.venv\\lib\\site-packages\\supervised\\utils\\learning_curves.py:113: FutureWarning: The behavior of Series.argmax/argmin with skipna=False and NAs, or with all-NAs is deprecated. In a future version this will raise ValueError.\n",
      "c:\\Users\\YanPC\\Downloads\\automl_hw2-master\\.venv\\lib\\site-packages\\supervised\\utils\\learning_curves.py:113: FutureWarning: The behavior of Series.argmax/argmin with skipna=False and NAs, or with all-NAs is deprecated. In a future version this will raise ValueError.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "67_NearestNeighbors f1 0.620647 trained in 11.26 seconds\n",
      "23_LightGBM f1 0.838988 trained in 20.68 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\YanPC\\Downloads\\automl_hw2-master\\.venv\\lib\\site-packages\\xgboost\\training.py:38: UserWarning: `feval` is deprecated, use `custom_metric` instead.  They have different behavior when custom objective is also used.See https://xgboost.readthedocs.io/en/latest/tutorials/custom_metric_obj.html for details on the `custom_metric`.\n",
      "c:\\Users\\YanPC\\Downloads\\automl_hw2-master\\.venv\\lib\\site-packages\\xgboost\\training.py:38: UserWarning: `feval` is deprecated, use `custom_metric` instead.  They have different behavior when custom objective is also used.See https://xgboost.readthedocs.io/en/latest/tutorials/custom_metric_obj.html for details on the `custom_metric`.\n",
      "c:\\Users\\YanPC\\Downloads\\automl_hw2-master\\.venv\\lib\\site-packages\\xgboost\\training.py:38: UserWarning: `feval` is deprecated, use `custom_metric` instead.  They have different behavior when custom objective is also used.See https://xgboost.readthedocs.io/en/latest/tutorials/custom_metric_obj.html for details on the `custom_metric`.\n",
      "c:\\Users\\YanPC\\Downloads\\automl_hw2-master\\.venv\\lib\\site-packages\\xgboost\\training.py:38: UserWarning: `feval` is deprecated, use `custom_metric` instead.  They have different behavior when custom objective is also used.See https://xgboost.readthedocs.io/en/latest/tutorials/custom_metric_obj.html for details on the `custom_metric`.\n",
      "c:\\Users\\YanPC\\Downloads\\automl_hw2-master\\.venv\\lib\\site-packages\\xgboost\\training.py:38: UserWarning: `feval` is deprecated, use `custom_metric` instead.  They have different behavior when custom objective is also used.See https://xgboost.readthedocs.io/en/latest/tutorials/custom_metric_obj.html for details on the `custom_metric`.\n",
      "c:\\Users\\YanPC\\Downloads\\automl_hw2-master\\.venv\\lib\\site-packages\\xgboost\\training.py:38: UserWarning: `feval` is deprecated, use `custom_metric` instead.  They have different behavior when custom objective is also used.See https://xgboost.readthedocs.io/en/latest/tutorials/custom_metric_obj.html for details on the `custom_metric`.\n",
      "c:\\Users\\YanPC\\Downloads\\automl_hw2-master\\.venv\\lib\\site-packages\\xgboost\\training.py:38: UserWarning: `feval` is deprecated, use `custom_metric` instead.  They have different behavior when custom objective is also used.See https://xgboost.readthedocs.io/en/latest/tutorials/custom_metric_obj.html for details on the `custom_metric`.\n",
      "c:\\Users\\YanPC\\Downloads\\automl_hw2-master\\.venv\\lib\\site-packages\\xgboost\\training.py:38: UserWarning: `feval` is deprecated, use `custom_metric` instead.  They have different behavior when custom objective is also used.See https://xgboost.readthedocs.io/en/latest/tutorials/custom_metric_obj.html for details on the `custom_metric`.\n",
      "c:\\Users\\YanPC\\Downloads\\automl_hw2-master\\.venv\\lib\\site-packages\\xgboost\\training.py:38: UserWarning: `feval` is deprecated, use `custom_metric` instead.  They have different behavior when custom objective is also used.See https://xgboost.readthedocs.io/en/latest/tutorials/custom_metric_obj.html for details on the `custom_metric`.\n",
      "c:\\Users\\YanPC\\Downloads\\automl_hw2-master\\.venv\\lib\\site-packages\\xgboost\\training.py:38: UserWarning: `feval` is deprecated, use `custom_metric` instead.  They have different behavior when custom objective is also used.See https://xgboost.readthedocs.io/en/latest/tutorials/custom_metric_obj.html for details on the `custom_metric`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "14_Xgboost f1 0.783387 trained in 31.94 seconds\n",
      "32_CatBoost f1 0.849148 trained in 44.36 seconds\n",
      "41_RandomForest f1 0.745993 trained in 26.77 seconds\n",
      "50_ExtraTrees f1 0.710237 trained in 19.53 seconds\n",
      "59_NeuralNetwork f1 0.681959 trained in 16.36 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\YanPC\\Downloads\\automl_hw2-master\\.venv\\lib\\site-packages\\supervised\\utils\\learning_curves.py:113: FutureWarning: The behavior of Series.argmax/argmin with skipna=False and NAs, or with all-NAs is deprecated. In a future version this will raise ValueError.\n",
      "c:\\Users\\YanPC\\Downloads\\automl_hw2-master\\.venv\\lib\\site-packages\\supervised\\utils\\learning_curves.py:113: FutureWarning: The behavior of Series.argmax/argmin with skipna=False and NAs, or with all-NAs is deprecated. In a future version this will raise ValueError.\n",
      "c:\\Users\\YanPC\\Downloads\\automl_hw2-master\\.venv\\lib\\site-packages\\supervised\\utils\\learning_curves.py:113: FutureWarning: The behavior of Series.argmax/argmin with skipna=False and NAs, or with all-NAs is deprecated. In a future version this will raise ValueError.\n",
      "c:\\Users\\YanPC\\Downloads\\automl_hw2-master\\.venv\\lib\\site-packages\\supervised\\utils\\learning_curves.py:113: FutureWarning: The behavior of Series.argmax/argmin with skipna=False and NAs, or with all-NAs is deprecated. In a future version this will raise ValueError.\n",
      "c:\\Users\\YanPC\\Downloads\\automl_hw2-master\\.venv\\lib\\site-packages\\supervised\\utils\\learning_curves.py:113: FutureWarning: The behavior of Series.argmax/argmin with skipna=False and NAs, or with all-NAs is deprecated. In a future version this will raise ValueError.\n",
      "c:\\Users\\YanPC\\Downloads\\automl_hw2-master\\.venv\\lib\\site-packages\\supervised\\utils\\learning_curves.py:113: FutureWarning: The behavior of Series.argmax/argmin with skipna=False and NAs, or with all-NAs is deprecated. In a future version this will raise ValueError.\n",
      "c:\\Users\\YanPC\\Downloads\\automl_hw2-master\\.venv\\lib\\site-packages\\supervised\\utils\\learning_curves.py:113: FutureWarning: The behavior of Series.argmax/argmin with skipna=False and NAs, or with all-NAs is deprecated. In a future version this will raise ValueError.\n",
      "c:\\Users\\YanPC\\Downloads\\automl_hw2-master\\.venv\\lib\\site-packages\\supervised\\utils\\learning_curves.py:113: FutureWarning: The behavior of Series.argmax/argmin with skipna=False and NAs, or with all-NAs is deprecated. In a future version this will raise ValueError.\n",
      "c:\\Users\\YanPC\\Downloads\\automl_hw2-master\\.venv\\lib\\site-packages\\supervised\\utils\\learning_curves.py:113: FutureWarning: The behavior of Series.argmax/argmin with skipna=False and NAs, or with all-NAs is deprecated. In a future version this will raise ValueError.\n",
      "c:\\Users\\YanPC\\Downloads\\automl_hw2-master\\.venv\\lib\\site-packages\\supervised\\utils\\learning_curves.py:113: FutureWarning: The behavior of Series.argmax/argmin with skipna=False and NAs, or with all-NAs is deprecated. In a future version this will raise ValueError.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "68_NearestNeighbors f1 0.620647 trained in 12.24 seconds\n",
      "24_LightGBM f1 0.836431 trained in 30.66 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\YanPC\\Downloads\\automl_hw2-master\\.venv\\lib\\site-packages\\xgboost\\training.py:38: UserWarning: `feval` is deprecated, use `custom_metric` instead.  They have different behavior when custom objective is also used.See https://xgboost.readthedocs.io/en/latest/tutorials/custom_metric_obj.html for details on the `custom_metric`.\n",
      "c:\\Users\\YanPC\\Downloads\\automl_hw2-master\\.venv\\lib\\site-packages\\xgboost\\training.py:38: UserWarning: `feval` is deprecated, use `custom_metric` instead.  They have different behavior when custom objective is also used.See https://xgboost.readthedocs.io/en/latest/tutorials/custom_metric_obj.html for details on the `custom_metric`.\n",
      "c:\\Users\\YanPC\\Downloads\\automl_hw2-master\\.venv\\lib\\site-packages\\xgboost\\training.py:38: UserWarning: `feval` is deprecated, use `custom_metric` instead.  They have different behavior when custom objective is also used.See https://xgboost.readthedocs.io/en/latest/tutorials/custom_metric_obj.html for details on the `custom_metric`.\n",
      "c:\\Users\\YanPC\\Downloads\\automl_hw2-master\\.venv\\lib\\site-packages\\xgboost\\training.py:38: UserWarning: `feval` is deprecated, use `custom_metric` instead.  They have different behavior when custom objective is also used.See https://xgboost.readthedocs.io/en/latest/tutorials/custom_metric_obj.html for details on the `custom_metric`.\n",
      "c:\\Users\\YanPC\\Downloads\\automl_hw2-master\\.venv\\lib\\site-packages\\xgboost\\training.py:38: UserWarning: `feval` is deprecated, use `custom_metric` instead.  They have different behavior when custom objective is also used.See https://xgboost.readthedocs.io/en/latest/tutorials/custom_metric_obj.html for details on the `custom_metric`.\n",
      "c:\\Users\\YanPC\\Downloads\\automl_hw2-master\\.venv\\lib\\site-packages\\xgboost\\training.py:38: UserWarning: `feval` is deprecated, use `custom_metric` instead.  They have different behavior when custom objective is also used.See https://xgboost.readthedocs.io/en/latest/tutorials/custom_metric_obj.html for details on the `custom_metric`.\n",
      "c:\\Users\\YanPC\\Downloads\\automl_hw2-master\\.venv\\lib\\site-packages\\xgboost\\training.py:38: UserWarning: `feval` is deprecated, use `custom_metric` instead.  They have different behavior when custom objective is also used.See https://xgboost.readthedocs.io/en/latest/tutorials/custom_metric_obj.html for details on the `custom_metric`.\n",
      "c:\\Users\\YanPC\\Downloads\\automl_hw2-master\\.venv\\lib\\site-packages\\xgboost\\training.py:38: UserWarning: `feval` is deprecated, use `custom_metric` instead.  They have different behavior when custom objective is also used.See https://xgboost.readthedocs.io/en/latest/tutorials/custom_metric_obj.html for details on the `custom_metric`.\n",
      "c:\\Users\\YanPC\\Downloads\\automl_hw2-master\\.venv\\lib\\site-packages\\xgboost\\training.py:38: UserWarning: `feval` is deprecated, use `custom_metric` instead.  They have different behavior when custom objective is also used.See https://xgboost.readthedocs.io/en/latest/tutorials/custom_metric_obj.html for details on the `custom_metric`.\n",
      "c:\\Users\\YanPC\\Downloads\\automl_hw2-master\\.venv\\lib\\site-packages\\xgboost\\training.py:38: UserWarning: `feval` is deprecated, use `custom_metric` instead.  They have different behavior when custom objective is also used.See https://xgboost.readthedocs.io/en/latest/tutorials/custom_metric_obj.html for details on the `custom_metric`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15_Xgboost f1 0.757196 trained in 32.89 seconds\n",
      "33_CatBoost f1 0.814452 trained in 16.11 seconds\n",
      "42_RandomForest f1 0.774436 trained in 26.26 seconds\n",
      "51_ExtraTrees f1 0.721559 trained in 24.57 seconds\n",
      "60_NeuralNetwork f1 0.678727 trained in 16.72 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\YanPC\\Downloads\\automl_hw2-master\\.venv\\lib\\site-packages\\supervised\\utils\\learning_curves.py:113: FutureWarning: The behavior of Series.argmax/argmin with skipna=False and NAs, or with all-NAs is deprecated. In a future version this will raise ValueError.\n",
      "c:\\Users\\YanPC\\Downloads\\automl_hw2-master\\.venv\\lib\\site-packages\\supervised\\utils\\learning_curves.py:113: FutureWarning: The behavior of Series.argmax/argmin with skipna=False and NAs, or with all-NAs is deprecated. In a future version this will raise ValueError.\n",
      "c:\\Users\\YanPC\\Downloads\\automl_hw2-master\\.venv\\lib\\site-packages\\supervised\\utils\\learning_curves.py:113: FutureWarning: The behavior of Series.argmax/argmin with skipna=False and NAs, or with all-NAs is deprecated. In a future version this will raise ValueError.\n",
      "c:\\Users\\YanPC\\Downloads\\automl_hw2-master\\.venv\\lib\\site-packages\\supervised\\utils\\learning_curves.py:113: FutureWarning: The behavior of Series.argmax/argmin with skipna=False and NAs, or with all-NAs is deprecated. In a future version this will raise ValueError.\n",
      "c:\\Users\\YanPC\\Downloads\\automl_hw2-master\\.venv\\lib\\site-packages\\supervised\\utils\\learning_curves.py:113: FutureWarning: The behavior of Series.argmax/argmin with skipna=False and NAs, or with all-NAs is deprecated. In a future version this will raise ValueError.\n",
      "c:\\Users\\YanPC\\Downloads\\automl_hw2-master\\.venv\\lib\\site-packages\\supervised\\utils\\learning_curves.py:113: FutureWarning: The behavior of Series.argmax/argmin with skipna=False and NAs, or with all-NAs is deprecated. In a future version this will raise ValueError.\n",
      "c:\\Users\\YanPC\\Downloads\\automl_hw2-master\\.venv\\lib\\site-packages\\supervised\\utils\\learning_curves.py:113: FutureWarning: The behavior of Series.argmax/argmin with skipna=False and NAs, or with all-NAs is deprecated. In a future version this will raise ValueError.\n",
      "c:\\Users\\YanPC\\Downloads\\automl_hw2-master\\.venv\\lib\\site-packages\\supervised\\utils\\learning_curves.py:113: FutureWarning: The behavior of Series.argmax/argmin with skipna=False and NAs, or with all-NAs is deprecated. In a future version this will raise ValueError.\n",
      "c:\\Users\\YanPC\\Downloads\\automl_hw2-master\\.venv\\lib\\site-packages\\supervised\\utils\\learning_curves.py:113: FutureWarning: The behavior of Series.argmax/argmin with skipna=False and NAs, or with all-NAs is deprecated. In a future version this will raise ValueError.\n",
      "c:\\Users\\YanPC\\Downloads\\automl_hw2-master\\.venv\\lib\\site-packages\\supervised\\utils\\learning_curves.py:113: FutureWarning: The behavior of Series.argmax/argmin with skipna=False and NAs, or with all-NAs is deprecated. In a future version this will raise ValueError.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "69_NearestNeighbors f1 0.620647 trained in 13.17 seconds\n",
      "25_LightGBM f1 0.827329 trained in 24.17 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\YanPC\\Downloads\\automl_hw2-master\\.venv\\lib\\site-packages\\xgboost\\training.py:38: UserWarning: `feval` is deprecated, use `custom_metric` instead.  They have different behavior when custom objective is also used.See https://xgboost.readthedocs.io/en/latest/tutorials/custom_metric_obj.html for details on the `custom_metric`.\n",
      "c:\\Users\\YanPC\\Downloads\\automl_hw2-master\\.venv\\lib\\site-packages\\xgboost\\training.py:38: UserWarning: `feval` is deprecated, use `custom_metric` instead.  They have different behavior when custom objective is also used.See https://xgboost.readthedocs.io/en/latest/tutorials/custom_metric_obj.html for details on the `custom_metric`.\n",
      "c:\\Users\\YanPC\\Downloads\\automl_hw2-master\\.venv\\lib\\site-packages\\xgboost\\training.py:38: UserWarning: `feval` is deprecated, use `custom_metric` instead.  They have different behavior when custom objective is also used.See https://xgboost.readthedocs.io/en/latest/tutorials/custom_metric_obj.html for details on the `custom_metric`.\n",
      "c:\\Users\\YanPC\\Downloads\\automl_hw2-master\\.venv\\lib\\site-packages\\xgboost\\training.py:38: UserWarning: `feval` is deprecated, use `custom_metric` instead.  They have different behavior when custom objective is also used.See https://xgboost.readthedocs.io/en/latest/tutorials/custom_metric_obj.html for details on the `custom_metric`.\n",
      "c:\\Users\\YanPC\\Downloads\\automl_hw2-master\\.venv\\lib\\site-packages\\xgboost\\training.py:38: UserWarning: `feval` is deprecated, use `custom_metric` instead.  They have different behavior when custom objective is also used.See https://xgboost.readthedocs.io/en/latest/tutorials/custom_metric_obj.html for details on the `custom_metric`.\n",
      "c:\\Users\\YanPC\\Downloads\\automl_hw2-master\\.venv\\lib\\site-packages\\xgboost\\training.py:38: UserWarning: `feval` is deprecated, use `custom_metric` instead.  They have different behavior when custom objective is also used.See https://xgboost.readthedocs.io/en/latest/tutorials/custom_metric_obj.html for details on the `custom_metric`.\n",
      "c:\\Users\\YanPC\\Downloads\\automl_hw2-master\\.venv\\lib\\site-packages\\xgboost\\training.py:38: UserWarning: `feval` is deprecated, use `custom_metric` instead.  They have different behavior when custom objective is also used.See https://xgboost.readthedocs.io/en/latest/tutorials/custom_metric_obj.html for details on the `custom_metric`.\n",
      "c:\\Users\\YanPC\\Downloads\\automl_hw2-master\\.venv\\lib\\site-packages\\xgboost\\training.py:38: UserWarning: `feval` is deprecated, use `custom_metric` instead.  They have different behavior when custom objective is also used.See https://xgboost.readthedocs.io/en/latest/tutorials/custom_metric_obj.html for details on the `custom_metric`.\n",
      "c:\\Users\\YanPC\\Downloads\\automl_hw2-master\\.venv\\lib\\site-packages\\xgboost\\training.py:38: UserWarning: `feval` is deprecated, use `custom_metric` instead.  They have different behavior when custom objective is also used.See https://xgboost.readthedocs.io/en/latest/tutorials/custom_metric_obj.html for details on the `custom_metric`.\n",
      "c:\\Users\\YanPC\\Downloads\\automl_hw2-master\\.venv\\lib\\site-packages\\xgboost\\training.py:38: UserWarning: `feval` is deprecated, use `custom_metric` instead.  They have different behavior when custom objective is also used.See https://xgboost.readthedocs.io/en/latest/tutorials/custom_metric_obj.html for details on the `custom_metric`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16_Xgboost f1 0.655194 trained in 33.88 seconds\n",
      "34_CatBoost f1 0.828589 trained in 19.94 seconds\n",
      "43_RandomForest f1 0.741347 trained in 25.22 seconds\n",
      "52_ExtraTrees f1 0.684178 trained in 28.14 seconds\n",
      "61_NeuralNetwork f1 0.705308 trained in 19.52 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\YanPC\\Downloads\\automl_hw2-master\\.venv\\lib\\site-packages\\supervised\\utils\\learning_curves.py:113: FutureWarning: The behavior of Series.argmax/argmin with skipna=False and NAs, or with all-NAs is deprecated. In a future version this will raise ValueError.\n",
      "c:\\Users\\YanPC\\Downloads\\automl_hw2-master\\.venv\\lib\\site-packages\\supervised\\utils\\learning_curves.py:113: FutureWarning: The behavior of Series.argmax/argmin with skipna=False and NAs, or with all-NAs is deprecated. In a future version this will raise ValueError.\n",
      "c:\\Users\\YanPC\\Downloads\\automl_hw2-master\\.venv\\lib\\site-packages\\supervised\\utils\\learning_curves.py:113: FutureWarning: The behavior of Series.argmax/argmin with skipna=False and NAs, or with all-NAs is deprecated. In a future version this will raise ValueError.\n",
      "c:\\Users\\YanPC\\Downloads\\automl_hw2-master\\.venv\\lib\\site-packages\\supervised\\utils\\learning_curves.py:113: FutureWarning: The behavior of Series.argmax/argmin with skipna=False and NAs, or with all-NAs is deprecated. In a future version this will raise ValueError.\n",
      "c:\\Users\\YanPC\\Downloads\\automl_hw2-master\\.venv\\lib\\site-packages\\supervised\\utils\\learning_curves.py:113: FutureWarning: The behavior of Series.argmax/argmin with skipna=False and NAs, or with all-NAs is deprecated. In a future version this will raise ValueError.\n",
      "c:\\Users\\YanPC\\Downloads\\automl_hw2-master\\.venv\\lib\\site-packages\\supervised\\utils\\learning_curves.py:113: FutureWarning: The behavior of Series.argmax/argmin with skipna=False and NAs, or with all-NAs is deprecated. In a future version this will raise ValueError.\n",
      "c:\\Users\\YanPC\\Downloads\\automl_hw2-master\\.venv\\lib\\site-packages\\supervised\\utils\\learning_curves.py:113: FutureWarning: The behavior of Series.argmax/argmin with skipna=False and NAs, or with all-NAs is deprecated. In a future version this will raise ValueError.\n",
      "c:\\Users\\YanPC\\Downloads\\automl_hw2-master\\.venv\\lib\\site-packages\\supervised\\utils\\learning_curves.py:113: FutureWarning: The behavior of Series.argmax/argmin with skipna=False and NAs, or with all-NAs is deprecated. In a future version this will raise ValueError.\n",
      "c:\\Users\\YanPC\\Downloads\\automl_hw2-master\\.venv\\lib\\site-packages\\supervised\\utils\\learning_curves.py:113: FutureWarning: The behavior of Series.argmax/argmin with skipna=False and NAs, or with all-NAs is deprecated. In a future version this will raise ValueError.\n",
      "c:\\Users\\YanPC\\Downloads\\automl_hw2-master\\.venv\\lib\\site-packages\\supervised\\utils\\learning_curves.py:113: FutureWarning: The behavior of Series.argmax/argmin with skipna=False and NAs, or with all-NAs is deprecated. In a future version this will raise ValueError.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "70_NearestNeighbors f1 0.652527 trained in 15.77 seconds\n",
      "26_LightGBM f1 0.836341 trained in 26.44 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\YanPC\\Downloads\\automl_hw2-master\\.venv\\lib\\site-packages\\xgboost\\training.py:38: UserWarning: `feval` is deprecated, use `custom_metric` instead.  They have different behavior when custom objective is also used.See https://xgboost.readthedocs.io/en/latest/tutorials/custom_metric_obj.html for details on the `custom_metric`.\n",
      "c:\\Users\\YanPC\\Downloads\\automl_hw2-master\\.venv\\lib\\site-packages\\xgboost\\training.py:38: UserWarning: `feval` is deprecated, use `custom_metric` instead.  They have different behavior when custom objective is also used.See https://xgboost.readthedocs.io/en/latest/tutorials/custom_metric_obj.html for details on the `custom_metric`.\n",
      "c:\\Users\\YanPC\\Downloads\\automl_hw2-master\\.venv\\lib\\site-packages\\xgboost\\training.py:38: UserWarning: `feval` is deprecated, use `custom_metric` instead.  They have different behavior when custom objective is also used.See https://xgboost.readthedocs.io/en/latest/tutorials/custom_metric_obj.html for details on the `custom_metric`.\n",
      "c:\\Users\\YanPC\\Downloads\\automl_hw2-master\\.venv\\lib\\site-packages\\xgboost\\training.py:38: UserWarning: `feval` is deprecated, use `custom_metric` instead.  They have different behavior when custom objective is also used.See https://xgboost.readthedocs.io/en/latest/tutorials/custom_metric_obj.html for details on the `custom_metric`.\n",
      "c:\\Users\\YanPC\\Downloads\\automl_hw2-master\\.venv\\lib\\site-packages\\xgboost\\training.py:38: UserWarning: `feval` is deprecated, use `custom_metric` instead.  They have different behavior when custom objective is also used.See https://xgboost.readthedocs.io/en/latest/tutorials/custom_metric_obj.html for details on the `custom_metric`.\n",
      "c:\\Users\\YanPC\\Downloads\\automl_hw2-master\\.venv\\lib\\site-packages\\xgboost\\training.py:38: UserWarning: `feval` is deprecated, use `custom_metric` instead.  They have different behavior when custom objective is also used.See https://xgboost.readthedocs.io/en/latest/tutorials/custom_metric_obj.html for details on the `custom_metric`.\n",
      "c:\\Users\\YanPC\\Downloads\\automl_hw2-master\\.venv\\lib\\site-packages\\xgboost\\training.py:38: UserWarning: `feval` is deprecated, use `custom_metric` instead.  They have different behavior when custom objective is also used.See https://xgboost.readthedocs.io/en/latest/tutorials/custom_metric_obj.html for details on the `custom_metric`.\n",
      "c:\\Users\\YanPC\\Downloads\\automl_hw2-master\\.venv\\lib\\site-packages\\xgboost\\training.py:38: UserWarning: `feval` is deprecated, use `custom_metric` instead.  They have different behavior when custom objective is also used.See https://xgboost.readthedocs.io/en/latest/tutorials/custom_metric_obj.html for details on the `custom_metric`.\n",
      "c:\\Users\\YanPC\\Downloads\\automl_hw2-master\\.venv\\lib\\site-packages\\xgboost\\training.py:38: UserWarning: `feval` is deprecated, use `custom_metric` instead.  They have different behavior when custom objective is also used.See https://xgboost.readthedocs.io/en/latest/tutorials/custom_metric_obj.html for details on the `custom_metric`.\n",
      "c:\\Users\\YanPC\\Downloads\\automl_hw2-master\\.venv\\lib\\site-packages\\xgboost\\training.py:38: UserWarning: `feval` is deprecated, use `custom_metric` instead.  They have different behavior when custom objective is also used.See https://xgboost.readthedocs.io/en/latest/tutorials/custom_metric_obj.html for details on the `custom_metric`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "17_Xgboost f1 0.783375 trained in 37.11 seconds\n",
      "35_CatBoost f1 0.819247 trained in 20.33 seconds\n",
      "44_RandomForest f1 0.802218 trained in 30.31 seconds\n",
      "53_ExtraTrees f1 0.762572 trained in 28.6 seconds\n",
      "62_NeuralNetwork f1 0.658278 trained in 25.96 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\YanPC\\Downloads\\automl_hw2-master\\.venv\\lib\\site-packages\\supervised\\utils\\learning_curves.py:113: FutureWarning: The behavior of Series.argmax/argmin with skipna=False and NAs, or with all-NAs is deprecated. In a future version this will raise ValueError.\n",
      "c:\\Users\\YanPC\\Downloads\\automl_hw2-master\\.venv\\lib\\site-packages\\supervised\\utils\\learning_curves.py:113: FutureWarning: The behavior of Series.argmax/argmin with skipna=False and NAs, or with all-NAs is deprecated. In a future version this will raise ValueError.\n",
      "c:\\Users\\YanPC\\Downloads\\automl_hw2-master\\.venv\\lib\\site-packages\\supervised\\utils\\learning_curves.py:113: FutureWarning: The behavior of Series.argmax/argmin with skipna=False and NAs, or with all-NAs is deprecated. In a future version this will raise ValueError.\n",
      "c:\\Users\\YanPC\\Downloads\\automl_hw2-master\\.venv\\lib\\site-packages\\supervised\\utils\\learning_curves.py:113: FutureWarning: The behavior of Series.argmax/argmin with skipna=False and NAs, or with all-NAs is deprecated. In a future version this will raise ValueError.\n",
      "c:\\Users\\YanPC\\Downloads\\automl_hw2-master\\.venv\\lib\\site-packages\\supervised\\utils\\learning_curves.py:113: FutureWarning: The behavior of Series.argmax/argmin with skipna=False and NAs, or with all-NAs is deprecated. In a future version this will raise ValueError.\n",
      "c:\\Users\\YanPC\\Downloads\\automl_hw2-master\\.venv\\lib\\site-packages\\supervised\\utils\\learning_curves.py:113: FutureWarning: The behavior of Series.argmax/argmin with skipna=False and NAs, or with all-NAs is deprecated. In a future version this will raise ValueError.\n",
      "c:\\Users\\YanPC\\Downloads\\automl_hw2-master\\.venv\\lib\\site-packages\\supervised\\utils\\learning_curves.py:113: FutureWarning: The behavior of Series.argmax/argmin with skipna=False and NAs, or with all-NAs is deprecated. In a future version this will raise ValueError.\n",
      "c:\\Users\\YanPC\\Downloads\\automl_hw2-master\\.venv\\lib\\site-packages\\supervised\\utils\\learning_curves.py:113: FutureWarning: The behavior of Series.argmax/argmin with skipna=False and NAs, or with all-NAs is deprecated. In a future version this will raise ValueError.\n",
      "c:\\Users\\YanPC\\Downloads\\automl_hw2-master\\.venv\\lib\\site-packages\\supervised\\utils\\learning_curves.py:113: FutureWarning: The behavior of Series.argmax/argmin with skipna=False and NAs, or with all-NAs is deprecated. In a future version this will raise ValueError.\n",
      "c:\\Users\\YanPC\\Downloads\\automl_hw2-master\\.venv\\lib\\site-packages\\supervised\\utils\\learning_curves.py:113: FutureWarning: The behavior of Series.argmax/argmin with skipna=False and NAs, or with all-NAs is deprecated. In a future version this will raise ValueError.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "71_NearestNeighbors f1 0.620647 trained in 16.67 seconds\n",
      "27_LightGBM f1 0.832718 trained in 28.83 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\YanPC\\Downloads\\automl_hw2-master\\.venv\\lib\\site-packages\\xgboost\\training.py:38: UserWarning: `feval` is deprecated, use `custom_metric` instead.  They have different behavior when custom objective is also used.See https://xgboost.readthedocs.io/en/latest/tutorials/custom_metric_obj.html for details on the `custom_metric`.\n",
      "c:\\Users\\YanPC\\Downloads\\automl_hw2-master\\.venv\\lib\\site-packages\\xgboost\\training.py:38: UserWarning: `feval` is deprecated, use `custom_metric` instead.  They have different behavior when custom objective is also used.See https://xgboost.readthedocs.io/en/latest/tutorials/custom_metric_obj.html for details on the `custom_metric`.\n",
      "c:\\Users\\YanPC\\Downloads\\automl_hw2-master\\.venv\\lib\\site-packages\\xgboost\\training.py:38: UserWarning: `feval` is deprecated, use `custom_metric` instead.  They have different behavior when custom objective is also used.See https://xgboost.readthedocs.io/en/latest/tutorials/custom_metric_obj.html for details on the `custom_metric`.\n",
      "c:\\Users\\YanPC\\Downloads\\automl_hw2-master\\.venv\\lib\\site-packages\\xgboost\\training.py:38: UserWarning: `feval` is deprecated, use `custom_metric` instead.  They have different behavior when custom objective is also used.See https://xgboost.readthedocs.io/en/latest/tutorials/custom_metric_obj.html for details on the `custom_metric`.\n",
      "c:\\Users\\YanPC\\Downloads\\automl_hw2-master\\.venv\\lib\\site-packages\\xgboost\\training.py:38: UserWarning: `feval` is deprecated, use `custom_metric` instead.  They have different behavior when custom objective is also used.See https://xgboost.readthedocs.io/en/latest/tutorials/custom_metric_obj.html for details on the `custom_metric`.\n",
      "c:\\Users\\YanPC\\Downloads\\automl_hw2-master\\.venv\\lib\\site-packages\\xgboost\\training.py:38: UserWarning: `feval` is deprecated, use `custom_metric` instead.  They have different behavior when custom objective is also used.See https://xgboost.readthedocs.io/en/latest/tutorials/custom_metric_obj.html for details on the `custom_metric`.\n",
      "c:\\Users\\YanPC\\Downloads\\automl_hw2-master\\.venv\\lib\\site-packages\\xgboost\\training.py:38: UserWarning: `feval` is deprecated, use `custom_metric` instead.  They have different behavior when custom objective is also used.See https://xgboost.readthedocs.io/en/latest/tutorials/custom_metric_obj.html for details on the `custom_metric`.\n",
      "c:\\Users\\YanPC\\Downloads\\automl_hw2-master\\.venv\\lib\\site-packages\\xgboost\\training.py:38: UserWarning: `feval` is deprecated, use `custom_metric` instead.  They have different behavior when custom objective is also used.See https://xgboost.readthedocs.io/en/latest/tutorials/custom_metric_obj.html for details on the `custom_metric`.\n",
      "c:\\Users\\YanPC\\Downloads\\automl_hw2-master\\.venv\\lib\\site-packages\\xgboost\\training.py:38: UserWarning: `feval` is deprecated, use `custom_metric` instead.  They have different behavior when custom objective is also used.See https://xgboost.readthedocs.io/en/latest/tutorials/custom_metric_obj.html for details on the `custom_metric`.\n",
      "c:\\Users\\YanPC\\Downloads\\automl_hw2-master\\.venv\\lib\\site-packages\\xgboost\\training.py:38: UserWarning: `feval` is deprecated, use `custom_metric` instead.  They have different behavior when custom objective is also used.See https://xgboost.readthedocs.io/en/latest/tutorials/custom_metric_obj.html for details on the `custom_metric`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "18_Xgboost f1 0.734036 trained in 35.27 seconds\n",
      "36_CatBoost f1 0.850277 trained in 27.29 seconds\n",
      "45_RandomForest f1 0.815366 trained in 36.58 seconds\n",
      "54_ExtraTrees f1 0.764059 trained in 29.04 seconds\n",
      "63_NeuralNetwork f1 0.661999 trained in 21.25 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\YanPC\\Downloads\\automl_hw2-master\\.venv\\lib\\site-packages\\supervised\\utils\\learning_curves.py:113: FutureWarning: The behavior of Series.argmax/argmin with skipna=False and NAs, or with all-NAs is deprecated. In a future version this will raise ValueError.\n",
      "c:\\Users\\YanPC\\Downloads\\automl_hw2-master\\.venv\\lib\\site-packages\\supervised\\utils\\learning_curves.py:113: FutureWarning: The behavior of Series.argmax/argmin with skipna=False and NAs, or with all-NAs is deprecated. In a future version this will raise ValueError.\n",
      "c:\\Users\\YanPC\\Downloads\\automl_hw2-master\\.venv\\lib\\site-packages\\supervised\\utils\\learning_curves.py:113: FutureWarning: The behavior of Series.argmax/argmin with skipna=False and NAs, or with all-NAs is deprecated. In a future version this will raise ValueError.\n",
      "c:\\Users\\YanPC\\Downloads\\automl_hw2-master\\.venv\\lib\\site-packages\\supervised\\utils\\learning_curves.py:113: FutureWarning: The behavior of Series.argmax/argmin with skipna=False and NAs, or with all-NAs is deprecated. In a future version this will raise ValueError.\n",
      "c:\\Users\\YanPC\\Downloads\\automl_hw2-master\\.venv\\lib\\site-packages\\supervised\\utils\\learning_curves.py:113: FutureWarning: The behavior of Series.argmax/argmin with skipna=False and NAs, or with all-NAs is deprecated. In a future version this will raise ValueError.\n",
      "c:\\Users\\YanPC\\Downloads\\automl_hw2-master\\.venv\\lib\\site-packages\\supervised\\utils\\learning_curves.py:113: FutureWarning: The behavior of Series.argmax/argmin with skipna=False and NAs, or with all-NAs is deprecated. In a future version this will raise ValueError.\n",
      "c:\\Users\\YanPC\\Downloads\\automl_hw2-master\\.venv\\lib\\site-packages\\supervised\\utils\\learning_curves.py:113: FutureWarning: The behavior of Series.argmax/argmin with skipna=False and NAs, or with all-NAs is deprecated. In a future version this will raise ValueError.\n",
      "c:\\Users\\YanPC\\Downloads\\automl_hw2-master\\.venv\\lib\\site-packages\\supervised\\utils\\learning_curves.py:113: FutureWarning: The behavior of Series.argmax/argmin with skipna=False and NAs, or with all-NAs is deprecated. In a future version this will raise ValueError.\n",
      "c:\\Users\\YanPC\\Downloads\\automl_hw2-master\\.venv\\lib\\site-packages\\supervised\\utils\\learning_curves.py:113: FutureWarning: The behavior of Series.argmax/argmin with skipna=False and NAs, or with all-NAs is deprecated. In a future version this will raise ValueError.\n",
      "c:\\Users\\YanPC\\Downloads\\automl_hw2-master\\.venv\\lib\\site-packages\\supervised\\utils\\learning_curves.py:113: FutureWarning: The behavior of Series.argmax/argmin with skipna=False and NAs, or with all-NAs is deprecated. In a future version this will raise ValueError.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "72_NearestNeighbors f1 0.620647 trained in 16.67 seconds\n",
      "28_LightGBM f1 0.834988 trained in 32.0 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\YanPC\\Downloads\\automl_hw2-master\\.venv\\lib\\site-packages\\xgboost\\training.py:38: UserWarning: `feval` is deprecated, use `custom_metric` instead.  They have different behavior when custom objective is also used.See https://xgboost.readthedocs.io/en/latest/tutorials/custom_metric_obj.html for details on the `custom_metric`.\n",
      "c:\\Users\\YanPC\\Downloads\\automl_hw2-master\\.venv\\lib\\site-packages\\xgboost\\training.py:38: UserWarning: `feval` is deprecated, use `custom_metric` instead.  They have different behavior when custom objective is also used.See https://xgboost.readthedocs.io/en/latest/tutorials/custom_metric_obj.html for details on the `custom_metric`.\n",
      "c:\\Users\\YanPC\\Downloads\\automl_hw2-master\\.venv\\lib\\site-packages\\xgboost\\training.py:38: UserWarning: `feval` is deprecated, use `custom_metric` instead.  They have different behavior when custom objective is also used.See https://xgboost.readthedocs.io/en/latest/tutorials/custom_metric_obj.html for details on the `custom_metric`.\n",
      "c:\\Users\\YanPC\\Downloads\\automl_hw2-master\\.venv\\lib\\site-packages\\xgboost\\training.py:38: UserWarning: `feval` is deprecated, use `custom_metric` instead.  They have different behavior when custom objective is also used.See https://xgboost.readthedocs.io/en/latest/tutorials/custom_metric_obj.html for details on the `custom_metric`.\n",
      "c:\\Users\\YanPC\\Downloads\\automl_hw2-master\\.venv\\lib\\site-packages\\xgboost\\training.py:38: UserWarning: `feval` is deprecated, use `custom_metric` instead.  They have different behavior when custom objective is also used.See https://xgboost.readthedocs.io/en/latest/tutorials/custom_metric_obj.html for details on the `custom_metric`.\n",
      "c:\\Users\\YanPC\\Downloads\\automl_hw2-master\\.venv\\lib\\site-packages\\xgboost\\training.py:38: UserWarning: `feval` is deprecated, use `custom_metric` instead.  They have different behavior when custom objective is also used.See https://xgboost.readthedocs.io/en/latest/tutorials/custom_metric_obj.html for details on the `custom_metric`.\n",
      "c:\\Users\\YanPC\\Downloads\\automl_hw2-master\\.venv\\lib\\site-packages\\xgboost\\training.py:38: UserWarning: `feval` is deprecated, use `custom_metric` instead.  They have different behavior when custom objective is also used.See https://xgboost.readthedocs.io/en/latest/tutorials/custom_metric_obj.html for details on the `custom_metric`.\n",
      "c:\\Users\\YanPC\\Downloads\\automl_hw2-master\\.venv\\lib\\site-packages\\xgboost\\training.py:38: UserWarning: `feval` is deprecated, use `custom_metric` instead.  They have different behavior when custom objective is also used.See https://xgboost.readthedocs.io/en/latest/tutorials/custom_metric_obj.html for details on the `custom_metric`.\n",
      "c:\\Users\\YanPC\\Downloads\\automl_hw2-master\\.venv\\lib\\site-packages\\xgboost\\training.py:38: UserWarning: `feval` is deprecated, use `custom_metric` instead.  They have different behavior when custom objective is also used.See https://xgboost.readthedocs.io/en/latest/tutorials/custom_metric_obj.html for details on the `custom_metric`.\n",
      "c:\\Users\\YanPC\\Downloads\\automl_hw2-master\\.venv\\lib\\site-packages\\xgboost\\training.py:38: UserWarning: `feval` is deprecated, use `custom_metric` instead.  They have different behavior when custom objective is also used.See https://xgboost.readthedocs.io/en/latest/tutorials/custom_metric_obj.html for details on the `custom_metric`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "19_Xgboost f1 0.775126 trained in 33.23 seconds\n",
      "37_CatBoost f1 0.858021 trained in 34.05 seconds\n",
      "46_RandomForest f1 0.745887 trained in 30.38 seconds\n",
      "55_ExtraTrees f1 0.678313 trained in 26.68 seconds\n",
      "64_NeuralNetwork f1 0.658551 trained in 20.94 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\YanPC\\Downloads\\automl_hw2-master\\.venv\\lib\\site-packages\\supervised\\utils\\learning_curves.py:113: FutureWarning: The behavior of Series.argmax/argmin with skipna=False and NAs, or with all-NAs is deprecated. In a future version this will raise ValueError.\n",
      "c:\\Users\\YanPC\\Downloads\\automl_hw2-master\\.venv\\lib\\site-packages\\supervised\\utils\\learning_curves.py:113: FutureWarning: The behavior of Series.argmax/argmin with skipna=False and NAs, or with all-NAs is deprecated. In a future version this will raise ValueError.\n",
      "c:\\Users\\YanPC\\Downloads\\automl_hw2-master\\.venv\\lib\\site-packages\\supervised\\utils\\learning_curves.py:113: FutureWarning: The behavior of Series.argmax/argmin with skipna=False and NAs, or with all-NAs is deprecated. In a future version this will raise ValueError.\n",
      "c:\\Users\\YanPC\\Downloads\\automl_hw2-master\\.venv\\lib\\site-packages\\supervised\\utils\\learning_curves.py:113: FutureWarning: The behavior of Series.argmax/argmin with skipna=False and NAs, or with all-NAs is deprecated. In a future version this will raise ValueError.\n",
      "c:\\Users\\YanPC\\Downloads\\automl_hw2-master\\.venv\\lib\\site-packages\\supervised\\utils\\learning_curves.py:113: FutureWarning: The behavior of Series.argmax/argmin with skipna=False and NAs, or with all-NAs is deprecated. In a future version this will raise ValueError.\n",
      "c:\\Users\\YanPC\\Downloads\\automl_hw2-master\\.venv\\lib\\site-packages\\supervised\\utils\\learning_curves.py:113: FutureWarning: The behavior of Series.argmax/argmin with skipna=False and NAs, or with all-NAs is deprecated. In a future version this will raise ValueError.\n",
      "c:\\Users\\YanPC\\Downloads\\automl_hw2-master\\.venv\\lib\\site-packages\\supervised\\utils\\learning_curves.py:113: FutureWarning: The behavior of Series.argmax/argmin with skipna=False and NAs, or with all-NAs is deprecated. In a future version this will raise ValueError.\n",
      "c:\\Users\\YanPC\\Downloads\\automl_hw2-master\\.venv\\lib\\site-packages\\supervised\\utils\\learning_curves.py:113: FutureWarning: The behavior of Series.argmax/argmin with skipna=False and NAs, or with all-NAs is deprecated. In a future version this will raise ValueError.\n",
      "c:\\Users\\YanPC\\Downloads\\automl_hw2-master\\.venv\\lib\\site-packages\\supervised\\utils\\learning_curves.py:113: FutureWarning: The behavior of Series.argmax/argmin with skipna=False and NAs, or with all-NAs is deprecated. In a future version this will raise ValueError.\n",
      "c:\\Users\\YanPC\\Downloads\\automl_hw2-master\\.venv\\lib\\site-packages\\supervised\\utils\\learning_curves.py:113: FutureWarning: The behavior of Series.argmax/argmin with skipna=False and NAs, or with all-NAs is deprecated. In a future version this will raise ValueError.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "29_LightGBM f1 0.838789 trained in 26.2 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\YanPC\\Downloads\\automl_hw2-master\\.venv\\lib\\site-packages\\xgboost\\training.py:38: UserWarning: `feval` is deprecated, use `custom_metric` instead.  They have different behavior when custom objective is also used.See https://xgboost.readthedocs.io/en/latest/tutorials/custom_metric_obj.html for details on the `custom_metric`.\n",
      "c:\\Users\\YanPC\\Downloads\\automl_hw2-master\\.venv\\lib\\site-packages\\xgboost\\training.py:38: UserWarning: `feval` is deprecated, use `custom_metric` instead.  They have different behavior when custom objective is also used.See https://xgboost.readthedocs.io/en/latest/tutorials/custom_metric_obj.html for details on the `custom_metric`.\n",
      "c:\\Users\\YanPC\\Downloads\\automl_hw2-master\\.venv\\lib\\site-packages\\xgboost\\training.py:38: UserWarning: `feval` is deprecated, use `custom_metric` instead.  They have different behavior when custom objective is also used.See https://xgboost.readthedocs.io/en/latest/tutorials/custom_metric_obj.html for details on the `custom_metric`.\n",
      "c:\\Users\\YanPC\\Downloads\\automl_hw2-master\\.venv\\lib\\site-packages\\xgboost\\training.py:38: UserWarning: `feval` is deprecated, use `custom_metric` instead.  They have different behavior when custom objective is also used.See https://xgboost.readthedocs.io/en/latest/tutorials/custom_metric_obj.html for details on the `custom_metric`.\n",
      "c:\\Users\\YanPC\\Downloads\\automl_hw2-master\\.venv\\lib\\site-packages\\xgboost\\training.py:38: UserWarning: `feval` is deprecated, use `custom_metric` instead.  They have different behavior when custom objective is also used.See https://xgboost.readthedocs.io/en/latest/tutorials/custom_metric_obj.html for details on the `custom_metric`.\n",
      "c:\\Users\\YanPC\\Downloads\\automl_hw2-master\\.venv\\lib\\site-packages\\xgboost\\training.py:38: UserWarning: `feval` is deprecated, use `custom_metric` instead.  They have different behavior when custom objective is also used.See https://xgboost.readthedocs.io/en/latest/tutorials/custom_metric_obj.html for details on the `custom_metric`.\n",
      "c:\\Users\\YanPC\\Downloads\\automl_hw2-master\\.venv\\lib\\site-packages\\xgboost\\training.py:38: UserWarning: `feval` is deprecated, use `custom_metric` instead.  They have different behavior when custom objective is also used.See https://xgboost.readthedocs.io/en/latest/tutorials/custom_metric_obj.html for details on the `custom_metric`.\n",
      "c:\\Users\\YanPC\\Downloads\\automl_hw2-master\\.venv\\lib\\site-packages\\xgboost\\training.py:38: UserWarning: `feval` is deprecated, use `custom_metric` instead.  They have different behavior when custom objective is also used.See https://xgboost.readthedocs.io/en/latest/tutorials/custom_metric_obj.html for details on the `custom_metric`.\n",
      "c:\\Users\\YanPC\\Downloads\\automl_hw2-master\\.venv\\lib\\site-packages\\xgboost\\training.py:38: UserWarning: `feval` is deprecated, use `custom_metric` instead.  They have different behavior when custom objective is also used.See https://xgboost.readthedocs.io/en/latest/tutorials/custom_metric_obj.html for details on the `custom_metric`.\n",
      "c:\\Users\\YanPC\\Downloads\\automl_hw2-master\\.venv\\lib\\site-packages\\xgboost\\training.py:38: UserWarning: `feval` is deprecated, use `custom_metric` instead.  They have different behavior when custom objective is also used.See https://xgboost.readthedocs.io/en/latest/tutorials/custom_metric_obj.html for details on the `custom_metric`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20_Xgboost f1 0.750157 trained in 38.77 seconds\n",
      "38_CatBoost f1 0.824455 trained in 22.71 seconds\n",
      "47_RandomForest f1 0.774074 trained in 27.57 seconds\n",
      "56_ExtraTrees f1 0.734644 trained in 25.97 seconds\n",
      "65_NeuralNetwork f1 0.664935 trained in 21.36 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\YanPC\\Downloads\\automl_hw2-master\\.venv\\lib\\site-packages\\supervised\\utils\\learning_curves.py:113: FutureWarning: The behavior of Series.argmax/argmin with skipna=False and NAs, or with all-NAs is deprecated. In a future version this will raise ValueError.\n",
      "c:\\Users\\YanPC\\Downloads\\automl_hw2-master\\.venv\\lib\\site-packages\\supervised\\utils\\learning_curves.py:113: FutureWarning: The behavior of Series.argmax/argmin with skipna=False and NAs, or with all-NAs is deprecated. In a future version this will raise ValueError.\n",
      "c:\\Users\\YanPC\\Downloads\\automl_hw2-master\\.venv\\lib\\site-packages\\supervised\\utils\\learning_curves.py:113: FutureWarning: The behavior of Series.argmax/argmin with skipna=False and NAs, or with all-NAs is deprecated. In a future version this will raise ValueError.\n",
      "c:\\Users\\YanPC\\Downloads\\automl_hw2-master\\.venv\\lib\\site-packages\\supervised\\utils\\learning_curves.py:113: FutureWarning: The behavior of Series.argmax/argmin with skipna=False and NAs, or with all-NAs is deprecated. In a future version this will raise ValueError.\n",
      "c:\\Users\\YanPC\\Downloads\\automl_hw2-master\\.venv\\lib\\site-packages\\supervised\\utils\\learning_curves.py:113: FutureWarning: The behavior of Series.argmax/argmin with skipna=False and NAs, or with all-NAs is deprecated. In a future version this will raise ValueError.\n",
      "c:\\Users\\YanPC\\Downloads\\automl_hw2-master\\.venv\\lib\\site-packages\\supervised\\utils\\learning_curves.py:113: FutureWarning: The behavior of Series.argmax/argmin with skipna=False and NAs, or with all-NAs is deprecated. In a future version this will raise ValueError.\n",
      "c:\\Users\\YanPC\\Downloads\\automl_hw2-master\\.venv\\lib\\site-packages\\supervised\\utils\\learning_curves.py:113: FutureWarning: The behavior of Series.argmax/argmin with skipna=False and NAs, or with all-NAs is deprecated. In a future version this will raise ValueError.\n",
      "c:\\Users\\YanPC\\Downloads\\automl_hw2-master\\.venv\\lib\\site-packages\\supervised\\utils\\learning_curves.py:113: FutureWarning: The behavior of Series.argmax/argmin with skipna=False and NAs, or with all-NAs is deprecated. In a future version this will raise ValueError.\n",
      "c:\\Users\\YanPC\\Downloads\\automl_hw2-master\\.venv\\lib\\site-packages\\supervised\\utils\\learning_curves.py:113: FutureWarning: The behavior of Series.argmax/argmin with skipna=False and NAs, or with all-NAs is deprecated. In a future version this will raise ValueError.\n",
      "c:\\Users\\YanPC\\Downloads\\automl_hw2-master\\.venv\\lib\\site-packages\\supervised\\utils\\learning_curves.py:113: FutureWarning: The behavior of Series.argmax/argmin with skipna=False and NAs, or with all-NAs is deprecated. In a future version this will raise ValueError.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "* Step golden_features will try to check up to 3 models\n",
      "None 10\n",
      "Add Golden Feature: feature_22_sum_feature_2\n",
      "Add Golden Feature: feature_11_diff_feature_12\n",
      "Add Golden Feature: feature_8_multiply_feature_2\n",
      "Add Golden Feature: feature_21_multiply_feature_3\n",
      "Add Golden Feature: feature_5_diff_feature_21\n",
      "Add Golden Feature: feature_17_diff_feature_20\n",
      "Add Golden Feature: feature_15_ratio_feature_11\n",
      "Add Golden Feature: feature_11_ratio_feature_15\n",
      "Add Golden Feature: feature_4_diff_feature_17\n",
      "Add Golden Feature: feature_11_sum_feature_1\n",
      "Created 10 Golden Features in 15.2 seconds.\n",
      "37_CatBoost_GoldenFeatures f1 0.84469 trained in 51.54 seconds\n",
      "36_CatBoost_GoldenFeatures f1 0.837787 trained in 30.92 seconds\n",
      "32_CatBoost_GoldenFeatures f1 0.83839 trained in 61.59 seconds\n",
      "* Step kmeans_features will try to check up to 3 models\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\YanPC\\Downloads\\automl_hw2-master\\.venv\\lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1934: FutureWarning: The default value of `n_init` will change from 3 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n",
      "c:\\Users\\YanPC\\Downloads\\automl_hw2-master\\.venv\\lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1934: FutureWarning: The default value of `n_init` will change from 3 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n",
      "c:\\Users\\YanPC\\Downloads\\automl_hw2-master\\.venv\\lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1934: FutureWarning: The default value of `n_init` will change from 3 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n",
      "c:\\Users\\YanPC\\Downloads\\automl_hw2-master\\.venv\\lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1934: FutureWarning: The default value of `n_init` will change from 3 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n",
      "c:\\Users\\YanPC\\Downloads\\automl_hw2-master\\.venv\\lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1934: FutureWarning: The default value of `n_init` will change from 3 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n",
      "c:\\Users\\YanPC\\Downloads\\automl_hw2-master\\.venv\\lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1934: FutureWarning: The default value of `n_init` will change from 3 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n",
      "c:\\Users\\YanPC\\Downloads\\automl_hw2-master\\.venv\\lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1934: FutureWarning: The default value of `n_init` will change from 3 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n",
      "c:\\Users\\YanPC\\Downloads\\automl_hw2-master\\.venv\\lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1934: FutureWarning: The default value of `n_init` will change from 3 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n",
      "c:\\Users\\YanPC\\Downloads\\automl_hw2-master\\.venv\\lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1934: FutureWarning: The default value of `n_init` will change from 3 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n",
      "c:\\Users\\YanPC\\Downloads\\automl_hw2-master\\.venv\\lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1934: FutureWarning: The default value of `n_init` will change from 3 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "37_CatBoost_KMeansFeatures f1 0.833542 trained in 44.19 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\YanPC\\Downloads\\automl_hw2-master\\.venv\\lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1934: FutureWarning: The default value of `n_init` will change from 3 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n",
      "c:\\Users\\YanPC\\Downloads\\automl_hw2-master\\.venv\\lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1934: FutureWarning: The default value of `n_init` will change from 3 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n",
      "c:\\Users\\YanPC\\Downloads\\automl_hw2-master\\.venv\\lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1934: FutureWarning: The default value of `n_init` will change from 3 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n",
      "c:\\Users\\YanPC\\Downloads\\automl_hw2-master\\.venv\\lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1934: FutureWarning: The default value of `n_init` will change from 3 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n",
      "c:\\Users\\YanPC\\Downloads\\automl_hw2-master\\.venv\\lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1934: FutureWarning: The default value of `n_init` will change from 3 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n",
      "c:\\Users\\YanPC\\Downloads\\automl_hw2-master\\.venv\\lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1934: FutureWarning: The default value of `n_init` will change from 3 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n",
      "c:\\Users\\YanPC\\Downloads\\automl_hw2-master\\.venv\\lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1934: FutureWarning: The default value of `n_init` will change from 3 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n",
      "c:\\Users\\YanPC\\Downloads\\automl_hw2-master\\.venv\\lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1934: FutureWarning: The default value of `n_init` will change from 3 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n",
      "c:\\Users\\YanPC\\Downloads\\automl_hw2-master\\.venv\\lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1934: FutureWarning: The default value of `n_init` will change from 3 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n",
      "c:\\Users\\YanPC\\Downloads\\automl_hw2-master\\.venv\\lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1934: FutureWarning: The default value of `n_init` will change from 3 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "36_CatBoost_KMeansFeatures f1 0.809435 trained in 37.11 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\YanPC\\Downloads\\automl_hw2-master\\.venv\\lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1934: FutureWarning: The default value of `n_init` will change from 3 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n",
      "c:\\Users\\YanPC\\Downloads\\automl_hw2-master\\.venv\\lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1934: FutureWarning: The default value of `n_init` will change from 3 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n",
      "c:\\Users\\YanPC\\Downloads\\automl_hw2-master\\.venv\\lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1934: FutureWarning: The default value of `n_init` will change from 3 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n",
      "c:\\Users\\YanPC\\Downloads\\automl_hw2-master\\.venv\\lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1934: FutureWarning: The default value of `n_init` will change from 3 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n",
      "c:\\Users\\YanPC\\Downloads\\automl_hw2-master\\.venv\\lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1934: FutureWarning: The default value of `n_init` will change from 3 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n",
      "c:\\Users\\YanPC\\Downloads\\automl_hw2-master\\.venv\\lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1934: FutureWarning: The default value of `n_init` will change from 3 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n",
      "c:\\Users\\YanPC\\Downloads\\automl_hw2-master\\.venv\\lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1934: FutureWarning: The default value of `n_init` will change from 3 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n",
      "c:\\Users\\YanPC\\Downloads\\automl_hw2-master\\.venv\\lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1934: FutureWarning: The default value of `n_init` will change from 3 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n",
      "c:\\Users\\YanPC\\Downloads\\automl_hw2-master\\.venv\\lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1934: FutureWarning: The default value of `n_init` will change from 3 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n",
      "c:\\Users\\YanPC\\Downloads\\automl_hw2-master\\.venv\\lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1934: FutureWarning: The default value of `n_init` will change from 3 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "32_CatBoost_KMeansFeatures f1 0.815546 trained in 85.24 seconds\n",
      "* Step insert_random_feature will try to check up to 1 model\n",
      "37_CatBoost_RandomFeature f1 0.848225 trained in 47.64 seconds\n",
      "Drop features ['feature_25', 'feature_5', 'feature_10', 'feature_12', 'feature_13', 'feature_7', 'feature_17', 'feature_20', 'feature_21', 'random_feature', 'feature_24', 'feature_18', 'feature_8']\n",
      "* Step features_selection will try to check up to 6 models\n",
      "37_CatBoost_SelectedFeatures f1 0.871007 trained in 28.64 seconds\n",
      "23_LightGBM_SelectedFeatures f1 0.85625 trained in 28.9 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\YanPC\\Downloads\\automl_hw2-master\\.venv\\lib\\site-packages\\xgboost\\training.py:38: UserWarning: `feval` is deprecated, use `custom_metric` instead.  They have different behavior when custom objective is also used.See https://xgboost.readthedocs.io/en/latest/tutorials/custom_metric_obj.html for details on the `custom_metric`.\n",
      "c:\\Users\\YanPC\\Downloads\\automl_hw2-master\\.venv\\lib\\site-packages\\xgboost\\training.py:38: UserWarning: `feval` is deprecated, use `custom_metric` instead.  They have different behavior when custom objective is also used.See https://xgboost.readthedocs.io/en/latest/tutorials/custom_metric_obj.html for details on the `custom_metric`.\n",
      "c:\\Users\\YanPC\\Downloads\\automl_hw2-master\\.venv\\lib\\site-packages\\xgboost\\training.py:38: UserWarning: `feval` is deprecated, use `custom_metric` instead.  They have different behavior when custom objective is also used.See https://xgboost.readthedocs.io/en/latest/tutorials/custom_metric_obj.html for details on the `custom_metric`.\n",
      "c:\\Users\\YanPC\\Downloads\\automl_hw2-master\\.venv\\lib\\site-packages\\xgboost\\training.py:38: UserWarning: `feval` is deprecated, use `custom_metric` instead.  They have different behavior when custom objective is also used.See https://xgboost.readthedocs.io/en/latest/tutorials/custom_metric_obj.html for details on the `custom_metric`.\n",
      "c:\\Users\\YanPC\\Downloads\\automl_hw2-master\\.venv\\lib\\site-packages\\xgboost\\training.py:38: UserWarning: `feval` is deprecated, use `custom_metric` instead.  They have different behavior when custom objective is also used.See https://xgboost.readthedocs.io/en/latest/tutorials/custom_metric_obj.html for details on the `custom_metric`.\n",
      "c:\\Users\\YanPC\\Downloads\\automl_hw2-master\\.venv\\lib\\site-packages\\xgboost\\training.py:38: UserWarning: `feval` is deprecated, use `custom_metric` instead.  They have different behavior when custom objective is also used.See https://xgboost.readthedocs.io/en/latest/tutorials/custom_metric_obj.html for details on the `custom_metric`.\n",
      "c:\\Users\\YanPC\\Downloads\\automl_hw2-master\\.venv\\lib\\site-packages\\xgboost\\training.py:38: UserWarning: `feval` is deprecated, use `custom_metric` instead.  They have different behavior when custom objective is also used.See https://xgboost.readthedocs.io/en/latest/tutorials/custom_metric_obj.html for details on the `custom_metric`.\n",
      "c:\\Users\\YanPC\\Downloads\\automl_hw2-master\\.venv\\lib\\site-packages\\xgboost\\training.py:38: UserWarning: `feval` is deprecated, use `custom_metric` instead.  They have different behavior when custom objective is also used.See https://xgboost.readthedocs.io/en/latest/tutorials/custom_metric_obj.html for details on the `custom_metric`.\n",
      "c:\\Users\\YanPC\\Downloads\\automl_hw2-master\\.venv\\lib\\site-packages\\xgboost\\training.py:38: UserWarning: `feval` is deprecated, use `custom_metric` instead.  They have different behavior when custom objective is also used.See https://xgboost.readthedocs.io/en/latest/tutorials/custom_metric_obj.html for details on the `custom_metric`.\n",
      "c:\\Users\\YanPC\\Downloads\\automl_hw2-master\\.venv\\lib\\site-packages\\xgboost\\training.py:38: UserWarning: `feval` is deprecated, use `custom_metric` instead.  They have different behavior when custom objective is also used.See https://xgboost.readthedocs.io/en/latest/tutorials/custom_metric_obj.html for details on the `custom_metric`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6_Default_Xgboost_SelectedFeatures f1 0.827671 trained in 29.82 seconds\n",
      "45_RandomForest_SelectedFeatures f1 0.813559 trained in 32.02 seconds\n",
      "48_ExtraTrees_SelectedFeatures f1 0.789306 trained in 26.51 seconds\n",
      "58_NeuralNetwork_SelectedFeatures f1 0.707535 trained in 23.55 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\YanPC\\Downloads\\automl_hw2-master\\.venv\\lib\\site-packages\\supervised\\utils\\learning_curves.py:113: FutureWarning: The behavior of Series.argmax/argmin with skipna=False and NAs, or with all-NAs is deprecated. In a future version this will raise ValueError.\n",
      "c:\\Users\\YanPC\\Downloads\\automl_hw2-master\\.venv\\lib\\site-packages\\supervised\\utils\\learning_curves.py:113: FutureWarning: The behavior of Series.argmax/argmin with skipna=False and NAs, or with all-NAs is deprecated. In a future version this will raise ValueError.\n",
      "c:\\Users\\YanPC\\Downloads\\automl_hw2-master\\.venv\\lib\\site-packages\\supervised\\utils\\learning_curves.py:113: FutureWarning: The behavior of Series.argmax/argmin with skipna=False and NAs, or with all-NAs is deprecated. In a future version this will raise ValueError.\n",
      "c:\\Users\\YanPC\\Downloads\\automl_hw2-master\\.venv\\lib\\site-packages\\supervised\\utils\\learning_curves.py:113: FutureWarning: The behavior of Series.argmax/argmin with skipna=False and NAs, or with all-NAs is deprecated. In a future version this will raise ValueError.\n",
      "c:\\Users\\YanPC\\Downloads\\automl_hw2-master\\.venv\\lib\\site-packages\\supervised\\utils\\learning_curves.py:113: FutureWarning: The behavior of Series.argmax/argmin with skipna=False and NAs, or with all-NAs is deprecated. In a future version this will raise ValueError.\n",
      "c:\\Users\\YanPC\\Downloads\\automl_hw2-master\\.venv\\lib\\site-packages\\supervised\\utils\\learning_curves.py:113: FutureWarning: The behavior of Series.argmax/argmin with skipna=False and NAs, or with all-NAs is deprecated. In a future version this will raise ValueError.\n",
      "c:\\Users\\YanPC\\Downloads\\automl_hw2-master\\.venv\\lib\\site-packages\\supervised\\utils\\learning_curves.py:113: FutureWarning: The behavior of Series.argmax/argmin with skipna=False and NAs, or with all-NAs is deprecated. In a future version this will raise ValueError.\n",
      "c:\\Users\\YanPC\\Downloads\\automl_hw2-master\\.venv\\lib\\site-packages\\supervised\\utils\\learning_curves.py:113: FutureWarning: The behavior of Series.argmax/argmin with skipna=False and NAs, or with all-NAs is deprecated. In a future version this will raise ValueError.\n",
      "c:\\Users\\YanPC\\Downloads\\automl_hw2-master\\.venv\\lib\\site-packages\\supervised\\utils\\learning_curves.py:113: FutureWarning: The behavior of Series.argmax/argmin with skipna=False and NAs, or with all-NAs is deprecated. In a future version this will raise ValueError.\n",
      "c:\\Users\\YanPC\\Downloads\\automl_hw2-master\\.venv\\lib\\site-packages\\supervised\\utils\\learning_curves.py:113: FutureWarning: The behavior of Series.argmax/argmin with skipna=False and NAs, or with all-NAs is deprecated. In a future version this will raise ValueError.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "* Step hill_climbing_1 will try to check up to 28 models\n",
      "73_CatBoost_SelectedFeatures f1 0.865727 trained in 29.24 seconds\n",
      "74_CatBoost_SelectedFeatures f1 0.874222 trained in 30.58 seconds\n",
      "75_CatBoost f1 0.844743 trained in 32.25 seconds\n",
      "76_CatBoost f1 0.848485 trained in 37.86 seconds\n",
      "77_LightGBM_SelectedFeatures f1 0.85625 trained in 30.1 seconds\n",
      "78_LightGBM_SelectedFeatures f1 0.85625 trained in 30.24 seconds\n",
      "79_CatBoost f1 0.843924 trained in 26.68 seconds\n",
      "80_LightGBM f1 0.838988 trained in 26.92 seconds\n",
      "81_LightGBM f1 0.838988 trained in 26.98 seconds\n",
      "82_LightGBM f1 0.838789 trained in 27.23 seconds\n",
      "83_LightGBM f1 0.838789 trained in 28.38 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\YanPC\\Downloads\\automl_hw2-master\\.venv\\lib\\site-packages\\xgboost\\training.py:38: UserWarning: `feval` is deprecated, use `custom_metric` instead.  They have different behavior when custom objective is also used.See https://xgboost.readthedocs.io/en/latest/tutorials/custom_metric_obj.html for details on the `custom_metric`.\n",
      "c:\\Users\\YanPC\\Downloads\\automl_hw2-master\\.venv\\lib\\site-packages\\xgboost\\training.py:38: UserWarning: `feval` is deprecated, use `custom_metric` instead.  They have different behavior when custom objective is also used.See https://xgboost.readthedocs.io/en/latest/tutorials/custom_metric_obj.html for details on the `custom_metric`.\n",
      "c:\\Users\\YanPC\\Downloads\\automl_hw2-master\\.venv\\lib\\site-packages\\xgboost\\training.py:38: UserWarning: `feval` is deprecated, use `custom_metric` instead.  They have different behavior when custom objective is also used.See https://xgboost.readthedocs.io/en/latest/tutorials/custom_metric_obj.html for details on the `custom_metric`.\n",
      "c:\\Users\\YanPC\\Downloads\\automl_hw2-master\\.venv\\lib\\site-packages\\xgboost\\training.py:38: UserWarning: `feval` is deprecated, use `custom_metric` instead.  They have different behavior when custom objective is also used.See https://xgboost.readthedocs.io/en/latest/tutorials/custom_metric_obj.html for details on the `custom_metric`.\n",
      "c:\\Users\\YanPC\\Downloads\\automl_hw2-master\\.venv\\lib\\site-packages\\xgboost\\training.py:38: UserWarning: `feval` is deprecated, use `custom_metric` instead.  They have different behavior when custom objective is also used.See https://xgboost.readthedocs.io/en/latest/tutorials/custom_metric_obj.html for details on the `custom_metric`.\n",
      "c:\\Users\\YanPC\\Downloads\\automl_hw2-master\\.venv\\lib\\site-packages\\xgboost\\training.py:38: UserWarning: `feval` is deprecated, use `custom_metric` instead.  They have different behavior when custom objective is also used.See https://xgboost.readthedocs.io/en/latest/tutorials/custom_metric_obj.html for details on the `custom_metric`.\n",
      "c:\\Users\\YanPC\\Downloads\\automl_hw2-master\\.venv\\lib\\site-packages\\xgboost\\training.py:38: UserWarning: `feval` is deprecated, use `custom_metric` instead.  They have different behavior when custom objective is also used.See https://xgboost.readthedocs.io/en/latest/tutorials/custom_metric_obj.html for details on the `custom_metric`.\n",
      "c:\\Users\\YanPC\\Downloads\\automl_hw2-master\\.venv\\lib\\site-packages\\xgboost\\training.py:38: UserWarning: `feval` is deprecated, use `custom_metric` instead.  They have different behavior when custom objective is also used.See https://xgboost.readthedocs.io/en/latest/tutorials/custom_metric_obj.html for details on the `custom_metric`.\n",
      "c:\\Users\\YanPC\\Downloads\\automl_hw2-master\\.venv\\lib\\site-packages\\xgboost\\training.py:38: UserWarning: `feval` is deprecated, use `custom_metric` instead.  They have different behavior when custom objective is also used.See https://xgboost.readthedocs.io/en/latest/tutorials/custom_metric_obj.html for details on the `custom_metric`.\n",
      "c:\\Users\\YanPC\\Downloads\\automl_hw2-master\\.venv\\lib\\site-packages\\xgboost\\training.py:38: UserWarning: `feval` is deprecated, use `custom_metric` instead.  They have different behavior when custom objective is also used.See https://xgboost.readthedocs.io/en/latest/tutorials/custom_metric_obj.html for details on the `custom_metric`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "84_Xgboost_SelectedFeatures f1 0.839875 trained in 33.74 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\YanPC\\Downloads\\automl_hw2-master\\.venv\\lib\\site-packages\\xgboost\\training.py:38: UserWarning: `feval` is deprecated, use `custom_metric` instead.  They have different behavior when custom objective is also used.See https://xgboost.readthedocs.io/en/latest/tutorials/custom_metric_obj.html for details on the `custom_metric`.\n",
      "c:\\Users\\YanPC\\Downloads\\automl_hw2-master\\.venv\\lib\\site-packages\\xgboost\\training.py:38: UserWarning: `feval` is deprecated, use `custom_metric` instead.  They have different behavior when custom objective is also used.See https://xgboost.readthedocs.io/en/latest/tutorials/custom_metric_obj.html for details on the `custom_metric`.\n",
      "c:\\Users\\YanPC\\Downloads\\automl_hw2-master\\.venv\\lib\\site-packages\\xgboost\\training.py:38: UserWarning: `feval` is deprecated, use `custom_metric` instead.  They have different behavior when custom objective is also used.See https://xgboost.readthedocs.io/en/latest/tutorials/custom_metric_obj.html for details on the `custom_metric`.\n",
      "c:\\Users\\YanPC\\Downloads\\automl_hw2-master\\.venv\\lib\\site-packages\\xgboost\\training.py:38: UserWarning: `feval` is deprecated, use `custom_metric` instead.  They have different behavior when custom objective is also used.See https://xgboost.readthedocs.io/en/latest/tutorials/custom_metric_obj.html for details on the `custom_metric`.\n",
      "c:\\Users\\YanPC\\Downloads\\automl_hw2-master\\.venv\\lib\\site-packages\\xgboost\\training.py:38: UserWarning: `feval` is deprecated, use `custom_metric` instead.  They have different behavior when custom objective is also used.See https://xgboost.readthedocs.io/en/latest/tutorials/custom_metric_obj.html for details on the `custom_metric`.\n",
      "c:\\Users\\YanPC\\Downloads\\automl_hw2-master\\.venv\\lib\\site-packages\\xgboost\\training.py:38: UserWarning: `feval` is deprecated, use `custom_metric` instead.  They have different behavior when custom objective is also used.See https://xgboost.readthedocs.io/en/latest/tutorials/custom_metric_obj.html for details on the `custom_metric`.\n",
      "c:\\Users\\YanPC\\Downloads\\automl_hw2-master\\.venv\\lib\\site-packages\\xgboost\\training.py:38: UserWarning: `feval` is deprecated, use `custom_metric` instead.  They have different behavior when custom objective is also used.See https://xgboost.readthedocs.io/en/latest/tutorials/custom_metric_obj.html for details on the `custom_metric`.\n",
      "c:\\Users\\YanPC\\Downloads\\automl_hw2-master\\.venv\\lib\\site-packages\\xgboost\\training.py:38: UserWarning: `feval` is deprecated, use `custom_metric` instead.  They have different behavior when custom objective is also used.See https://xgboost.readthedocs.io/en/latest/tutorials/custom_metric_obj.html for details on the `custom_metric`.\n",
      "c:\\Users\\YanPC\\Downloads\\automl_hw2-master\\.venv\\lib\\site-packages\\xgboost\\training.py:38: UserWarning: `feval` is deprecated, use `custom_metric` instead.  They have different behavior when custom objective is also used.See https://xgboost.readthedocs.io/en/latest/tutorials/custom_metric_obj.html for details on the `custom_metric`.\n",
      "c:\\Users\\YanPC\\Downloads\\automl_hw2-master\\.venv\\lib\\site-packages\\xgboost\\training.py:38: UserWarning: `feval` is deprecated, use `custom_metric` instead.  They have different behavior when custom objective is also used.See https://xgboost.readthedocs.io/en/latest/tutorials/custom_metric_obj.html for details on the `custom_metric`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "85_Xgboost f1 0.82419 trained in 35.24 seconds\n",
      "86_RandomForest f1 0.80803 trained in 38.83 seconds\n",
      "87_RandomForest_SelectedFeatures f1 0.820737 trained in 30.29 seconds\n",
      "88_RandomForest f1 0.814491 trained in 36.12 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\YanPC\\Downloads\\automl_hw2-master\\.venv\\lib\\site-packages\\xgboost\\training.py:38: UserWarning: `feval` is deprecated, use `custom_metric` instead.  They have different behavior when custom objective is also used.See https://xgboost.readthedocs.io/en/latest/tutorials/custom_metric_obj.html for details on the `custom_metric`.\n",
      "c:\\Users\\YanPC\\Downloads\\automl_hw2-master\\.venv\\lib\\site-packages\\xgboost\\training.py:38: UserWarning: `feval` is deprecated, use `custom_metric` instead.  They have different behavior when custom objective is also used.See https://xgboost.readthedocs.io/en/latest/tutorials/custom_metric_obj.html for details on the `custom_metric`.\n",
      "c:\\Users\\YanPC\\Downloads\\automl_hw2-master\\.venv\\lib\\site-packages\\xgboost\\training.py:38: UserWarning: `feval` is deprecated, use `custom_metric` instead.  They have different behavior when custom objective is also used.See https://xgboost.readthedocs.io/en/latest/tutorials/custom_metric_obj.html for details on the `custom_metric`.\n",
      "c:\\Users\\YanPC\\Downloads\\automl_hw2-master\\.venv\\lib\\site-packages\\xgboost\\training.py:38: UserWarning: `feval` is deprecated, use `custom_metric` instead.  They have different behavior when custom objective is also used.See https://xgboost.readthedocs.io/en/latest/tutorials/custom_metric_obj.html for details on the `custom_metric`.\n",
      "c:\\Users\\YanPC\\Downloads\\automl_hw2-master\\.venv\\lib\\site-packages\\xgboost\\training.py:38: UserWarning: `feval` is deprecated, use `custom_metric` instead.  They have different behavior when custom objective is also used.See https://xgboost.readthedocs.io/en/latest/tutorials/custom_metric_obj.html for details on the `custom_metric`.\n",
      "c:\\Users\\YanPC\\Downloads\\automl_hw2-master\\.venv\\lib\\site-packages\\xgboost\\training.py:38: UserWarning: `feval` is deprecated, use `custom_metric` instead.  They have different behavior when custom objective is also used.See https://xgboost.readthedocs.io/en/latest/tutorials/custom_metric_obj.html for details on the `custom_metric`.\n",
      "c:\\Users\\YanPC\\Downloads\\automl_hw2-master\\.venv\\lib\\site-packages\\xgboost\\training.py:38: UserWarning: `feval` is deprecated, use `custom_metric` instead.  They have different behavior when custom objective is also used.See https://xgboost.readthedocs.io/en/latest/tutorials/custom_metric_obj.html for details on the `custom_metric`.\n",
      "c:\\Users\\YanPC\\Downloads\\automl_hw2-master\\.venv\\lib\\site-packages\\xgboost\\training.py:38: UserWarning: `feval` is deprecated, use `custom_metric` instead.  They have different behavior when custom objective is also used.See https://xgboost.readthedocs.io/en/latest/tutorials/custom_metric_obj.html for details on the `custom_metric`.\n",
      "c:\\Users\\YanPC\\Downloads\\automl_hw2-master\\.venv\\lib\\site-packages\\xgboost\\training.py:38: UserWarning: `feval` is deprecated, use `custom_metric` instead.  They have different behavior when custom objective is also used.See https://xgboost.readthedocs.io/en/latest/tutorials/custom_metric_obj.html for details on the `custom_metric`.\n",
      "c:\\Users\\YanPC\\Downloads\\automl_hw2-master\\.venv\\lib\\site-packages\\xgboost\\training.py:38: UserWarning: `feval` is deprecated, use `custom_metric` instead.  They have different behavior when custom objective is also used.See https://xgboost.readthedocs.io/en/latest/tutorials/custom_metric_obj.html for details on the `custom_metric`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "89_Xgboost f1 0.786802 trained in 36.68 seconds\n",
      "90_ExtraTrees_SelectedFeatures f1 0.79156 trained in 29.59 seconds\n",
      "91_ExtraTrees f1 0.780269 trained in 30.98 seconds\n",
      "92_ExtraTrees f1 0.765319 trained in 33.46 seconds\n",
      "93_DecisionTree f1 0.688807 trained in 20.1 seconds\n",
      "94_NeuralNetwork_SelectedFeatures f1 0.749842 trained in 27.06 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\YanPC\\Downloads\\automl_hw2-master\\.venv\\lib\\site-packages\\supervised\\utils\\learning_curves.py:113: FutureWarning: The behavior of Series.argmax/argmin with skipna=False and NAs, or with all-NAs is deprecated. In a future version this will raise ValueError.\n",
      "c:\\Users\\YanPC\\Downloads\\automl_hw2-master\\.venv\\lib\\site-packages\\supervised\\utils\\learning_curves.py:113: FutureWarning: The behavior of Series.argmax/argmin with skipna=False and NAs, or with all-NAs is deprecated. In a future version this will raise ValueError.\n",
      "c:\\Users\\YanPC\\Downloads\\automl_hw2-master\\.venv\\lib\\site-packages\\supervised\\utils\\learning_curves.py:113: FutureWarning: The behavior of Series.argmax/argmin with skipna=False and NAs, or with all-NAs is deprecated. In a future version this will raise ValueError.\n",
      "c:\\Users\\YanPC\\Downloads\\automl_hw2-master\\.venv\\lib\\site-packages\\supervised\\utils\\learning_curves.py:113: FutureWarning: The behavior of Series.argmax/argmin with skipna=False and NAs, or with all-NAs is deprecated. In a future version this will raise ValueError.\n",
      "c:\\Users\\YanPC\\Downloads\\automl_hw2-master\\.venv\\lib\\site-packages\\supervised\\utils\\learning_curves.py:113: FutureWarning: The behavior of Series.argmax/argmin with skipna=False and NAs, or with all-NAs is deprecated. In a future version this will raise ValueError.\n",
      "c:\\Users\\YanPC\\Downloads\\automl_hw2-master\\.venv\\lib\\site-packages\\supervised\\utils\\learning_curves.py:113: FutureWarning: The behavior of Series.argmax/argmin with skipna=False and NAs, or with all-NAs is deprecated. In a future version this will raise ValueError.\n",
      "c:\\Users\\YanPC\\Downloads\\automl_hw2-master\\.venv\\lib\\site-packages\\supervised\\utils\\learning_curves.py:113: FutureWarning: The behavior of Series.argmax/argmin with skipna=False and NAs, or with all-NAs is deprecated. In a future version this will raise ValueError.\n",
      "c:\\Users\\YanPC\\Downloads\\automl_hw2-master\\.venv\\lib\\site-packages\\supervised\\utils\\learning_curves.py:113: FutureWarning: The behavior of Series.argmax/argmin with skipna=False and NAs, or with all-NAs is deprecated. In a future version this will raise ValueError.\n",
      "c:\\Users\\YanPC\\Downloads\\automl_hw2-master\\.venv\\lib\\site-packages\\supervised\\utils\\learning_curves.py:113: FutureWarning: The behavior of Series.argmax/argmin with skipna=False and NAs, or with all-NAs is deprecated. In a future version this will raise ValueError.\n",
      "c:\\Users\\YanPC\\Downloads\\automl_hw2-master\\.venv\\lib\\site-packages\\supervised\\utils\\learning_curves.py:113: FutureWarning: The behavior of Series.argmax/argmin with skipna=False and NAs, or with all-NAs is deprecated. In a future version this will raise ValueError.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "95_NeuralNetwork f1 0.700129 trained in 25.74 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\YanPC\\Downloads\\automl_hw2-master\\.venv\\lib\\site-packages\\supervised\\utils\\learning_curves.py:113: FutureWarning: The behavior of Series.argmax/argmin with skipna=False and NAs, or with all-NAs is deprecated. In a future version this will raise ValueError.\n",
      "c:\\Users\\YanPC\\Downloads\\automl_hw2-master\\.venv\\lib\\site-packages\\supervised\\utils\\learning_curves.py:113: FutureWarning: The behavior of Series.argmax/argmin with skipna=False and NAs, or with all-NAs is deprecated. In a future version this will raise ValueError.\n",
      "c:\\Users\\YanPC\\Downloads\\automl_hw2-master\\.venv\\lib\\site-packages\\supervised\\utils\\learning_curves.py:113: FutureWarning: The behavior of Series.argmax/argmin with skipna=False and NAs, or with all-NAs is deprecated. In a future version this will raise ValueError.\n",
      "c:\\Users\\YanPC\\Downloads\\automl_hw2-master\\.venv\\lib\\site-packages\\supervised\\utils\\learning_curves.py:113: FutureWarning: The behavior of Series.argmax/argmin with skipna=False and NAs, or with all-NAs is deprecated. In a future version this will raise ValueError.\n",
      "c:\\Users\\YanPC\\Downloads\\automl_hw2-master\\.venv\\lib\\site-packages\\supervised\\utils\\learning_curves.py:113: FutureWarning: The behavior of Series.argmax/argmin with skipna=False and NAs, or with all-NAs is deprecated. In a future version this will raise ValueError.\n",
      "c:\\Users\\YanPC\\Downloads\\automl_hw2-master\\.venv\\lib\\site-packages\\supervised\\utils\\learning_curves.py:113: FutureWarning: The behavior of Series.argmax/argmin with skipna=False and NAs, or with all-NAs is deprecated. In a future version this will raise ValueError.\n",
      "c:\\Users\\YanPC\\Downloads\\automl_hw2-master\\.venv\\lib\\site-packages\\supervised\\utils\\learning_curves.py:113: FutureWarning: The behavior of Series.argmax/argmin with skipna=False and NAs, or with all-NAs is deprecated. In a future version this will raise ValueError.\n",
      "c:\\Users\\YanPC\\Downloads\\automl_hw2-master\\.venv\\lib\\site-packages\\supervised\\utils\\learning_curves.py:113: FutureWarning: The behavior of Series.argmax/argmin with skipna=False and NAs, or with all-NAs is deprecated. In a future version this will raise ValueError.\n",
      "c:\\Users\\YanPC\\Downloads\\automl_hw2-master\\.venv\\lib\\site-packages\\supervised\\utils\\learning_curves.py:113: FutureWarning: The behavior of Series.argmax/argmin with skipna=False and NAs, or with all-NAs is deprecated. In a future version this will raise ValueError.\n",
      "c:\\Users\\YanPC\\Downloads\\automl_hw2-master\\.venv\\lib\\site-packages\\supervised\\utils\\learning_curves.py:113: FutureWarning: The behavior of Series.argmax/argmin with skipna=False and NAs, or with all-NAs is deprecated. In a future version this will raise ValueError.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "96_DecisionTree f1 0.688807 trained in 20.68 seconds\n",
      "97_NearestNeighbors f1 0.652527 trained in 22.73 seconds\n",
      "98_NearestNeighbors f1 0.652527 trained in 22.87 seconds\n",
      "99_NearestNeighbors f1 0.639112 trained in 33.12 seconds\n",
      "100_DecisionTree f1 0.706651 trained in 23.31 seconds\n",
      "* Step hill_climbing_2 will try to check up to 12 models\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[32], line 11\u001b[0m\n\u001b[0;32m      9\u001b[0m train_y \u001b[38;5;241m=\u001b[39m train_y\u001b[38;5;241m.\u001b[39mcopy()\u001b[38;5;241m.\u001b[39mreshape(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m     10\u001b[0m \u001b[38;5;28mprint\u001b[39m(train_y)\n\u001b[1;32m---> 11\u001b[0m \u001b[43mautoml\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain_x\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcopy\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_y\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\YanPC\\Downloads\\automl_hw2-master\\.venv\\lib\\site-packages\\supervised\\automl.py:433\u001b[0m, in \u001b[0;36mAutoML.fit\u001b[1;34m(self, X, y, sample_weight, cv, sensitive_features)\u001b[0m\n\u001b[0;32m    406\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mfit\u001b[39m(\n\u001b[0;32m    407\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m    408\u001b[0m     X: Union[numpy\u001b[38;5;241m.\u001b[39mndarray, pandas\u001b[38;5;241m.\u001b[39mDataFrame],\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    414\u001b[0m     ] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m    415\u001b[0m ):\n\u001b[0;32m    416\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Fit the AutoML model.\u001b[39;00m\n\u001b[0;32m    417\u001b[0m \n\u001b[0;32m    418\u001b[0m \u001b[38;5;124;03m    Arguments:\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    431\u001b[0m \u001b[38;5;124;03m        AutoML object: Returns `self`\u001b[39;00m\n\u001b[0;32m    432\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 433\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_fit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msample_weight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcv\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msensitive_features\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\YanPC\\Downloads\\automl_hw2-master\\.venv\\lib\\site-packages\\supervised\\base_automl.py:1195\u001b[0m, in \u001b[0;36mBaseAutoML._fit\u001b[1;34m(self, X, y, sample_weight, cv, sensitive_features)\u001b[0m\n\u001b[0;32m   1191\u001b[0m     trained \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mensemble_step(\n\u001b[0;32m   1192\u001b[0m         is_stacked\u001b[38;5;241m=\u001b[39mparams[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mis_stacked\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[0;32m   1193\u001b[0m     )\n\u001b[0;32m   1194\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1195\u001b[0m     trained \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mparams\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1196\u001b[0m params[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstatus\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtrained\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m trained \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mskipped\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1197\u001b[0m params[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfinal_loss\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_models[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m]\u001b[38;5;241m.\u001b[39mget_final_loss()\n",
      "File \u001b[1;32mc:\\Users\\YanPC\\Downloads\\automl_hw2-master\\.venv\\lib\\site-packages\\supervised\\base_automl.py:401\u001b[0m, in \u001b[0;36mBaseAutoML.train_model\u001b[1;34m(self, params)\u001b[0m\n\u001b[0;32m    397\u001b[0m \u001b[38;5;66;03m# start training\u001b[39;00m\n\u001b[0;32m    398\u001b[0m logger\u001b[38;5;241m.\u001b[39minfo(\n\u001b[0;32m    399\u001b[0m     \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTrain model #\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_models)\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m / Model name: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mparams[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mname\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    400\u001b[0m )\n\u001b[1;32m--> 401\u001b[0m \u001b[43mmf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mresults_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel_subpath\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    403\u001b[0m \u001b[38;5;66;03m# keep info about the model\u001b[39;00m\n\u001b[0;32m    404\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mkeep_model(mf, model_subpath)\n",
      "File \u001b[1;32mc:\\Users\\YanPC\\Downloads\\automl_hw2-master\\.venv\\lib\\site-packages\\supervised\\model_framework.py:254\u001b[0m, in \u001b[0;36mModelFramework.train\u001b[1;34m(self, results_path, model_subpath)\u001b[0m\n\u001b[0;32m    251\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(learner\u001b[38;5;241m.\u001b[39mmax_iters):\n\u001b[0;32m    252\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcallbacks\u001b[38;5;241m.\u001b[39mon_iteration_start()\n\u001b[1;32m--> 254\u001b[0m     \u001b[43mlearner\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    255\u001b[0m \u001b[43m        \u001b[49m\u001b[43mX_train\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    256\u001b[0m \u001b[43m        \u001b[49m\u001b[43my_train\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    257\u001b[0m \u001b[43m        \u001b[49m\u001b[43msample_weight\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    258\u001b[0m \u001b[43m        \u001b[49m\u001b[43mX_validation\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    259\u001b[0m \u001b[43m        \u001b[49m\u001b[43my_validation\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    260\u001b[0m \u001b[43m        \u001b[49m\u001b[43msample_weight_validation\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    261\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlog_to_file\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    262\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_max_time_for_learner\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    263\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    265\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mparams\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124minjected_sample_weight\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mFalse\u001b[39;00m):\n\u001b[0;32m    266\u001b[0m         \u001b[38;5;66;03m# print(\"Dont use sample weight in model evaluation\")\u001b[39;00m\n\u001b[0;32m    267\u001b[0m         sample_weight \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\YanPC\\Downloads\\automl_hw2-master\\.venv\\lib\\site-packages\\supervised\\algorithms\\catboost.py:225\u001b[0m, in \u001b[0;36mCatBoostAlgorithm.fit\u001b[1;34m(self, X, y, sample_weight, X_validation, y_validation, sample_weight_validation, log_to_file, max_time)\u001b[0m\n\u001b[0;32m    222\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel\u001b[38;5;241m.\u001b[39mset_params(iterations\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mparams\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnum_boost_round\u001b[39m\u001b[38;5;124m\"\u001b[39m))\n\u001b[0;32m    223\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mearly_stopping_rounds \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mparams\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mearly_stopping_rounds\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;241m50\u001b[39m)\n\u001b[1;32m--> 225\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    226\u001b[0m \u001b[43m    \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    227\u001b[0m \u001b[43m    \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    228\u001b[0m \u001b[43m    \u001b[49m\u001b[43msample_weight\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msample_weight\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    229\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcat_features\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcat_features\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    230\u001b[0m \u001b[43m    \u001b[49m\u001b[43minit_model\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmodel_init\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    231\u001b[0m \u001b[43m    \u001b[49m\u001b[43meval_set\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43meval_set\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    232\u001b[0m \u001b[43m    \u001b[49m\u001b[43mearly_stopping_rounds\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mearly_stopping_rounds\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    233\u001b[0m \u001b[43m    \u001b[49m\u001b[43mverbose_eval\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    234\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    236\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel\u001b[38;5;241m.\u001b[39mbest_iteration_ \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    237\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m model_init \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[1;32mc:\\Users\\YanPC\\Downloads\\automl_hw2-master\\.venv\\lib\\site-packages\\catboost\\core.py:5100\u001b[0m, in \u001b[0;36mCatBoostClassifier.fit\u001b[1;34m(self, X, y, cat_features, text_features, embedding_features, sample_weight, baseline, use_best_model, eval_set, verbose, logging_level, plot, plot_file, column_description, verbose_eval, metric_period, silent, early_stopping_rounds, save_snapshot, snapshot_file, snapshot_interval, init_model, callbacks, log_cout, log_cerr)\u001b[0m\n\u001b[0;32m   5097\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mloss_function\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;129;01min\u001b[39;00m params:\n\u001b[0;32m   5098\u001b[0m     CatBoostClassifier\u001b[38;5;241m.\u001b[39m_check_is_compatible_loss(params[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mloss_function\u001b[39m\u001b[38;5;124m'\u001b[39m])\n\u001b[1;32m-> 5100\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_fit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcat_features\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtext_features\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43membedding_features\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msample_weight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbaseline\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43muse_best_model\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   5101\u001b[0m \u001b[43m          \u001b[49m\u001b[43meval_set\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mverbose\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlogging_level\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mplot\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mplot_file\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcolumn_description\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mverbose_eval\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmetric_period\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   5102\u001b[0m \u001b[43m          \u001b[49m\u001b[43msilent\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mearly_stopping_rounds\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msave_snapshot\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msnapshot_file\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msnapshot_interval\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minit_model\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlog_cout\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlog_cerr\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   5103\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\n",
      "File \u001b[1;32mc:\\Users\\YanPC\\Downloads\\automl_hw2-master\\.venv\\lib\\site-packages\\catboost\\core.py:2319\u001b[0m, in \u001b[0;36mCatBoost._fit\u001b[1;34m(self, X, y, cat_features, text_features, embedding_features, pairs, sample_weight, group_id, group_weight, subgroup_id, pairs_weight, baseline, use_best_model, eval_set, verbose, logging_level, plot, plot_file, column_description, verbose_eval, metric_period, silent, early_stopping_rounds, save_snapshot, snapshot_file, snapshot_interval, init_model, callbacks, log_cout, log_cerr)\u001b[0m\n\u001b[0;32m   2315\u001b[0m allow_clear_pool \u001b[38;5;241m=\u001b[39m train_params[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mallow_clear_pool\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[0;32m   2317\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m log_fixup(log_cout, log_cerr), \\\n\u001b[0;32m   2318\u001b[0m     plot_wrapper(plot, plot_file, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mTraining plots\u001b[39m\u001b[38;5;124m'\u001b[39m, [_get_train_dir(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mget_params())]):\n\u001b[1;32m-> 2319\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_train\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   2320\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtrain_pool\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2321\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtrain_params\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43meval_sets\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2322\u001b[0m \u001b[43m        \u001b[49m\u001b[43mparams\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2323\u001b[0m \u001b[43m        \u001b[49m\u001b[43mallow_clear_pool\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2324\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtrain_params\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43minit_model\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\n\u001b[0;32m   2325\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   2327\u001b[0m \u001b[38;5;66;03m# Have property feature_importance possibly set\u001b[39;00m\n\u001b[0;32m   2328\u001b[0m loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_object\u001b[38;5;241m.\u001b[39m_get_loss_function_name()\n",
      "File \u001b[1;32mc:\\Users\\YanPC\\Downloads\\automl_hw2-master\\.venv\\lib\\site-packages\\catboost\\core.py:1723\u001b[0m, in \u001b[0;36m_CatBoostBase._train\u001b[1;34m(self, train_pool, test_pool, params, allow_clear_pool, init_model)\u001b[0m\n\u001b[0;32m   1722\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_train\u001b[39m(\u001b[38;5;28mself\u001b[39m, train_pool, test_pool, params, allow_clear_pool, init_model):\n\u001b[1;32m-> 1723\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_object\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_train\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain_pool\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtest_pool\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mparams\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mallow_clear_pool\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minit_model\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_object\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43minit_model\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[0;32m   1724\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_set_trained_model_attributes()\n",
      "File \u001b[1;32m_catboost.pyx:4645\u001b[0m, in \u001b[0;36m_catboost._CatBoost._train\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32m_catboost.pyx:4694\u001b[0m, in \u001b[0;36m_catboost._CatBoost._train\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "automl = AutoML(\n",
    "    mode=\"Compete\",\n",
    "    ml_task=\"binary_classification\",\n",
    "    total_time_limit=TRAIN_TIME_LIMIT_MLJAR,\n",
    "    eval_metric=\"f1\",\n",
    "    random_state=SEED,\n",
    "    results_path=path.join(OUTPUT_DIR_MLJAR, UNIQUE_ID, \"tmp\"),\n",
    ")\n",
    "train_y = train_y.copy().reshape(-1)\n",
    "print(train_y)\n",
    "automl.fit(train_x.copy(), train_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loaded = AutoML(\n",
    "    mode=\"Compete\",\n",
    "    ml_task=\"binary_classification\",\n",
    "    total_time_limit=TRAIN_TIME_LIMIT_MLJAR,\n",
    "    eval_metric=\"f1\",\n",
    "    random_state=SEED,\n",
    "    results_path=\"output\\\\mljar\\\\20240114_002215\",\n",
    ")\n",
    "\n",
    "print(valid_x.shape, valid_y.shape)\n",
    "\n",
    "print(train_x.shape, train_y.shape)\n",
    "predictions = loaded.predict_proba(valid_x.copy().reshape(-1))\n",
    "\n",
    "score = balanced_accuracy_score(valid_y, predictions)\n",
    "\n",
    "print(f\"Model Balanced Accuracy: {score}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_path = path.join(OUTPUT_DIR_MLJAR, UNIQUE_ID, \"mljar_model_proba.txt\")\n",
    "dump_proba(automl, test_x, output_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ensemble_predict(X, model1, model2):\n",
    "    pred1 = model1.predict_proba(pd.DataFrame(X)).values[:, 1]\n",
    "    pred2 = model2.predict_proba(X)[:, 1]\n",
    "    print(pred1, pred2)\n",
    "    avg_pred = (pred1 + pred2) / 2\n",
    "\n",
    "    return avg_pred\n",
    "\n",
    "\n",
    "final_predictions = ensemble_predict(test_x, predictor, automl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.makedirs(path.join(\"ensamble\", UNIQUE_ID), exist_ok=True)\n",
    "\n",
    "np.savetxt(\n",
    "    path.join(\"ensamble\", UNIQUE_ID, \"123manual_model_pred.txt\"),\n",
    "    final_predictions,\n",
    "    delimiter=\"\\n\",\n",
    "    comments=\"\",\n",
    "    header='\"313201\"',\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Auto SKLearn TODO OUT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install auto-sklearn\n",
    "# !pip install ydata-profiling\n",
    "# from autosklearn.classification import AutoSklearnClassifier\n",
    "from autosklearn.experimental.askl2 import AutoSklearn2Classifier\n",
    "from autosklearn.metrics import balanced_accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "settings = {\n",
    "    \"time_left_for_this_task\": TRAIN_TIME_LIMIT_AUTO_SKLEARN,\n",
    "    \"seed\": SEED,\n",
    "    \"metric\": balanced_accuracy,\n",
    "    \"n_jobs\": -1,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "askl2 = AutoSklearn2Classifier(**settings)\n",
    "askl2.fit(train_x, train_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "leaderboard = askl2.leaderboard(sort_by=\"model_id\", ensemble_only=True)\n",
    "print(leaderboard)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = askl2.predict(valid_x)\n",
    "balanced_accuracy_score(valid_y, predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "proba = askl2.predict_proba(test_x)\n",
    "output_path = path.join(OUTPUT_DIR_AUTO_SKLEARN, UNIQUE_ID, \"manual_model.txt\")\n",
    "np.savetxt(output_path, proba, delimiter=\"\\n\")\n",
    "askl2.save(path.join(OUTPUT_DIR_AUTO_SKLEARN, UNIQUE_ID, \"manual_model.pkl\"))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
