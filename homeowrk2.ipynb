{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.feature_selection import VarianceThreshold\n",
    "from sklearn.metrics import mutual_info_score\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "####################################################################\n",
    "#                          Read data                               #\n",
    "####################################################################\n",
    "\n",
    "prefix = \"\"\n",
    "\n",
    "_test_x = pd.read_table(prefix + \"artificial_test.data\", sep=\" \", header=None)\n",
    "_test_x.drop(_test_x.columns[500], axis=1, inplace=True)\n",
    "_train_y = pd.read_table(prefix + \"artificial_train.labels\", header=None)\n",
    "_train_x = pd.read_table(prefix + \"artificial_train.data\", sep=\" \", header=None)\n",
    "_train_x.drop(_train_x.columns[500], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_train_and_validation_data():\n",
    "    split = 400\n",
    "    train_x, valid_x = _train_x[split:], _train_x[:split]\n",
    "    train_y, valid_y = _train_y[split:], _train_y[:split]\n",
    "    print(\"train_x.shape: \", train_x.shape)\n",
    "    print(\"train_y.shape: \", train_y.shape)\n",
    "    print(\"valid_x.shape: \", valid_x.shape)\n",
    "    print(\"valid_y.shape: \", valid_y.shape)\n",
    "    return train_x, train_y, valid_x, valid_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_x.shape:  (1600, 500)\n",
      "train_y.shape:  (1600, 1)\n",
      "valid_x.shape:  (400, 500)\n",
      "valid_y.shape:  (400, 1)\n"
     ]
    }
   ],
   "source": [
    "train_x, train_y, valid_x, valid_y = get_train_and_validation_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Remove Highly Correlated Columns\n",
    "def remove_highly_correlated_features(train_x, valid_x, threshold=0.95):\n",
    "    corr_matrix = train_x.corr().abs()\n",
    "    # Select upper triangle of correlation matrix\n",
    "    upper = corr_matrix.where(np.triu(np.ones(corr_matrix.shape), k=1).astype(bool))\n",
    "    # Find index of feature columns with correlation greater than threshold\n",
    "    to_drop = [column for column in upper.columns if any(upper[column] > threshold)]\n",
    "    train_x = train_x.drop(to_drop, axis=1)\n",
    "    valid_x = valid_x.drop(to_drop, axis=1)\n",
    "    return train_x, valid_x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1600, 490)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_x, valid_x = remove_highly_correlated_features(train_x, valid_x)\n",
    "train_x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. Remove Low Variance Columns\n",
    "def remove_low_variance_features(train_x, valid_x, threshold=(0.8 * (1 - 0.8))):\n",
    "    sel = VarianceThreshold(threshold=threshold)\n",
    "    sel.fit(train_x)\n",
    "    train_x = train_x[train_x.columns[sel.get_support(indices=True)]]\n",
    "    valid_x = valid_x[valid_x.columns[sel.get_support(indices=True)]]\n",
    "    return train_x, valid_x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1600, 490)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_x, valid_x = remove_low_variance_features(train_x, valid_x)\n",
    "train_x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. Remove Random Columns (Optional)\n",
    "# This step is an approximation and should be tailored to your specific needs\n",
    "# Here we use a Decision Tree to estimate feature importance\n",
    "def remove_random_features(\n",
    "    train_x: pd.DataFrame,\n",
    "    train_y: pd.DataFrame,\n",
    "    valid_x: pd.DataFrame,\n",
    "    importance=0.005,\n",
    "):\n",
    "    tree: DecisionTreeClassifier = DecisionTreeClassifier(random_state=0)\n",
    "    tree.fit(train_x, train_y)\n",
    "    importances = tree.feature_importances_\n",
    "\n",
    "    # Assume columns with very low importance are \"random\"\n",
    "    # This threshold can be adjusted based on domain knowledge\n",
    "    important_indices = [i for i, imp in enumerate(importances) if imp > importance]\n",
    "    train_x = train_x.iloc[:, important_indices]\n",
    "    valid_x = valid_x.iloc[:, important_indices]\n",
    "    return train_x, valid_x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1600, 51)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_x, valid_x = remove_random_features(\n",
    "    train_x=train_x, train_y=train_y, valid_x=valid_x\n",
    ")\n",
    "train_x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.feature_selection import SelectKBest, f_classif\n",
    "\n",
    "\n",
    "def anova_filter(\n",
    "    train_x: pd.DataFrame,\n",
    "    train_y: pd.DataFrame,\n",
    "    valid_x: pd.DataFrame,\n",
    "    k: int = 50,\n",
    "):\n",
    "    # Using ANOVA F-test to select features\n",
    "    selector = SelectKBest(\n",
    "        f_classif, k=k\n",
    "    )  # Change k to select the number of features you want\n",
    "    selector.fit(train_x, train_y)\n",
    "\n",
    "    # Get F-values and p-values for each feature\n",
    "    f_values = selector.scores_\n",
    "    p_values = selector.pvalues_\n",
    "\n",
    "    # Selecting features (you can use a threshold or select top k features)\n",
    "    selected_features = train_x.columns[selector.get_support()]\n",
    "\n",
    "    # Transforming train_x to include only the selected features\n",
    "    train_x = selector.transform(train_x)\n",
    "    valid_x = selector.transform(valid_x)\n",
    "    return train_x, valid_x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_x, valid_x = anove_filter(train_x=train_x, train_y=train_y, valid_x=valid_x)\n",
    "# train_x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "label = \"class\"\n",
    "train_y = train_y.rename(columns={0: label})\n",
    "valid_y = valid_y.rename(columns={0: label})\n",
    "train_data = pd.concat([train_x, train_y[label]], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_x.shape:  (1600, 500)\n",
      "train_y.shape:  (1600, 1)\n",
      "valid_x.shape:  (400, 500)\n",
      "valid_y.shape:  (400, 1)\n"
     ]
    }
   ],
   "source": [
    "# sanity check\n",
    "(\n",
    "    original_train_x,\n",
    "    original_train_y,\n",
    "    original_valid_x,\n",
    "    original_valid_y,\n",
    ") = get_train_and_validation_data()\n",
    "for y, original_y in zip([train_y, valid_y], [original_train_y, original_valid_y]):\n",
    "    assert y.shape == original_y.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### manual model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_y = train_y.values.ravel()\n",
    "valid_y = valid_y.values.ravel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 100 candidates, totalling 500 fits\n",
      "Model Balanced Accuracy: 0.7947368421052632\n"
     ]
    }
   ],
   "source": [
    "from scipy.stats import randint, uniform\n",
    "from sklearn.ensemble import RandomForestClassifier, StackingClassifier\n",
    "from sklearn.linear_model import ElasticNet, LogisticRegression\n",
    "from sklearn.metrics import balanced_accuracy_score\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "# Base classifiers for the stacking ensemble\n",
    "base_classifiers = [\n",
    "    (\n",
    "        \"rf\",\n",
    "        make_pipeline(\n",
    "            StandardScaler(), RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "        ),\n",
    "    ),\n",
    "    (\"svc\", make_pipeline(StandardScaler(), SVC(random_state=42))),\n",
    "    (\"dt\", make_pipeline(StandardScaler(), DecisionTreeClassifier(random_state=42))),\n",
    "    (\n",
    "        \"elasticnet\",\n",
    "        make_pipeline(\n",
    "            StandardScaler(),\n",
    "            ElasticNet(\n",
    "                alpha=0.0001, l1_ratio=0.15, max_iter=1000, tol=1e-3, random_state=42\n",
    "            ),\n",
    "        ),\n",
    "    ),\n",
    "    (\n",
    "        \"mlp\",\n",
    "        make_pipeline(\n",
    "            StandardScaler(),\n",
    "            MLPClassifier(\n",
    "                random_state=42,\n",
    "                max_iter=1000,\n",
    "                tol=1e-3,\n",
    "                hidden_layer_sizes=(100, 300, 200, 100),\n",
    "            ),\n",
    "        ),\n",
    "    ),\n",
    "]\n",
    "\n",
    "param_distributions = {\n",
    "    \"stackingclassifier__rf__randomforestclassifier__n_estimators\": randint(50, 200),\n",
    "    \"stackingclassifier__rf__randomforestclassifier__max_depth\": randint(3, 50),\n",
    "    \"stackingclassifier__rf__randomforestclassifier__min_samples_split\": randint(2, 20),\n",
    "    \"stackingclassifier__rf__randomforestclassifier__min_samples_leaf\": randint(1, 20),\n",
    "    \"stackingclassifier__svc__svc__C\": uniform(0.1, 10),\n",
    "    \"stackingclassifier__svc__svc__gamma\": [\"scale\", \"auto\"],\n",
    "    \"stackingclassifier__dt__decisiontreeclassifier__max_depth\": randint(3, 50),\n",
    "    \"stackingclassifier__dt__decisiontreeclassifier__min_samples_split\": randint(2, 20),\n",
    "    \"stackingclassifier__dt__decisiontreeclassifier__min_samples_leaf\": randint(1, 20),\n",
    "    \"stackingclassifier__elasticnet__elasticnet__alpha\": uniform(0.0001, 1),\n",
    "    \"stackingclassifier__elasticnet__elasticnet__l1_ratio\": uniform(0, 1),\n",
    "    \"stackingclassifier__mlp__mlpclassifier__alpha\": uniform(0.0001, 1),\n",
    "    \"stackingclassifier__mlp__mlpclassifier__learning_rate_init\": uniform(0.001, 0.1),\n",
    "    \"stackingclassifier__mlp__mlpclassifier__hidden_layer_sizes\": [\n",
    "        (100, 300, 200, 100),\n",
    "        (100, 300, 200, 100, 50),\n",
    "        (100, 300, 200, 100, 50, 25),\n",
    "    ],\n",
    "    \"stackingclassifier__final_estimator__C\": uniform(0.01, 10),\n",
    "}\n",
    "\n",
    "\n",
    "# Stacking ensemble model\n",
    "stacked_ensemble_model = make_pipeline(\n",
    "    StackingClassifier(\n",
    "        estimators=base_classifiers,\n",
    "        final_estimator=LogisticRegression(),\n",
    "        cv=5,\n",
    "    )\n",
    ")\n",
    "\n",
    "# Randomized Search with cross-validation\n",
    "random_search = RandomizedSearchCV(\n",
    "    stacked_ensemble_model,\n",
    "    param_distributions=param_distributions,\n",
    "    scoring=\"balanced_accuracy\",\n",
    "    n_iter=100,  # Number of parameter settings that are sampled\n",
    "    cv=5,  # Cross-validation splitting strategy\n",
    "    verbose=4,  # Controls the verbosity: the higher, the more messages\n",
    "    random_state=412,\n",
    "    n_jobs=-1,  # Number of jobs to run in parallel\n",
    ")\n",
    "random_search.fit(train_x, train_y)\n",
    "y_pred = random_search.predict(valid_x)\n",
    "balanced_accuracy = balanced_accuracy_score(valid_y, y_pred)\n",
    "print(f\"Model Balanced Accuracy: {balanced_accuracy}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from autogluon.tabular import TabularPredictor\n",
    "\n",
    "# save_path = \"some_path\"\n",
    "# # train for 3 minutes with increased num_boost_round\n",
    "# predictor = TabularPredictor(\n",
    "#     label=label, path=save_path, eval_metric=\"balanced_accuracy\",  problem_type=\"binary\"\n",
    "# ).fit(train_data, time_limit=60 * 10,presets = \"best_quality\", hyperparameters =\"default\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# predictor.leaderboard()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # check on validation data\n",
    "# print(valid_x.shape)\n",
    "# print(valid_y.shape)\n",
    "# valid_data = pd.concat([valid_x, valid_y[label]], axis=1)\n",
    "# predictor.evaluate(valid_data)\n",
    "# # best model WeightedEnsamble_L2 score 0.86"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install mljar-supervised"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from supervised.automl import AutoML  # mljar-supervised\n",
    "\n",
    "# train models with AutoML\n",
    "automl = AutoML(\n",
    "    mode=\"Compete\",\n",
    "    ml_task=\"binary_classification\",\n",
    "    total_time_limit=60 * 10,\n",
    "    eval_metric=\"f1\",\n",
    ")\n",
    "automl.fit(train_x, train_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import balanced_accuracy_score\n",
    "\n",
    "predictions = automl.predict(valid_x)\n",
    "balanced_accuracy_score(valid_y, predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install auto-sklearn\n",
    "# !pip install ydata-profiling\n",
    "from autosklearn.classification import AutoSklearnClassifier\n",
    "from autosklearn.experimental.askl2 import AutoSklearn2Classifier\n",
    "from autosklearn.metrics import balanced_accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'balanced_accuracy' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[2], line 8\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# Edit the settings to try in both AutoSklearn1 and AutoSklearn2\u001b[39;00m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;66;03m# Possibilities https://automl.github.io/auto-sklearn/master/api.html\u001b[39;00m\n\u001b[0;32m      3\u001b[0m \n\u001b[0;32m      4\u001b[0m \u001b[38;5;66;03m#------------------------- edit code here\u001b[39;00m\n\u001b[0;32m      5\u001b[0m settings \u001b[38;5;241m=\u001b[39m {\n\u001b[0;32m      6\u001b[0m   \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtime_left_for_this_task\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;241m120\u001b[39m,  \u001b[38;5;66;03m# seconds\u001b[39;00m\n\u001b[0;32m      7\u001b[0m   \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mseed\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;241m42\u001b[39m,\n\u001b[1;32m----> 8\u001b[0m   \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmetric\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[43mbalanced_accuracy\u001b[49m,\n\u001b[0;32m      9\u001b[0m   \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mn_jobs\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;241m4\u001b[39m,\n\u001b[0;32m     10\u001b[0m }\n\u001b[0;32m     13\u001b[0m \u001b[38;5;66;03m# This will only be used by autosklearn 1 while autosklearn 2 will automatically\u001b[39;00m\n\u001b[0;32m     14\u001b[0m \u001b[38;5;66;03m# select a strategy\u001b[39;00m\n\u001b[0;32m     15\u001b[0m resampling_strategy \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mholdout\u001b[39m\u001b[38;5;124m\"\u001b[39m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'balanced_accuracy' is not defined"
     ]
    }
   ],
   "source": [
    "# Edit the settings to try in both AutoSklearn1 and AutoSklearn2\n",
    "# Possibilities https://automl.github.io/auto-sklearn/master/api.html\n",
    "\n",
    "# ------------------------- edit code here\n",
    "settings = {\n",
    "    \"time_left_for_this_task\": 120,  # seconds\n",
    "    \"seed\": 42,\n",
    "    \"metric\": balanced_accuracy,\n",
    "    \"n_jobs\": 4,\n",
    "}\n",
    "\n",
    "\n",
    "# This will only be used by autosklearn 1 while autosklearn 2 will automatically\n",
    "# select a strategy\n",
    "resampling_strategy = \"holdout\"\n",
    "\n",
    "# -------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create and train an ensemble with AutoML\n",
    "# Auto-sklearn will ingest the pandas dataframe and detects column types\n",
    "askl2 = AutoSklearn2Classifier(**settings, resampling_strategy=resampling_strategy)\n",
    "askl2.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "leaderboard = askl2.leaderboard(sort_by=\"model_id\", ensemble_only=True)\n",
    "print(leaderboard)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calucalte balanced accuracy on validation data\n",
    "predictions = askl2.predict(X_test)\n",
    "balanced_accuracy_score(y_test, predictions)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
