{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.feature_selection import VarianceThreshold\n",
    "from sklearn.metrics import mutual_info_score\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "####################################################################\n",
    "#                          Read data                               #\n",
    "####################################################################\n",
    "\n",
    "prefix = \"\"\n",
    "\n",
    "_test_x = pd.read_table(prefix + \"artificial_test.data\", sep=\" \", header=None)\n",
    "_test_x.drop(_test_x.columns[500], axis=1, inplace=True)\n",
    "_train_y = pd.read_table(prefix + \"artificial_train.labels\", header=None)\n",
    "_train_x = pd.read_table(prefix + \"artificial_train.data\", sep=\" \", header=None)\n",
    "_train_x.drop(_train_x.columns[500], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_train_and_validation_data():\n",
    "    split = 400\n",
    "    train_x, valid_x = _train_x[split:], _train_x[:split]\n",
    "    train_y, valid_y = _train_y[split:], _train_y[:split]\n",
    "    print(\"train_x.shape: \", train_x.shape)\n",
    "    print(\"train_y.shape: \", train_y.shape)\n",
    "    print(\"valid_x.shape: \", valid_x.shape)\n",
    "    print(\"valid_y.shape: \", valid_y.shape)\n",
    "    return train_x, train_y, valid_x, valid_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_x, train_y, valid_x, valid_y = get_train_and_validation_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Remove Highly Correlated Columns\n",
    "def remove_highly_correlated_features(train_x, valid_x, threshold=0.95):\n",
    "    corr_matrix = train_x.corr().abs()\n",
    "    # Select upper triangle of correlation matrix\n",
    "    upper = corr_matrix.where(np.triu(np.ones(corr_matrix.shape), k=1).astype(bool))\n",
    "    # Find index of feature columns with correlation greater than threshold\n",
    "    to_drop = [column for column in upper.columns if any(upper[column] > threshold)]\n",
    "    train_x = train_x.drop(to_drop, axis=1)\n",
    "    valid_x = valid_x.drop(to_drop, axis=1)\n",
    "    return train_x, valid_x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_x, valid_x = remove_highly_correlated_features(train_x, valid_x)\n",
    "train_x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. Remove Low Variance Columns\n",
    "def remove_low_variance_features(train_x, valid_x, threshold=(0.8 * (1 - 0.8))):\n",
    "    sel = VarianceThreshold(threshold=threshold)\n",
    "    sel.fit(train_x)\n",
    "    train_x = train_x[train_x.columns[sel.get_support(indices=True)]]\n",
    "    valid_x = valid_x[valid_x.columns[sel.get_support(indices=True)]]\n",
    "    return train_x, valid_x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_x, valid_x = remove_low_variance_features(train_x, valid_x)\n",
    "train_x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. Remove Random Columns (Optional)\n",
    "# This step is an approximation and should be tailored to your specific needs\n",
    "# Here we use a Decision Tree to estimate feature importance\n",
    "def remove_random_features(\n",
    "    train_x: pd.DataFrame,\n",
    "    train_y: pd.DataFrame,\n",
    "    valid_x: pd.DataFrame,\n",
    "    importance=0.005,\n",
    "):\n",
    "    tree: DecisionTreeClassifier = DecisionTreeClassifier(random_state=0)\n",
    "    tree.fit(train_x, train_y)\n",
    "    importances = tree.feature_importances_\n",
    "\n",
    "    # Assume columns with very low importance are \"random\"\n",
    "    # This threshold can be adjusted based on domain knowledge\n",
    "    important_indices = [i for i, imp in enumerate(importances) if imp > importance]\n",
    "    train_x = train_x.iloc[:, important_indices]\n",
    "    valid_x = valid_x.iloc[:, important_indices]\n",
    "    return train_x, valid_x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_x, valid_x = remove_random_features(\n",
    "    train_x=train_x, train_y=train_y, valid_x=valid_x\n",
    ")\n",
    "train_x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.feature_selection import SelectKBest, f_classif\n",
    "\n",
    "\n",
    "def anova_filter(\n",
    "    train_x: pd.DataFrame,\n",
    "    train_y: pd.DataFrame,\n",
    "    valid_x: pd.DataFrame,\n",
    "    k: int = 50,\n",
    "):\n",
    "    # Using ANOVA F-test to select features\n",
    "    selector = SelectKBest(\n",
    "        f_classif, k=k\n",
    "    )  # Change k to select the number of features you want\n",
    "    selector.fit(train_x, train_y)\n",
    "\n",
    "    # Get F-values and p-values for each feature\n",
    "    f_values = selector.scores_\n",
    "    p_values = selector.pvalues_\n",
    "\n",
    "    # Selecting features (you can use a threshold or select top k features)\n",
    "    selected_features = train_x.columns[selector.get_support()]\n",
    "\n",
    "    # Transforming train_x to include only the selected features\n",
    "    train_x = selector.transform(train_x)\n",
    "    valid_x = selector.transform(valid_x)\n",
    "    return train_x, valid_x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_x, valid_x = anove_filter(train_x=train_x, train_y=train_y, valid_x=valid_x)\n",
    "# train_x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "label = \"class\"\n",
    "train_y = train_y.rename(columns={0: label})\n",
    "valid_y = valid_y.rename(columns={0: label})\n",
    "train_data = pd.concat([train_x, train_y[label]], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sanity check\n",
    "(\n",
    "    original_train_x,\n",
    "    original_train_y,\n",
    "    original_valid_x,\n",
    "    original_valid_y,\n",
    ") = get_train_and_validation_data()\n",
    "for y, original_y in zip([train_y, valid_y], [original_train_y, original_valid_y]):\n",
    "    assert y.shape == original_y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from autogluon.tabular import TabularPredictor\n",
    "\n",
    "# save_path = \"some_path\"\n",
    "# # train for 3 minutes with increased num_boost_round\n",
    "# predictor = TabularPredictor(\n",
    "#     label=label, path=save_path, eval_metric=\"balanced_accuracy\",  problem_type=\"binary\"\n",
    "# ).fit(train_data, time_limit=60 * 10,presets = \"best_quality\", hyperparameters =\"default\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# predictor.leaderboard()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # check on validation data\n",
    "# print(valid_x.shape)\n",
    "# print(valid_y.shape)\n",
    "# valid_data = pd.concat([valid_x, valid_y[label]], axis=1)\n",
    "# predictor.evaluate(valid_data)\n",
    "# # best model WeightedEnsamble_L2 score 0.86"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install mljar-supervised"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from supervised.automl import AutoML  # mljar-supervised\n",
    "\n",
    "# train models with AutoML\n",
    "automl = AutoML(\n",
    "    mode=\"Compete\", ml_task=\"binary_classification\", total_time_limit=60 * 10, eval_metric=\"f1\"\n",
    ")\n",
    "automl.fit(train_x, train_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import balanced_accuracy_score\n",
    "\n",
    "predictions = automl.predict(valid_x)\n",
    "balanced_accuracy_score(valid_y, predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install auto-sklearn\n",
    "# !pip install ydata-profiling\n",
    "from autosklearn.classification import AutoSklearnClassifier\n",
    "from autosklearn.experimental.askl2 import AutoSklearn2Classifier\n",
    "from autosklearn.metrics import balanced_accuracy "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Edit the settings to try in both AutoSklearn1 and AutoSklearn2\n",
    "# Possibilities https://automl.github.io/auto-sklearn/master/api.html\n",
    "\n",
    "#------------------------- edit code here\n",
    "settings = {\n",
    "  \"time_left_for_this_task\": 120,  # seconds\n",
    "  \"seed\": 42,\n",
    "  \"metric\": balanced_accuracy,\n",
    "  \"n_jobs\": 4,\n",
    "}\n",
    "\n",
    "# This will only be used by autosklearn 1 while autosklearn 2 will automatically\n",
    "# select a strategy\n",
    "resampling_strategy = \"holdout\"\n",
    "\n",
    "#-------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create and train an ensemble with AutoML\n",
    "# Auto-sklearn will ingest the pandas dataframe and detects column types\n",
    "askl2 = AutoSklearn2Classifier(\n",
    "    **settings,\n",
    "    resampling_strategy=resampling_strategy\n",
    ")\n",
    "askl2.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "leaderboard = askl2.leaderboard(sort_by=\"model_id\", ensemble_only=True)\n",
    "print(leaderboard)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calucalte balanced accuracy on validation data\n",
    "predictions = askl2.predict(X_test)\n",
    "balanced_accuracy_score(y_test, predictions)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
