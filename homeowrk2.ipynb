{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install mljar-supervised"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "from os import path\n",
    "\n",
    "import joblib\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from autogluon.tabular import TabularPredictor\n",
    "from catboost import CatBoostClassifier\n",
    "from scipy.stats import randint, uniform\n",
    "from sklearn.ensemble import (\n",
    "    ExtraTreesClassifier,\n",
    "    GradientBoostingClassifier,\n",
    "    RandomForestClassifier,\n",
    "    StackingClassifier,\n",
    "    VotingClassifier,\n",
    ")\n",
    "from sklearn.feature_selection import (\n",
    "    RFE,\n",
    "    RFECV,\n",
    "    SelectKBest,\n",
    "    VarianceThreshold,\n",
    "    f_classif,\n",
    ")\n",
    "from sklearn.linear_model import ElasticNet, LogisticRegression\n",
    "from sklearn.metrics import balanced_accuracy_score, mutual_info_score\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.utils import shuffle\n",
    "from supervised.automl import AutoML\n",
    "from xgboost import XGBClassifier\n",
    "\n",
    "# from supervised.automl import AutoML  # mljar-supervised"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Constants\n",
    "\n",
    "SEED = 42\n",
    "N_JOBS = -1\n",
    "RANDOM_SEARCH_N_ITER = 1\n",
    "TRAIN_TIME_LIMIT_AUTOGLUON = 60 * 1 * 1\n",
    "TRAIN_TIME_LIMIT_MLJAR = 60 * 1 * 1\n",
    "OUTPUT_DIR_MANUAL = path.join(\"output\", \"manual\")\n",
    "OUTPUT_DIR_AUTOGLUON = path.join(\"output\", \"autogluon\")\n",
    "OUTPUT_DIR_MLJAR = path.join(\"output\", \"mljar\")\n",
    "UNIQUE_ID = time.strftime(\"%Y%m%d_%H%M%S\")\n",
    "APPLY_REMOVE_LOW_VARIANCE_FEATURES = False\n",
    "APPLY_REMOVE_CORRELATED_FEATURES = False\n",
    "APPLY_REMOVE_RANDOM_FEATURES = False\n",
    "APPLY_ANOVA = False\n",
    "ANOVE_FEATURES = 25"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating output directory output\\manual\\20240114_144651\n",
      "Creating output directory output\\autogluon\\20240114_144651\n",
      "Creating output directory output\\mljar\\20240114_144651\n",
      "Creating output directory output\\auto_sklearn\\20240114_144651\n"
     ]
    }
   ],
   "source": [
    "# prepare output directories\n",
    "for output_dir in [OUTPUT_DIR_MANUAL, OUTPUT_DIR_AUTOGLUON, OUTPUT_DIR_MLJAR]:\n",
    "    if not path.exists(path.join(output_dir, UNIQUE_ID)):\n",
    "        print(f\"Creating output directory {path.join(output_dir, UNIQUE_ID)}\")\n",
    "        os.makedirs(path.join(output_dir, UNIQUE_ID))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_highly_correlated_features(train_x, valid_x, test_x, threshold=0.95):\n",
    "    # Calculate correlation matrix\n",
    "    corr_matrix = np.corrcoef(train_x, rowvar=False)\n",
    "    # Select upper triangle of correlation matrix\n",
    "    upper = np.triu(corr_matrix, k=1)\n",
    "    # Find indices of feature columns with correlation greater than threshold\n",
    "    to_drop = [i for i in range(upper.shape[1]) if any(upper[:, i] > threshold)]\n",
    "\n",
    "    # Drop features from train, validation, and test set\n",
    "    train_x = np.delete(train_x, to_drop, axis=1)\n",
    "    valid_x = np.delete(valid_x, to_drop, axis=1)\n",
    "    test_x = np.delete(test_x, to_drop, axis=1)\n",
    "\n",
    "    return train_x, valid_x, test_x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove Low Variance Columns\n",
    "def remove_low_variance_features(train_x, valid_x, test_x, threshold=(0.8 * (1 - 0.8))):\n",
    "    sel = VarianceThreshold(threshold=threshold)\n",
    "    sel.fit(train_x)\n",
    "    train_x = train_x[:, sel.get_support(indices=True)]\n",
    "    valid_x = valid_x[:, sel.get_support(indices=True)]\n",
    "    test_x = test_x[:, sel.get_support(indices=True)]\n",
    "    return train_x, valid_x, test_x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove Random Columns\n",
    "def remove_random_features(\n",
    "    train_x: np.ndarray,\n",
    "    train_y: np.ndarray,\n",
    "    valid_x: np.ndarray,\n",
    "    test_x: np.ndarray,\n",
    "    importance=0.005,\n",
    "):\n",
    "    tree: DecisionTreeClassifier = DecisionTreeClassifier(random_state=0)\n",
    "    tree.fit(train_x, train_y)\n",
    "    importances = tree.feature_importances_\n",
    "\n",
    "    # Assume columns with very low importance are \"random\"\n",
    "    # This threshold can be adjusted based on domain knowledge\n",
    "    important_indices = [i for i, imp in enumerate(importances) if imp > importance]\n",
    "    train_x = train_x[:, important_indices]\n",
    "    valid_x = valid_x[:, important_indices]\n",
    "    test_x = test_x[:, important_indices]\n",
    "    return train_x, valid_x, test_x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def anova_filter(\n",
    "    train_x: np.ndarray,\n",
    "    train_y: np.ndarray,\n",
    "    valid_x: np.ndarray,\n",
    "    test_x: np.ndarray,\n",
    "    k: int = 50,\n",
    "):\n",
    "    selector = SelectKBest(f_classif, k=k)\n",
    "    selector.fit(train_x, train_y)\n",
    "\n",
    "    train_x = selector.transform(train_x)\n",
    "    valid_x = selector.transform(valid_x)\n",
    "    test_x = selector.transform(test_x)\n",
    "    return train_x, valid_x, test_x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dump_proba(model, test_x, output_path_proba):\n",
    "    proba = model.predict_proba(test_x)\n",
    "\n",
    "    if isinstance(proba, pd.DataFrame):\n",
    "        proba = proba.values\n",
    "\n",
    "    np.savetxt(\n",
    "        output_path_proba,\n",
    "        proba[:, 1],\n",
    "        delimiter=\"\\n\",\n",
    "        header='\"313201_313212\"',\n",
    "        comments=\"\",\n",
    "        # fmt=\"%.19f\",\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dump_model(model, output_path_model):\n",
    "    joblib.dump(model, output_path_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import ExtraTreesClassifier, RandomForestClassifier\n",
    "from sklearn.feature_selection import RFE, RFECV\n",
    "\n",
    "\n",
    "def perform_feature_selection(\n",
    "    train_x, train_y, valid_x, test_x, n_features_to_select=None\n",
    "):\n",
    "    estimator_et = ExtraTreesClassifier()\n",
    "    rfe_et = RFE(estimator=estimator_et, n_features_to_select=250)\n",
    "    rfe_et.fit(train_x, train_y)\n",
    "    train_x = train_x[:, rfe_et.support_]\n",
    "    valid_x = valid_x[:, rfe_et.support_]\n",
    "    test_x = test_x[:, rfe_et.support_]\n",
    "    print(train_x.shape, valid_x.shape, test_x.shape)\n",
    "\n",
    "    estimator_rf = RandomForestClassifier()\n",
    "    rfe_rf = RFE(estimator=estimator_rf, n_features_to_select=125)\n",
    "    rfe_rf.fit(train_x, train_y)\n",
    "    train_x = train_x[:, rfe_rf.support_]\n",
    "    valid_x = valid_x[:, rfe_rf.support_]\n",
    "    test_x = test_x[:, rfe_rf.support_]\n",
    "    print(train_x.shape, valid_x.shape, test_x.shape)\n",
    "\n",
    "    rfecv_et = RFECV(estimator=estimator_et, cv=3, min_features_to_select=25)\n",
    "    rfecv_et.fit(train_x, train_y)\n",
    "    train_x = train_x[:, rfecv_et.support_]\n",
    "    valid_x = valid_x[:, rfecv_et.support_]\n",
    "    test_x = test_x[:, rfecv_et.support_]\n",
    "    print(train_x.shape, valid_x.shape, test_x.shape)\n",
    "\n",
    "    rfecv_rf = RFECV(estimator=estimator_rf, cv=3, min_features_to_select=15)\n",
    "    rfecv_rf.fit(train_x, train_y)\n",
    "    train_x = train_x[:, rfecv_rf.support_]\n",
    "    valid_x = valid_x[:, rfecv_rf.support_]\n",
    "    test_x = test_x[:, rfecv_rf.support_]\n",
    "    print(train_x.shape, valid_x.shape, test_x.shape)\n",
    "\n",
    "    return train_x, valid_x, test_x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "prefix = \"\"\n",
    "\n",
    "_test_x = pd.read_table(prefix + \"artificial_test.data\", sep=\" \", header=None)\n",
    "_test_x.drop(_test_x.columns[500], axis=1, inplace=True)\n",
    "_train_y = pd.read_table(prefix + \"artificial_train.labels\", header=None)\n",
    "_train_x = pd.read_table(prefix + \"artificial_train.data\", sep=\" \", header=None)\n",
    "_train_x.drop(_train_x.columns[500], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "_test_x = np.array(_test_x, dtype=float, copy=True)\n",
    "_train_x = np.array(_train_x, dtype=float, copy=True)\n",
    "_train_y = np.array(_train_y, dtype=float, copy=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "_train_x, _train_y = shuffle(_train_x, _train_y, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_train_and_validation_data():\n",
    "    split = 400\n",
    "    train_x, valid_x = _train_x[split:].copy(), _train_x[:split].copy()\n",
    "    train_y, valid_y = _train_y[split:].copy(), _train_y[:split].copy()\n",
    "    return train_x, train_y, valid_x, valid_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1600, 500) (1600, 1) (400, 500) (400, 1)\n"
     ]
    }
   ],
   "source": [
    "train_x, train_y, valid_x, valid_y = get_train_and_validation_data()\n",
    "print(train_x.shape, train_y.shape, valid_x.shape, valid_y.shape)\n",
    "print(train_x.head)\n",
    "print(train_y.head)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1600, 250) (400, 250) (600, 250)\n",
      "(1600, 125) (400, 125) (600, 125)\n",
      "(1600, 25) (400, 25) (600, 25)\n",
      "(1600, 20) (400, 20) (600, 20)\n",
      "(1600, 20) (400, 20) (600, 20)\n"
     ]
    }
   ],
   "source": [
    "train_x, valid_x, test_x = perform_feature_selection(\n",
    "    train_x, train_y.copy().ravel(), valid_x, _test_x, n_features_to_select=None\n",
    ")\n",
    "print(train_x.shape, valid_x.shape, test_x.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "if APPLY_REMOVE_CORRELATED_FEATURES:\n",
    "    train_x, valid_x, test_x = remove_highly_correlated_features(\n",
    "        train_x, valid_x, _test_x\n",
    "    )\n",
    "    print(\"train_x.shape: \", train_x.shape)\n",
    "    print(\"valid_x.shape: \", valid_x.shape)\n",
    "    print(\"test_x.shape: \", test_x.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "if APPLY_REMOVE_LOW_VARIANCE_FEATURES:\n",
    "    train_x, valid_x, test_x = remove_low_variance_features(train_x, valid_x, test_x)\n",
    "    print(\"train_x.shape: \", train_x.shape)\n",
    "    print(\"valid_x.shape: \", valid_x.shape)\n",
    "    print(\"test_x.shape: \", test_x.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "if APPLY_REMOVE_RANDOM_FEATURES:\n",
    "    train_x, valid_x, test_x = remove_random_features(\n",
    "        train_x=train_x, train_y=train_y, valid_x=valid_x, test_x=test_x\n",
    "    )\n",
    "    print(\"train_x.shape: \", train_x.shape)\n",
    "    print(\"valid_x.shape: \", valid_x.shape)\n",
    "    print(\"test_x.shape: \", test_x.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "if APPLY_ANOVA:\n",
    "    train_x, valid_x, test_x = anova_filter(\n",
    "        train_x=train_x,\n",
    "        train_y=train_y,\n",
    "        valid_x=valid_x,\n",
    "        test_x=test_x,\n",
    "        k=ANOVE_FEATURES,\n",
    "    )\n",
    "    print(\"train_x.shape: \", train_x.shape)\n",
    "    print(\"valid_x.shape: \", valid_x.shape)\n",
    "    print(\"test_x.shape: \", test_x.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_x.shape:  (1600, 20)\n",
      "train_y.shape:  (1600, 1)\n",
      "valid_x.shape:  (400, 20)\n",
      "valid_y.shape:  (400, 1)\n",
      "test_x.shape:  (600, 20)\n"
     ]
    }
   ],
   "source": [
    "print(\"train_x.shape: \", train_x.shape)\n",
    "print(\"train_y.shape: \", train_y.shape)\n",
    "print(\"valid_x.shape: \", valid_x.shape)\n",
    "print(\"valid_y.shape: \", valid_y.shape)\n",
    "print(\"test_x.shape: \", test_x.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ok\n",
      "ok\n",
      "ok\n",
      "ok\n",
      "ok\n"
     ]
    }
   ],
   "source": [
    "if isinstance(train_x, np.ndarray):\n",
    "    print(\"ok\")\n",
    "if isinstance(train_y, np.ndarray):\n",
    "    print(\"ok\")\n",
    "if isinstance(valid_x, np.ndarray):\n",
    "    print(\"ok\")\n",
    "if isinstance(valid_y, np.ndarray):\n",
    "    print(\"ok\")\n",
    "if isinstance(test_x, np.ndarray):\n",
    "    print(\"ok\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### manual model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0:\tlearn: 0.7775000\ttotal: 122ms\tremaining: 42.6s\n",
      "100:\tlearn: 0.9162500\ttotal: 294ms\tremaining: 724ms\n",
      "200:\tlearn: 0.9418750\ttotal: 475ms\tremaining: 352ms\n",
      "300:\tlearn: 0.9631250\ttotal: 648ms\tremaining: 105ms\n",
      "349:\tlearn: 0.9706250\ttotal: 728ms\tremaining: 0us\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-1 {color: black;}#sk-container-id-1 pre{padding: 0;}#sk-container-id-1 div.sk-toggleable {background-color: white;}#sk-container-id-1 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-1 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-1 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-1 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-1 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-1 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-1 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-1 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-1 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-1 div.sk-item {position: relative;z-index: 1;}#sk-container-id-1 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-1 div.sk-item::before, #sk-container-id-1 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-1 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-1 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-1 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-1 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-1 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-1 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-1 div.sk-label-container {text-align: center;}#sk-container-id-1 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-1 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-1\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>VotingClassifier(estimators=[(&#x27;stacked_ensemble_1&#x27;,\n",
       "                              StackingClassifier(cv=5,\n",
       "                                                 estimators=[(&#x27;mlp&#x27;,\n",
       "                                                              Pipeline(steps=[(&#x27;standardscaler&#x27;,\n",
       "                                                                               StandardScaler()),\n",
       "                                                                              (&#x27;mlpclassifier&#x27;,\n",
       "                                                                               MLPClassifier(alpha=0.001,\n",
       "                                                                                             early_stopping=True,\n",
       "                                                                                             hidden_layer_sizes=(100,\n",
       "                                                                                                                 300,\n",
       "                                                                                                                 200,\n",
       "                                                                                                                 100),\n",
       "                                                                                             max_iter=1000,\n",
       "                                                                                             random_state=42,\n",
       "                                                                                             solver=&#x27;lbfgs&#x27;,\n",
       "                                                                                             tol=0.001))])),\n",
       "                                                             (&#x27;rf&#x27;,\n",
       "                                                              Pipeline(steps=[(&#x27;standardscaler&#x27;,\n",
       "                                                                               Stan...\n",
       "                                                             grow_policy=None,\n",
       "                                                             importance_type=None,\n",
       "                                                             interaction_constraints=None,\n",
       "                                                             learning_rate=0.02,\n",
       "                                                             max_bin=None,\n",
       "                                                             max_cat_threshold=None,\n",
       "                                                             max_cat_to_onehot=None,\n",
       "                                                             max_delta_step=None,\n",
       "                                                             max_depth=6,\n",
       "                                                             max_leaves=None,\n",
       "                                                             min_child_weight=1,\n",
       "                                                             missing=nan,\n",
       "                                                             monotone_constraints=None,\n",
       "                                                             multi_strategy=None,\n",
       "                                                             n_estimators=350,\n",
       "                                                             n_jobs=None,\n",
       "                                                             num_parallel_tree=None,\n",
       "                                                             random_state=42, ...))]))],\n",
       "                 voting=&#x27;soft&#x27;)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item sk-dashed-wrapped\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-1\" type=\"checkbox\" ><label for=\"sk-estimator-id-1\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">VotingClassifier</label><div class=\"sk-toggleable__content\"><pre>VotingClassifier(estimators=[(&#x27;stacked_ensemble_1&#x27;,\n",
       "                              StackingClassifier(cv=5,\n",
       "                                                 estimators=[(&#x27;mlp&#x27;,\n",
       "                                                              Pipeline(steps=[(&#x27;standardscaler&#x27;,\n",
       "                                                                               StandardScaler()),\n",
       "                                                                              (&#x27;mlpclassifier&#x27;,\n",
       "                                                                               MLPClassifier(alpha=0.001,\n",
       "                                                                                             early_stopping=True,\n",
       "                                                                                             hidden_layer_sizes=(100,\n",
       "                                                                                                                 300,\n",
       "                                                                                                                 200,\n",
       "                                                                                                                 100),\n",
       "                                                                                             max_iter=1000,\n",
       "                                                                                             random_state=42,\n",
       "                                                                                             solver=&#x27;lbfgs&#x27;,\n",
       "                                                                                             tol=0.001))])),\n",
       "                                                             (&#x27;rf&#x27;,\n",
       "                                                              Pipeline(steps=[(&#x27;standardscaler&#x27;,\n",
       "                                                                               Stan...\n",
       "                                                             grow_policy=None,\n",
       "                                                             importance_type=None,\n",
       "                                                             interaction_constraints=None,\n",
       "                                                             learning_rate=0.02,\n",
       "                                                             max_bin=None,\n",
       "                                                             max_cat_threshold=None,\n",
       "                                                             max_cat_to_onehot=None,\n",
       "                                                             max_delta_step=None,\n",
       "                                                             max_depth=6,\n",
       "                                                             max_leaves=None,\n",
       "                                                             min_child_weight=1,\n",
       "                                                             missing=nan,\n",
       "                                                             monotone_constraints=None,\n",
       "                                                             multi_strategy=None,\n",
       "                                                             n_estimators=350,\n",
       "                                                             n_jobs=None,\n",
       "                                                             num_parallel_tree=None,\n",
       "                                                             random_state=42, ...))]))],\n",
       "                 voting=&#x27;soft&#x27;)</pre></div></div></div><div class=\"sk-parallel\"><div class=\"sk-parallel-item\"><div class=\"sk-item\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><label>stacked_ensemble_1</label></div></div><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-parallel\"><div class=\"sk-parallel-item\"><div class=\"sk-item\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><label>mlp</label></div></div><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-2\" type=\"checkbox\" ><label for=\"sk-estimator-id-2\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">StandardScaler</label><div class=\"sk-toggleable__content\"><pre>StandardScaler()</pre></div></div></div><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-3\" type=\"checkbox\" ><label for=\"sk-estimator-id-3\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">MLPClassifier</label><div class=\"sk-toggleable__content\"><pre>MLPClassifier(alpha=0.001, early_stopping=True,\n",
       "              hidden_layer_sizes=(100, 300, 200, 100), max_iter=1000,\n",
       "              random_state=42, solver=&#x27;lbfgs&#x27;, tol=0.001)</pre></div></div></div></div></div></div></div></div><div class=\"sk-parallel-item\"><div class=\"sk-item\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><label>rf</label></div></div><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-4\" type=\"checkbox\" ><label for=\"sk-estimator-id-4\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">StandardScaler</label><div class=\"sk-toggleable__content\"><pre>StandardScaler()</pre></div></div></div><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-5\" type=\"checkbox\" ><label for=\"sk-estimator-id-5\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">RandomForestClassifier</label><div class=\"sk-toggleable__content\"><pre>RandomForestClassifier(max_depth=30, min_samples_leaf=4, n_estimators=350,\n",
       "                       random_state=42)</pre></div></div></div></div></div></div></div></div></div></div><div class=\"sk-item\"><div class=\"sk-parallel\"><div class=\"sk-parallel-item\"><div class=\"sk-item\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><label>final_estimator</label></div></div><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-6\" type=\"checkbox\" ><label for=\"sk-estimator-id-6\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">LogisticRegression</label><div class=\"sk-toggleable__content\"><pre>LogisticRegression()</pre></div></div></div></div></div></div></div></div></div></div></div></div></div><div class=\"sk-parallel-item\"><div class=\"sk-item\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><label>stacked_ensemble_2</label></div></div><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-parallel\"><div class=\"sk-parallel-item\"><div class=\"sk-item\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><label>mlp</label></div></div><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-7\" type=\"checkbox\" ><label for=\"sk-estimator-id-7\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">StandardScaler</label><div class=\"sk-toggleable__content\"><pre>StandardScaler()</pre></div></div></div><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-8\" type=\"checkbox\" ><label for=\"sk-estimator-id-8\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">MLPClassifier</label><div class=\"sk-toggleable__content\"><pre>MLPClassifier(alpha=0.001, early_stopping=True,\n",
       "              hidden_layer_sizes=(100, 300, 200, 100), max_iter=1000,\n",
       "              random_state=42, solver=&#x27;lbfgs&#x27;, tol=0.001)</pre></div></div></div></div></div></div></div></div><div class=\"sk-parallel-item\"><div class=\"sk-item\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><label>gbc</label></div></div><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-9\" type=\"checkbox\" ><label for=\"sk-estimator-id-9\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">StandardScaler</label><div class=\"sk-toggleable__content\"><pre>StandardScaler()</pre></div></div></div><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-10\" type=\"checkbox\" ><label for=\"sk-estimator-id-10\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">GradientBoostingClassifier</label><div class=\"sk-toggleable__content\"><pre>GradientBoostingClassifier(max_depth=30, min_samples_leaf=2,\n",
       "                           min_samples_split=5, n_estimators=350,\n",
       "                           random_state=42)</pre></div></div></div></div></div></div></div></div></div></div><div class=\"sk-item\"><div class=\"sk-parallel\"><div class=\"sk-parallel-item\"><div class=\"sk-item\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><label>final_estimator</label></div></div><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-11\" type=\"checkbox\" ><label for=\"sk-estimator-id-11\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">LogisticRegression</label><div class=\"sk-toggleable__content\"><pre>LogisticRegression()</pre></div></div></div></div></div></div></div></div></div></div></div></div></div><div class=\"sk-parallel-item\"><div class=\"sk-item\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><label>gbc</label></div></div><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-12\" type=\"checkbox\" ><label for=\"sk-estimator-id-12\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">StandardScaler</label><div class=\"sk-toggleable__content\"><pre>StandardScaler()</pre></div></div></div><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-13\" type=\"checkbox\" ><label for=\"sk-estimator-id-13\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">GradientBoostingClassifier</label><div class=\"sk-toggleable__content\"><pre>GradientBoostingClassifier(max_depth=30, min_samples_leaf=2,\n",
       "                           min_samples_split=5, n_estimators=350,\n",
       "                           random_state=42)</pre></div></div></div></div></div></div></div></div><div class=\"sk-parallel-item\"><div class=\"sk-item\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><label>rf</label></div></div><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-14\" type=\"checkbox\" ><label for=\"sk-estimator-id-14\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">StandardScaler</label><div class=\"sk-toggleable__content\"><pre>StandardScaler()</pre></div></div></div><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-15\" type=\"checkbox\" ><label for=\"sk-estimator-id-15\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">RandomForestClassifier</label><div class=\"sk-toggleable__content\"><pre>RandomForestClassifier(max_depth=30, min_samples_leaf=4, n_estimators=350,\n",
       "                       random_state=42)</pre></div></div></div></div></div></div></div></div><div class=\"sk-parallel-item\"><div class=\"sk-item\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><label>mlp</label></div></div><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-16\" type=\"checkbox\" ><label for=\"sk-estimator-id-16\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">StandardScaler</label><div class=\"sk-toggleable__content\"><pre>StandardScaler()</pre></div></div></div><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-17\" type=\"checkbox\" ><label for=\"sk-estimator-id-17\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">MLPClassifier</label><div class=\"sk-toggleable__content\"><pre>MLPClassifier(alpha=0.001, early_stopping=True,\n",
       "              hidden_layer_sizes=(100, 300, 200, 100), max_iter=1000,\n",
       "              random_state=42, solver=&#x27;lbfgs&#x27;, tol=0.001)</pre></div></div></div></div></div></div></div></div><div class=\"sk-parallel-item\"><div class=\"sk-item\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><label>cb</label></div></div><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-18\" type=\"checkbox\" ><label for=\"sk-estimator-id-18\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">StandardScaler</label><div class=\"sk-toggleable__content\"><pre>StandardScaler()</pre></div></div></div><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-19\" type=\"checkbox\" ><label for=\"sk-estimator-id-19\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">CatBoostClassifier</label><div class=\"sk-toggleable__content\"><pre>&lt;catboost.core.CatBoostClassifier object at 0x000001C9CD517550&gt;</pre></div></div></div></div></div></div></div></div><div class=\"sk-parallel-item\"><div class=\"sk-item\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><label>xgb</label></div></div><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-20\" type=\"checkbox\" ><label for=\"sk-estimator-id-20\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">StandardScaler</label><div class=\"sk-toggleable__content\"><pre>StandardScaler()</pre></div></div></div><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-21\" type=\"checkbox\" ><label for=\"sk-estimator-id-21\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">XGBClassifier</label><div class=\"sk-toggleable__content\"><pre>XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
       "              colsample_bylevel=None, colsample_bynode=None,\n",
       "              colsample_bytree=0.8, device=None, early_stopping_rounds=None,\n",
       "              enable_categorical=False,\n",
       "              eval_metric=&lt;function balanced_accuracy_score at 0x000001C9BD6AD1F0&gt;,\n",
       "              feature_types=None, gamma=0, grow_policy=None,\n",
       "              importance_type=None, interaction_constraints=None,\n",
       "              learning_rate=0.02, max_bin=None, max_cat_threshold=None,\n",
       "              max_cat_to_onehot=None, max_delta_step=None, max_depth=6,\n",
       "              max_leaves=None, min_child_weight=1, missing=nan,\n",
       "              monotone_constraints=None, multi_strategy=None, n_estimators=350,\n",
       "              n_jobs=None, num_parallel_tree=None, random_state=42, ...)</pre></div></div></div></div></div></div></div></div></div></div></div></div>"
      ],
      "text/plain": [
       "VotingClassifier(estimators=[('stacked_ensemble_1',\n",
       "                              StackingClassifier(cv=5,\n",
       "                                                 estimators=[('mlp',\n",
       "                                                              Pipeline(steps=[('standardscaler',\n",
       "                                                                               StandardScaler()),\n",
       "                                                                              ('mlpclassifier',\n",
       "                                                                               MLPClassifier(alpha=0.001,\n",
       "                                                                                             early_stopping=True,\n",
       "                                                                                             hidden_layer_sizes=(100,\n",
       "                                                                                                                 300,\n",
       "                                                                                                                 200,\n",
       "                                                                                                                 100),\n",
       "                                                                                             max_iter=1000,\n",
       "                                                                                             random_state=42,\n",
       "                                                                                             solver='lbfgs',\n",
       "                                                                                             tol=0.001))])),\n",
       "                                                             ('rf',\n",
       "                                                              Pipeline(steps=[('standardscaler',\n",
       "                                                                               Stan...\n",
       "                                                             grow_policy=None,\n",
       "                                                             importance_type=None,\n",
       "                                                             interaction_constraints=None,\n",
       "                                                             learning_rate=0.02,\n",
       "                                                             max_bin=None,\n",
       "                                                             max_cat_threshold=None,\n",
       "                                                             max_cat_to_onehot=None,\n",
       "                                                             max_delta_step=None,\n",
       "                                                             max_depth=6,\n",
       "                                                             max_leaves=None,\n",
       "                                                             min_child_weight=1,\n",
       "                                                             missing=nan,\n",
       "                                                             monotone_constraints=None,\n",
       "                                                             multi_strategy=None,\n",
       "                                                             n_estimators=350,\n",
       "                                                             n_jobs=None,\n",
       "                                                             num_parallel_tree=None,\n",
       "                                                             random_state=42, ...))]))],\n",
       "                 voting='soft')"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.ensemble import GradientBoostingClassifier, VotingClassifier\n",
    "from sklearn.metrics import balanced_accuracy_score\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "\n",
    "base_classifiers_1 = [\n",
    "    (\n",
    "        \"mlp\",\n",
    "        make_pipeline(\n",
    "            StandardScaler(),\n",
    "            MLPClassifier(\n",
    "                random_state=SEED,\n",
    "                max_iter=1000,\n",
    "                early_stopping=True,\n",
    "                tol=1e-3,\n",
    "                solver=\"lbfgs\",\n",
    "                hidden_layer_sizes=(100, 300, 200, 100),\n",
    "                alpha=0.001,\n",
    "            ),\n",
    "        ),\n",
    "    ),\n",
    "    (\n",
    "        \"rf\",\n",
    "        make_pipeline(\n",
    "            StandardScaler(),\n",
    "            RandomForestClassifier(\n",
    "                random_state=SEED,\n",
    "                n_estimators=500,\n",
    "                max_depth=30,\n",
    "                min_samples_leaf=4,\n",
    "                min_samples_split=2,\n",
    "            ),\n",
    "        ),\n",
    "    ),\n",
    "]\n",
    "\n",
    "base_classifiers_2 = [\n",
    "    (\n",
    "        \"mlp\",\n",
    "        make_pipeline(\n",
    "            StandardScaler(),\n",
    "            MLPClassifier(\n",
    "                random_state=SEED,\n",
    "                max_iter=1000,\n",
    "                early_stopping=True,\n",
    "                tol=1e-3,\n",
    "                solver=\"lbfgs\",\n",
    "                hidden_layer_sizes=(100, 300, 200, 100),\n",
    "                alpha=0.001,\n",
    "            ),\n",
    "        ),\n",
    "    ),\n",
    "    (\n",
    "        \"gbc\",\n",
    "        make_pipeline(\n",
    "            StandardScaler(),\n",
    "            GradientBoostingClassifier(\n",
    "                random_state=SEED,\n",
    "                max_features=None,\n",
    "                n_estimators=500,\n",
    "                max_depth=30,\n",
    "                min_samples_leaf=2,\n",
    "                min_samples_split=5,\n",
    "            ),\n",
    "        ),\n",
    "    ),\n",
    "]\n",
    "\n",
    "stacked_ensamble_1 = StackingClassifier(\n",
    "    estimators=base_classifiers_1, final_estimator=LogisticRegression(), cv=5\n",
    ")\n",
    "\n",
    "stacked_ensamble_2 = StackingClassifier(\n",
    "    estimators=base_classifiers_2, final_estimator=LogisticRegression(), cv=5\n",
    ")\n",
    "\n",
    "committee_models = [\n",
    "    (\"stacked_ensemble_1\", stacked_ensamble_1),\n",
    "    (\"stacked_ensemble_2\", stacked_ensamble_2),\n",
    "    (\n",
    "        \"gbc\",\n",
    "        make_pipeline(\n",
    "            StandardScaler(),\n",
    "            GradientBoostingClassifier(\n",
    "                random_state=SEED,\n",
    "                max_features=None,\n",
    "                n_estimators=500,\n",
    "                max_depth=30,\n",
    "                min_samples_leaf=2,\n",
    "                min_samples_split=5,\n",
    "            ),\n",
    "        ),\n",
    "    ),\n",
    "    (\"et\", make_pipeline(StandardScaler(), ExtraTreesClassifier(\n",
    "        random_state=SEED,\n",
    "        n_estimators=500,\n",
    "        max_depth=30,\n",
    "        min_samples_leaf=4,\n",
    "        min_samples_split=2,\n",
    "        ))),\n",
    "    (\n",
    "        \"rf\",\n",
    "        make_pipeline(\n",
    "            StandardScaler(),\n",
    "            RandomForestClassifier(\n",
    "                random_state=SEED,\n",
    "                n_estimators=500,\n",
    "                max_depth=30,\n",
    "                min_samples_leaf=4,\n",
    "                min_samples_split=2,\n",
    "            ),\n",
    "        ),\n",
    "    ),\n",
    "    (\n",
    "        \"mlp\",\n",
    "        make_pipeline(\n",
    "            StandardScaler(),\n",
    "            MLPClassifier(\n",
    "                random_state=SEED,\n",
    "                max_iter=1000,\n",
    "                early_stopping=True,\n",
    "                tol=1e-3,\n",
    "                solver=\"lbfgs\",\n",
    "                hidden_layer_sizes=(100, 300, 200, 100),\n",
    "                alpha=0.001,\n",
    "            ),\n",
    "        ),\n",
    "    ),\n",
    "    (\n",
    "        \"cb\",\n",
    "        make_pipeline(\n",
    "            StandardScaler(),\n",
    "            CatBoostClassifier(\n",
    "                iterations=500,\n",
    "                learning_rate=0.03,\n",
    "                depth=6,\n",
    "                l2_leaf_reg=3,\n",
    "                border_count=32,\n",
    "                cat_features=None,\n",
    "                loss_function=\"Logloss\",\n",
    "                eval_metric=\"Accuracy\",\n",
    "                random_seed=SEED,\n",
    "                early_stopping_rounds=50,\n",
    "                verbose=100,\n",
    "            ),\n",
    "        ),\n",
    "    ),\n",
    "    (\n",
    "        \"xgb\",\n",
    "        make_pipeline(\n",
    "            StandardScaler(),\n",
    "            XGBClassifier(\n",
    "                random_state=SEED,\n",
    "                use_label_encoder=False,\n",
    "                eval_metric=balanced_accuracy_score,\n",
    "                n_estimators=500,\n",
    "                learning_rate=0.02,\n",
    "                max_depth=6,\n",
    "                min_child_weight=1,\n",
    "                subsample=0.8,\n",
    "                colsample_bytree=0.8,\n",
    "                gamma=0,\n",
    "                reg_alpha=0.1,\n",
    "                reg_lambda=1.0,\n",
    "                scale_pos_weight=1,\n",
    "            ),\n",
    "        ),\n",
    "    ),\n",
    "]\n",
    "\n",
    "committee_model = VotingClassifier(committee_models, voting=\"soft\")\n",
    "committee_model.fit(train_x.copy(), train_y.copy().ravel())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model Balanced Accuracy: 0.9000525118151584\n"
     ]
    }
   ],
   "source": [
    "y_pred = committee_model.predict(valid_x)\n",
    "balanced_accuracy = balanced_accuracy_score(valid_y, y_pred)\n",
    "\n",
    "print(f\"Model Balanced Accuracy: {balanced_accuracy}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_path_proba = path.join(OUTPUT_DIR_MANUAL, UNIQUE_ID, \"manual_model_proba.txt\")\n",
    "output_path_model = path.join(OUTPUT_DIR_MANUAL, UNIQUE_ID, \"manual_model.pkl\")\n",
    "dump_proba(committee_model, test_x, output_path_proba)\n",
    "dump_model(committee_model, output_path_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Autogluon"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1600, 21) (400, 21)\n"
     ]
    }
   ],
   "source": [
    "train_data = np.concatenate((train_x.copy(), train_y.copy()), axis=1)\n",
    "train_data_pd = pd.DataFrame(train_data, copy=True)\n",
    "train_data_pd.rename(columns={train_data_pd.columns[-1]: \"class\"}, inplace=True)\n",
    "\n",
    "valid_data = np.concatenate((valid_x.copy(), valid_y.copy()), axis=1)\n",
    "valid_data_pd = pd.DataFrame(data=valid_data, copy=True)\n",
    "valid_data_pd.rename(columns={valid_data_pd.columns[-1]: \"class\"}, inplace=True)\n",
    "\n",
    "print(train_data_pd.shape, valid_data_pd.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning: path already exists! This predictor may overwrite an existing predictor! path=\"output\\autogluon\\20240114_144651\"\n",
      "Presets specified: ['best_quality']\n",
      "Warning: hyperparameter tuning is currently experimental and may cause the process to hang.\n",
      "Stack configuration (auto_stack=True): num_stack_levels=3, num_bag_folds=15, num_bag_sets=25\n",
      "Dynamic stacking is enabled (dynamic_stacking=True). AutoGluon will try to determine whether the input data is affected by stacked overfitting and enable or disable stacking as a consequence.\n",
      "Detecting stacked overfitting by sub-fitting AutoGluon on the input data. That is, copies of AutoGluon will be sub-fit on subset(s) of the data. Then, the holdout validation data is used to detect stacked overfitting.\n",
      "Sub-fit(s) time limit is: 60 seconds.\n",
      "Starting holdout-based sub-fit for dynamic stacking. Context path is: output\\autogluon\\20240114_144651/ds_sub_fit/sub_fit_ho.\n",
      "2024-01-14 14:47:04,082\tINFO util.py:159 -- Missing packages: ['ipywidgets']. Run `pip install -U ipywidgets`, then restart the notebook server for rich notebook output.\n",
      "Beginning AutoGluon training ... Time limit = 15s\n",
      "AutoGluon will save models to \"output\\autogluon\\20240114_144651/ds_sub_fit/sub_fit_ho\"\n",
      "=================== System Info ===================\n",
      "AutoGluon Version:  1.0.0\n",
      "Python Version:     3.8.10\n",
      "Operating System:   Windows\n",
      "Platform Machine:   AMD64\n",
      "Platform Version:   10.0.22621\n",
      "CPU Count:          8\n",
      "Memory Avail:       4.58 GB / 15.71 GB (29.1%)\n",
      "Disk Space Avail:   53.38 GB / 357.30 GB (14.9%)\n",
      "===================================================\n",
      "Train Data Rows:    1422\n",
      "Train Data Columns: 20\n",
      "Label Column:       class\n",
      "Problem Type:       binary\n",
      "Preprocessing data ...\n",
      "Selected class <--> label mapping:  class 1 = 1.0, class 0 = -1.0\n",
      "\tNote: For your binary classification, AutoGluon arbitrarily selected which label-value represents positive (1.0) vs negative (-1.0) class.\n",
      "\tTo explicitly set the positive_class, either rename classes to 1 and 0, or specify positive_class in Predictor init.\n",
      "Using Feature Generators to preprocess the data ...\n",
      "Fitting AutoMLPipelineFeatureGenerator...\n",
      "\tAvailable Memory:                    4694.87 MB\n",
      "\tTrain Data (Original)  Memory Usage: 0.22 MB (0.0% of available memory)\n",
      "\tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n",
      "\tStage 1 Generators:\n",
      "\t\tFitting AsTypeFeatureGenerator...\n",
      "\tStage 2 Generators:\n",
      "\t\tFitting FillNaFeatureGenerator...\n",
      "\tStage 3 Generators:\n",
      "\t\tFitting IdentityFeatureGenerator...\n",
      "\tStage 4 Generators:\n",
      "\t\tFitting DropUniqueFeatureGenerator...\n",
      "\tStage 5 Generators:\n",
      "\t\tFitting DropDuplicatesFeatureGenerator...\n",
      "\tTypes of features in original data (raw dtype, special dtypes):\n",
      "\t\t('float', []) : 20 | ['0', '1', '2', '3', '4', ...]\n",
      "\tTypes of features in processed data (raw dtype, special dtypes):\n",
      "\t\t('float', []) : 20 | ['0', '1', '2', '3', '4', ...]\n",
      "\t0.1s = Fit runtime\n",
      "\t20 features in original data used to generate 20 features in processed data.\n",
      "\tTrain Data (Processed) Memory Usage: 0.22 MB (0.0% of available memory)\n",
      "Data preprocessing and feature engineering runtime = 0.11s ...\n",
      "AutoGluon will gauge predictive performance using evaluation metric: 'balanced_accuracy'\n",
      "\tTo change this, specify the eval_metric parameter of Predictor()\n",
      "User-specified model hyperparameters to be fit:\n",
      "{\n",
      "\t'NN_TORCH': {},\n",
      "\t'GBM': [{'extra_trees': True, 'ag_args': {'name_suffix': 'XT'}}, {}, 'GBMLarge'],\n",
      "\t'CAT': {},\n",
      "\t'XGB': {},\n",
      "\t'FASTAI': {},\n",
      "\t'RF': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
      "\t'XT': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
      "\t'KNN': [{'weights': 'uniform', 'ag_args': {'name_suffix': 'Unif'}}, {'weights': 'distance', 'ag_args': {'name_suffix': 'Dist'}}],\n",
      "}\n",
      "AutoGluon will fit 4 stack levels (L1 to L4) ...\n",
      "Fitting 13 L1 models ...\n",
      "Hyperparameter tuning model: KNeighborsUnif_BAG_L1 ... Tuning model for up to 0.34s of the 14.87s of remaining time.\n",
      "Warning: Exception caused KNeighborsUnif_BAG_L1 to fail during hyperparameter tuning... Skipping this model.\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\JAN_CICHOMSKI\\STUDIA\\STUDIA_SEMESTR_7_2023_ZIMA\\auto_ml\\homeworks\\homework_1\\AUTOML_HM2\\.venv\\lib\\site-packages\\autogluon\\core\\trainer\\abstract_trainer.py\", line 2135, in _train_single_full\n",
      "    hpo_models, hpo_results = model.hyperparameter_tune(\n",
      "  File \"c:\\JAN_CICHOMSKI\\STUDIA\\STUDIA_SEMESTR_7_2023_ZIMA\\auto_ml\\homeworks\\homework_1\\AUTOML_HM2\\.venv\\lib\\site-packages\\autogluon\\core\\models\\abstract\\abstract_model.py\", line 1466, in hyperparameter_tune\n",
      "    hpo_executor.initialize(hyperparameter_tune_kwargs, default_num_trials=default_num_trials, time_limit=time_limit)\n",
      "  File \"c:\\JAN_CICHOMSKI\\STUDIA\\STUDIA_SEMESTR_7_2023_ZIMA\\auto_ml\\homeworks\\homework_1\\AUTOML_HM2\\.venv\\lib\\site-packages\\autogluon\\core\\hpo\\executors.py\", line 489, in initialize\n",
      "    hyperparameter_tune_kwargs = scheduler_factory(hyperparameter_tune_kwargs, num_trials=num_trials, nthreads_per_trial=\"auto\", ngpus_per_trial=\"auto\")\n",
      "  File \"c:\\JAN_CICHOMSKI\\STUDIA\\STUDIA_SEMESTR_7_2023_ZIMA\\auto_ml\\homeworks\\homework_1\\AUTOML_HM2\\.venv\\lib\\site-packages\\autogluon\\core\\scheduler\\scheduler_factory.py\", line 191, in scheduler_factory\n",
      "    raise ValueError(f\"Required key 'scheduler' is not present in hyperparameter_tune_kwargs: {hyperparameter_tune_kwargs}\")\n",
      "ValueError: Required key 'scheduler' is not present in hyperparameter_tune_kwargs: {'searcher': 'auto', 'time_out': 1200, 'num_trials': 30}\n",
      "Required key 'scheduler' is not present in hyperparameter_tune_kwargs: {'searcher': 'auto', 'time_out': 1200, 'num_trials': 30}\n",
      "Hyperparameter tuning model: KNeighborsDist_BAG_L1 ... Tuning model for up to 0.34s of the 14.86s of remaining time.\n",
      "Warning: Exception caused KNeighborsDist_BAG_L1 to fail during hyperparameter tuning... Skipping this model.\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\JAN_CICHOMSKI\\STUDIA\\STUDIA_SEMESTR_7_2023_ZIMA\\auto_ml\\homeworks\\homework_1\\AUTOML_HM2\\.venv\\lib\\site-packages\\autogluon\\core\\trainer\\abstract_trainer.py\", line 2135, in _train_single_full\n",
      "    hpo_models, hpo_results = model.hyperparameter_tune(\n",
      "  File \"c:\\JAN_CICHOMSKI\\STUDIA\\STUDIA_SEMESTR_7_2023_ZIMA\\auto_ml\\homeworks\\homework_1\\AUTOML_HM2\\.venv\\lib\\site-packages\\autogluon\\core\\models\\abstract\\abstract_model.py\", line 1466, in hyperparameter_tune\n",
      "    hpo_executor.initialize(hyperparameter_tune_kwargs, default_num_trials=default_num_trials, time_limit=time_limit)\n",
      "  File \"c:\\JAN_CICHOMSKI\\STUDIA\\STUDIA_SEMESTR_7_2023_ZIMA\\auto_ml\\homeworks\\homework_1\\AUTOML_HM2\\.venv\\lib\\site-packages\\autogluon\\core\\hpo\\executors.py\", line 489, in initialize\n",
      "    hyperparameter_tune_kwargs = scheduler_factory(hyperparameter_tune_kwargs, num_trials=num_trials, nthreads_per_trial=\"auto\", ngpus_per_trial=\"auto\")\n",
      "  File \"c:\\JAN_CICHOMSKI\\STUDIA\\STUDIA_SEMESTR_7_2023_ZIMA\\auto_ml\\homeworks\\homework_1\\AUTOML_HM2\\.venv\\lib\\site-packages\\autogluon\\core\\scheduler\\scheduler_factory.py\", line 191, in scheduler_factory\n",
      "    raise ValueError(f\"Required key 'scheduler' is not present in hyperparameter_tune_kwargs: {hyperparameter_tune_kwargs}\")\n",
      "ValueError: Required key 'scheduler' is not present in hyperparameter_tune_kwargs: {'searcher': 'auto', 'time_out': 1200, 'num_trials': 30}\n",
      "Required key 'scheduler' is not present in hyperparameter_tune_kwargs: {'searcher': 'auto', 'time_out': 1200, 'num_trials': 30}\n",
      "Hyperparameter tuning model: LightGBMXT_BAG_L1 ... Tuning model for up to 0.34s of the 14.85s of remaining time.\n",
      "Warning: Exception caused LightGBMXT_BAG_L1 to fail during hyperparameter tuning... Skipping this model.\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\JAN_CICHOMSKI\\STUDIA\\STUDIA_SEMESTR_7_2023_ZIMA\\auto_ml\\homeworks\\homework_1\\AUTOML_HM2\\.venv\\lib\\site-packages\\autogluon\\core\\trainer\\abstract_trainer.py\", line 2135, in _train_single_full\n",
      "    hpo_models, hpo_results = model.hyperparameter_tune(\n",
      "  File \"c:\\JAN_CICHOMSKI\\STUDIA\\STUDIA_SEMESTR_7_2023_ZIMA\\auto_ml\\homeworks\\homework_1\\AUTOML_HM2\\.venv\\lib\\site-packages\\autogluon\\core\\models\\abstract\\abstract_model.py\", line 1466, in hyperparameter_tune\n",
      "    hpo_executor.initialize(hyperparameter_tune_kwargs, default_num_trials=default_num_trials, time_limit=time_limit)\n",
      "  File \"c:\\JAN_CICHOMSKI\\STUDIA\\STUDIA_SEMESTR_7_2023_ZIMA\\auto_ml\\homeworks\\homework_1\\AUTOML_HM2\\.venv\\lib\\site-packages\\autogluon\\core\\hpo\\executors.py\", line 489, in initialize\n",
      "    hyperparameter_tune_kwargs = scheduler_factory(hyperparameter_tune_kwargs, num_trials=num_trials, nthreads_per_trial=\"auto\", ngpus_per_trial=\"auto\")\n",
      "  File \"c:\\JAN_CICHOMSKI\\STUDIA\\STUDIA_SEMESTR_7_2023_ZIMA\\auto_ml\\homeworks\\homework_1\\AUTOML_HM2\\.venv\\lib\\site-packages\\autogluon\\core\\scheduler\\scheduler_factory.py\", line 191, in scheduler_factory\n",
      "    raise ValueError(f\"Required key 'scheduler' is not present in hyperparameter_tune_kwargs: {hyperparameter_tune_kwargs}\")\n",
      "ValueError: Required key 'scheduler' is not present in hyperparameter_tune_kwargs: {'searcher': 'auto', 'time_out': 1200, 'num_trials': 30}\n",
      "Required key 'scheduler' is not present in hyperparameter_tune_kwargs: {'searcher': 'auto', 'time_out': 1200, 'num_trials': 30}\n",
      "Hyperparameter tuning model: LightGBM_BAG_L1 ... Tuning model for up to 0.34s of the 14.84s of remaining time.\n",
      "Warning: Exception caused LightGBM_BAG_L1 to fail during hyperparameter tuning... Skipping this model.\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\JAN_CICHOMSKI\\STUDIA\\STUDIA_SEMESTR_7_2023_ZIMA\\auto_ml\\homeworks\\homework_1\\AUTOML_HM2\\.venv\\lib\\site-packages\\autogluon\\core\\trainer\\abstract_trainer.py\", line 2135, in _train_single_full\n",
      "    hpo_models, hpo_results = model.hyperparameter_tune(\n",
      "  File \"c:\\JAN_CICHOMSKI\\STUDIA\\STUDIA_SEMESTR_7_2023_ZIMA\\auto_ml\\homeworks\\homework_1\\AUTOML_HM2\\.venv\\lib\\site-packages\\autogluon\\core\\models\\abstract\\abstract_model.py\", line 1466, in hyperparameter_tune\n",
      "    hpo_executor.initialize(hyperparameter_tune_kwargs, default_num_trials=default_num_trials, time_limit=time_limit)\n",
      "  File \"c:\\JAN_CICHOMSKI\\STUDIA\\STUDIA_SEMESTR_7_2023_ZIMA\\auto_ml\\homeworks\\homework_1\\AUTOML_HM2\\.venv\\lib\\site-packages\\autogluon\\core\\hpo\\executors.py\", line 489, in initialize\n",
      "    hyperparameter_tune_kwargs = scheduler_factory(hyperparameter_tune_kwargs, num_trials=num_trials, nthreads_per_trial=\"auto\", ngpus_per_trial=\"auto\")\n",
      "  File \"c:\\JAN_CICHOMSKI\\STUDIA\\STUDIA_SEMESTR_7_2023_ZIMA\\auto_ml\\homeworks\\homework_1\\AUTOML_HM2\\.venv\\lib\\site-packages\\autogluon\\core\\scheduler\\scheduler_factory.py\", line 191, in scheduler_factory\n",
      "    raise ValueError(f\"Required key 'scheduler' is not present in hyperparameter_tune_kwargs: {hyperparameter_tune_kwargs}\")\n",
      "ValueError: Required key 'scheduler' is not present in hyperparameter_tune_kwargs: {'searcher': 'auto', 'time_out': 1200, 'num_trials': 30}\n",
      "Required key 'scheduler' is not present in hyperparameter_tune_kwargs: {'searcher': 'auto', 'time_out': 1200, 'num_trials': 30}\n",
      "Hyperparameter tuning model: RandomForestGini_BAG_L1 ... Tuning model for up to 0.34s of the 14.84s of remaining time.\n",
      "Warning: Exception caused RandomForestGini_BAG_L1 to fail during hyperparameter tuning... Skipping this model.\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\JAN_CICHOMSKI\\STUDIA\\STUDIA_SEMESTR_7_2023_ZIMA\\auto_ml\\homeworks\\homework_1\\AUTOML_HM2\\.venv\\lib\\site-packages\\autogluon\\core\\trainer\\abstract_trainer.py\", line 2135, in _train_single_full\n",
      "    hpo_models, hpo_results = model.hyperparameter_tune(\n",
      "  File \"c:\\JAN_CICHOMSKI\\STUDIA\\STUDIA_SEMESTR_7_2023_ZIMA\\auto_ml\\homeworks\\homework_1\\AUTOML_HM2\\.venv\\lib\\site-packages\\autogluon\\core\\models\\abstract\\abstract_model.py\", line 1466, in hyperparameter_tune\n",
      "    hpo_executor.initialize(hyperparameter_tune_kwargs, default_num_trials=default_num_trials, time_limit=time_limit)\n",
      "  File \"c:\\JAN_CICHOMSKI\\STUDIA\\STUDIA_SEMESTR_7_2023_ZIMA\\auto_ml\\homeworks\\homework_1\\AUTOML_HM2\\.venv\\lib\\site-packages\\autogluon\\core\\hpo\\executors.py\", line 489, in initialize\n",
      "    hyperparameter_tune_kwargs = scheduler_factory(hyperparameter_tune_kwargs, num_trials=num_trials, nthreads_per_trial=\"auto\", ngpus_per_trial=\"auto\")\n",
      "  File \"c:\\JAN_CICHOMSKI\\STUDIA\\STUDIA_SEMESTR_7_2023_ZIMA\\auto_ml\\homeworks\\homework_1\\AUTOML_HM2\\.venv\\lib\\site-packages\\autogluon\\core\\scheduler\\scheduler_factory.py\", line 191, in scheduler_factory\n",
      "    raise ValueError(f\"Required key 'scheduler' is not present in hyperparameter_tune_kwargs: {hyperparameter_tune_kwargs}\")\n",
      "ValueError: Required key 'scheduler' is not present in hyperparameter_tune_kwargs: {'searcher': 'auto', 'time_out': 1200, 'num_trials': 30}\n",
      "Required key 'scheduler' is not present in hyperparameter_tune_kwargs: {'searcher': 'auto', 'time_out': 1200, 'num_trials': 30}\n",
      "Hyperparameter tuning model: RandomForestEntr_BAG_L1 ... Tuning model for up to 0.34s of the 14.83s of remaining time.\n",
      "Warning: Exception caused RandomForestEntr_BAG_L1 to fail during hyperparameter tuning... Skipping this model.\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\JAN_CICHOMSKI\\STUDIA\\STUDIA_SEMESTR_7_2023_ZIMA\\auto_ml\\homeworks\\homework_1\\AUTOML_HM2\\.venv\\lib\\site-packages\\autogluon\\core\\trainer\\abstract_trainer.py\", line 2135, in _train_single_full\n",
      "    hpo_models, hpo_results = model.hyperparameter_tune(\n",
      "  File \"c:\\JAN_CICHOMSKI\\STUDIA\\STUDIA_SEMESTR_7_2023_ZIMA\\auto_ml\\homeworks\\homework_1\\AUTOML_HM2\\.venv\\lib\\site-packages\\autogluon\\core\\models\\abstract\\abstract_model.py\", line 1466, in hyperparameter_tune\n",
      "    hpo_executor.initialize(hyperparameter_tune_kwargs, default_num_trials=default_num_trials, time_limit=time_limit)\n",
      "  File \"c:\\JAN_CICHOMSKI\\STUDIA\\STUDIA_SEMESTR_7_2023_ZIMA\\auto_ml\\homeworks\\homework_1\\AUTOML_HM2\\.venv\\lib\\site-packages\\autogluon\\core\\hpo\\executors.py\", line 489, in initialize\n",
      "    hyperparameter_tune_kwargs = scheduler_factory(hyperparameter_tune_kwargs, num_trials=num_trials, nthreads_per_trial=\"auto\", ngpus_per_trial=\"auto\")\n",
      "  File \"c:\\JAN_CICHOMSKI\\STUDIA\\STUDIA_SEMESTR_7_2023_ZIMA\\auto_ml\\homeworks\\homework_1\\AUTOML_HM2\\.venv\\lib\\site-packages\\autogluon\\core\\scheduler\\scheduler_factory.py\", line 191, in scheduler_factory\n",
      "    raise ValueError(f\"Required key 'scheduler' is not present in hyperparameter_tune_kwargs: {hyperparameter_tune_kwargs}\")\n",
      "ValueError: Required key 'scheduler' is not present in hyperparameter_tune_kwargs: {'searcher': 'auto', 'time_out': 1200, 'num_trials': 30}\n",
      "Required key 'scheduler' is not present in hyperparameter_tune_kwargs: {'searcher': 'auto', 'time_out': 1200, 'num_trials': 30}\n",
      "Hyperparameter tuning model: CatBoost_BAG_L1 ... Tuning model for up to 0.34s of the 14.82s of remaining time.\n",
      "Warning: Exception caused CatBoost_BAG_L1 to fail during hyperparameter tuning... Skipping this model.\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\JAN_CICHOMSKI\\STUDIA\\STUDIA_SEMESTR_7_2023_ZIMA\\auto_ml\\homeworks\\homework_1\\AUTOML_HM2\\.venv\\lib\\site-packages\\autogluon\\core\\trainer\\abstract_trainer.py\", line 2135, in _train_single_full\n",
      "    hpo_models, hpo_results = model.hyperparameter_tune(\n",
      "  File \"c:\\JAN_CICHOMSKI\\STUDIA\\STUDIA_SEMESTR_7_2023_ZIMA\\auto_ml\\homeworks\\homework_1\\AUTOML_HM2\\.venv\\lib\\site-packages\\autogluon\\core\\models\\abstract\\abstract_model.py\", line 1466, in hyperparameter_tune\n",
      "    hpo_executor.initialize(hyperparameter_tune_kwargs, default_num_trials=default_num_trials, time_limit=time_limit)\n",
      "  File \"c:\\JAN_CICHOMSKI\\STUDIA\\STUDIA_SEMESTR_7_2023_ZIMA\\auto_ml\\homeworks\\homework_1\\AUTOML_HM2\\.venv\\lib\\site-packages\\autogluon\\core\\hpo\\executors.py\", line 489, in initialize\n",
      "    hyperparameter_tune_kwargs = scheduler_factory(hyperparameter_tune_kwargs, num_trials=num_trials, nthreads_per_trial=\"auto\", ngpus_per_trial=\"auto\")\n",
      "  File \"c:\\JAN_CICHOMSKI\\STUDIA\\STUDIA_SEMESTR_7_2023_ZIMA\\auto_ml\\homeworks\\homework_1\\AUTOML_HM2\\.venv\\lib\\site-packages\\autogluon\\core\\scheduler\\scheduler_factory.py\", line 191, in scheduler_factory\n",
      "    raise ValueError(f\"Required key 'scheduler' is not present in hyperparameter_tune_kwargs: {hyperparameter_tune_kwargs}\")\n",
      "ValueError: Required key 'scheduler' is not present in hyperparameter_tune_kwargs: {'searcher': 'auto', 'time_out': 1200, 'num_trials': 30}\n",
      "Required key 'scheduler' is not present in hyperparameter_tune_kwargs: {'searcher': 'auto', 'time_out': 1200, 'num_trials': 30}\n",
      "Hyperparameter tuning model: ExtraTreesGini_BAG_L1 ... Tuning model for up to 0.34s of the 14.82s of remaining time.\n",
      "Warning: Exception caused ExtraTreesGini_BAG_L1 to fail during hyperparameter tuning... Skipping this model.\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\JAN_CICHOMSKI\\STUDIA\\STUDIA_SEMESTR_7_2023_ZIMA\\auto_ml\\homeworks\\homework_1\\AUTOML_HM2\\.venv\\lib\\site-packages\\autogluon\\core\\trainer\\abstract_trainer.py\", line 2135, in _train_single_full\n",
      "    hpo_models, hpo_results = model.hyperparameter_tune(\n",
      "  File \"c:\\JAN_CICHOMSKI\\STUDIA\\STUDIA_SEMESTR_7_2023_ZIMA\\auto_ml\\homeworks\\homework_1\\AUTOML_HM2\\.venv\\lib\\site-packages\\autogluon\\core\\models\\abstract\\abstract_model.py\", line 1466, in hyperparameter_tune\n",
      "    hpo_executor.initialize(hyperparameter_tune_kwargs, default_num_trials=default_num_trials, time_limit=time_limit)\n",
      "  File \"c:\\JAN_CICHOMSKI\\STUDIA\\STUDIA_SEMESTR_7_2023_ZIMA\\auto_ml\\homeworks\\homework_1\\AUTOML_HM2\\.venv\\lib\\site-packages\\autogluon\\core\\hpo\\executors.py\", line 489, in initialize\n",
      "    hyperparameter_tune_kwargs = scheduler_factory(hyperparameter_tune_kwargs, num_trials=num_trials, nthreads_per_trial=\"auto\", ngpus_per_trial=\"auto\")\n",
      "  File \"c:\\JAN_CICHOMSKI\\STUDIA\\STUDIA_SEMESTR_7_2023_ZIMA\\auto_ml\\homeworks\\homework_1\\AUTOML_HM2\\.venv\\lib\\site-packages\\autogluon\\core\\scheduler\\scheduler_factory.py\", line 191, in scheduler_factory\n",
      "    raise ValueError(f\"Required key 'scheduler' is not present in hyperparameter_tune_kwargs: {hyperparameter_tune_kwargs}\")\n",
      "ValueError: Required key 'scheduler' is not present in hyperparameter_tune_kwargs: {'searcher': 'auto', 'time_out': 1200, 'num_trials': 30}\n",
      "Required key 'scheduler' is not present in hyperparameter_tune_kwargs: {'searcher': 'auto', 'time_out': 1200, 'num_trials': 30}\n",
      "Hyperparameter tuning model: ExtraTreesEntr_BAG_L1 ... Tuning model for up to 0.34s of the 14.82s of remaining time.\n",
      "Warning: Exception caused ExtraTreesEntr_BAG_L1 to fail during hyperparameter tuning... Skipping this model.\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\JAN_CICHOMSKI\\STUDIA\\STUDIA_SEMESTR_7_2023_ZIMA\\auto_ml\\homeworks\\homework_1\\AUTOML_HM2\\.venv\\lib\\site-packages\\autogluon\\core\\trainer\\abstract_trainer.py\", line 2135, in _train_single_full\n",
      "    hpo_models, hpo_results = model.hyperparameter_tune(\n",
      "  File \"c:\\JAN_CICHOMSKI\\STUDIA\\STUDIA_SEMESTR_7_2023_ZIMA\\auto_ml\\homeworks\\homework_1\\AUTOML_HM2\\.venv\\lib\\site-packages\\autogluon\\core\\models\\abstract\\abstract_model.py\", line 1466, in hyperparameter_tune\n",
      "    hpo_executor.initialize(hyperparameter_tune_kwargs, default_num_trials=default_num_trials, time_limit=time_limit)\n",
      "  File \"c:\\JAN_CICHOMSKI\\STUDIA\\STUDIA_SEMESTR_7_2023_ZIMA\\auto_ml\\homeworks\\homework_1\\AUTOML_HM2\\.venv\\lib\\site-packages\\autogluon\\core\\hpo\\executors.py\", line 489, in initialize\n",
      "    hyperparameter_tune_kwargs = scheduler_factory(hyperparameter_tune_kwargs, num_trials=num_trials, nthreads_per_trial=\"auto\", ngpus_per_trial=\"auto\")\n",
      "  File \"c:\\JAN_CICHOMSKI\\STUDIA\\STUDIA_SEMESTR_7_2023_ZIMA\\auto_ml\\homeworks\\homework_1\\AUTOML_HM2\\.venv\\lib\\site-packages\\autogluon\\core\\scheduler\\scheduler_factory.py\", line 191, in scheduler_factory\n",
      "    raise ValueError(f\"Required key 'scheduler' is not present in hyperparameter_tune_kwargs: {hyperparameter_tune_kwargs}\")\n",
      "ValueError: Required key 'scheduler' is not present in hyperparameter_tune_kwargs: {'searcher': 'auto', 'time_out': 1200, 'num_trials': 30}\n",
      "Required key 'scheduler' is not present in hyperparameter_tune_kwargs: {'searcher': 'auto', 'time_out': 1200, 'num_trials': 30}\n",
      "Hyperparameter tuning model: NeuralNetFastAI_BAG_L1 ... Tuning model for up to 0.34s of the 14.8s of remaining time.\n",
      "Warning: Exception caused NeuralNetFastAI_BAG_L1 to fail during hyperparameter tuning... Skipping this model.\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\JAN_CICHOMSKI\\STUDIA\\STUDIA_SEMESTR_7_2023_ZIMA\\auto_ml\\homeworks\\homework_1\\AUTOML_HM2\\.venv\\lib\\site-packages\\autogluon\\core\\trainer\\abstract_trainer.py\", line 2135, in _train_single_full\n",
      "    hpo_models, hpo_results = model.hyperparameter_tune(\n",
      "  File \"c:\\JAN_CICHOMSKI\\STUDIA\\STUDIA_SEMESTR_7_2023_ZIMA\\auto_ml\\homeworks\\homework_1\\AUTOML_HM2\\.venv\\lib\\site-packages\\autogluon\\core\\models\\abstract\\abstract_model.py\", line 1466, in hyperparameter_tune\n",
      "    hpo_executor.initialize(hyperparameter_tune_kwargs, default_num_trials=default_num_trials, time_limit=time_limit)\n",
      "  File \"c:\\JAN_CICHOMSKI\\STUDIA\\STUDIA_SEMESTR_7_2023_ZIMA\\auto_ml\\homeworks\\homework_1\\AUTOML_HM2\\.venv\\lib\\site-packages\\autogluon\\core\\hpo\\executors.py\", line 336, in initialize\n",
      "    hyperparameter_tune_kwargs[\"scheduler\"], hyperparameter_tune_kwargs[\"scheduler\"]\n",
      "KeyError: 'scheduler'\n",
      "'scheduler'\n",
      "Hyperparameter tuning model: XGBoost_BAG_L1 ... Tuning model for up to 0.34s of the 14.8s of remaining time.\n",
      "Warning: Exception caused XGBoost_BAG_L1 to fail during hyperparameter tuning... Skipping this model.\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\JAN_CICHOMSKI\\STUDIA\\STUDIA_SEMESTR_7_2023_ZIMA\\auto_ml\\homeworks\\homework_1\\AUTOML_HM2\\.venv\\lib\\site-packages\\autogluon\\core\\trainer\\abstract_trainer.py\", line 2135, in _train_single_full\n",
      "    hpo_models, hpo_results = model.hyperparameter_tune(\n",
      "  File \"c:\\JAN_CICHOMSKI\\STUDIA\\STUDIA_SEMESTR_7_2023_ZIMA\\auto_ml\\homeworks\\homework_1\\AUTOML_HM2\\.venv\\lib\\site-packages\\autogluon\\core\\models\\abstract\\abstract_model.py\", line 1466, in hyperparameter_tune\n",
      "    hpo_executor.initialize(hyperparameter_tune_kwargs, default_num_trials=default_num_trials, time_limit=time_limit)\n",
      "  File \"c:\\JAN_CICHOMSKI\\STUDIA\\STUDIA_SEMESTR_7_2023_ZIMA\\auto_ml\\homeworks\\homework_1\\AUTOML_HM2\\.venv\\lib\\site-packages\\autogluon\\core\\hpo\\executors.py\", line 489, in initialize\n",
      "    hyperparameter_tune_kwargs = scheduler_factory(hyperparameter_tune_kwargs, num_trials=num_trials, nthreads_per_trial=\"auto\", ngpus_per_trial=\"auto\")\n",
      "  File \"c:\\JAN_CICHOMSKI\\STUDIA\\STUDIA_SEMESTR_7_2023_ZIMA\\auto_ml\\homeworks\\homework_1\\AUTOML_HM2\\.venv\\lib\\site-packages\\autogluon\\core\\scheduler\\scheduler_factory.py\", line 191, in scheduler_factory\n",
      "    raise ValueError(f\"Required key 'scheduler' is not present in hyperparameter_tune_kwargs: {hyperparameter_tune_kwargs}\")\n",
      "ValueError: Required key 'scheduler' is not present in hyperparameter_tune_kwargs: {'searcher': 'auto', 'time_out': 1200, 'num_trials': 30}\n",
      "Required key 'scheduler' is not present in hyperparameter_tune_kwargs: {'searcher': 'auto', 'time_out': 1200, 'num_trials': 30}\n",
      "Hyperparameter tuning model: NeuralNetTorch_BAG_L1 ... Tuning model for up to 0.34s of the 14.79s of remaining time.\n",
      "Warning: Exception caused NeuralNetTorch_BAG_L1 to fail during hyperparameter tuning... Skipping this model.\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\JAN_CICHOMSKI\\STUDIA\\STUDIA_SEMESTR_7_2023_ZIMA\\auto_ml\\homeworks\\homework_1\\AUTOML_HM2\\.venv\\lib\\site-packages\\autogluon\\core\\trainer\\abstract_trainer.py\", line 2135, in _train_single_full\n",
      "    hpo_models, hpo_results = model.hyperparameter_tune(\n",
      "  File \"c:\\JAN_CICHOMSKI\\STUDIA\\STUDIA_SEMESTR_7_2023_ZIMA\\auto_ml\\homeworks\\homework_1\\AUTOML_HM2\\.venv\\lib\\site-packages\\autogluon\\core\\models\\abstract\\abstract_model.py\", line 1466, in hyperparameter_tune\n",
      "    hpo_executor.initialize(hyperparameter_tune_kwargs, default_num_trials=default_num_trials, time_limit=time_limit)\n",
      "  File \"c:\\JAN_CICHOMSKI\\STUDIA\\STUDIA_SEMESTR_7_2023_ZIMA\\auto_ml\\homeworks\\homework_1\\AUTOML_HM2\\.venv\\lib\\site-packages\\autogluon\\core\\hpo\\executors.py\", line 336, in initialize\n",
      "    hyperparameter_tune_kwargs[\"scheduler\"], hyperparameter_tune_kwargs[\"scheduler\"]\n",
      "KeyError: 'scheduler'\n",
      "'scheduler'\n",
      "Fitting model: LightGBMLarge_BAG_L1 ... Training model for up to 0.34s of the 14.79s of remaining time.\n",
      "\tFitting 15 child models (S1F1 - S1F15) | Fitting with ParallelLocalFoldFittingStrategy (8 workers, per: cpus=1, gpus=0, memory=0.35%)\n",
      "\tTime limit exceeded... Skipping LightGBMLarge_BAG_L1.\n",
      "Completed 1/25 k-fold bagging repeats ...\n",
      "No base models to train on, skipping auxiliary stack level 2...\n",
      "No base models to train on, skipping stack level 2...\n",
      "No base models to train on, skipping auxiliary stack level 3...\n",
      "No base models to train on, skipping stack level 3...\n",
      "No base models to train on, skipping auxiliary stack level 4...\n",
      "No base models to train on, skipping stack level 4...\n",
      "No base models to train on, skipping auxiliary stack level 5...\n",
      "No base models to train on, skipping auxiliary stack level 5...\n",
      "Warning: AutoGluon did not successfully train any models\n",
      "AutoGluon training complete, total runtime = 9.26s ... Best model: \"None\"\n",
      "Warning: No models found, skipping post_fit logic...\n",
      "TabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"output\\autogluon\\20240114_144651/ds_sub_fit/sub_fit_ho\")\n",
      "Unable to determine stacked overfitting. AutoGluon's sub-fit did not successfully train any models!\n",
      "Stacked overfitting occurred: False.\n",
      "Spend 10 seconds for the sub-fit(s) during dynamic stacking.\n",
      "Time left for full fit of AutoGluon: 50 seconds.\n",
      "Starting full fit now with num_stack_levels 3.\n",
      "Beginning AutoGluon training ... Time limit = 50s\n",
      "AutoGluon will save models to \"output\\autogluon\\20240114_144651\"\n",
      "=================== System Info ===================\n",
      "AutoGluon Version:  1.0.0\n",
      "Python Version:     3.8.10\n",
      "Operating System:   Windows\n",
      "Platform Machine:   AMD64\n",
      "Platform Version:   10.0.22621\n",
      "CPU Count:          8\n",
      "Memory Avail:       3.57 GB / 15.71 GB (22.7%)\n",
      "Disk Space Avail:   50.84 GB / 357.30 GB (14.2%)\n",
      "===================================================\n",
      "Train Data Rows:    1600\n",
      "Train Data Columns: 20\n",
      "Label Column:       class\n",
      "Problem Type:       binary\n",
      "Preprocessing data ...\n",
      "Selected class <--> label mapping:  class 1 = 1.0, class 0 = -1.0\n",
      "\tNote: For your binary classification, AutoGluon arbitrarily selected which label-value represents positive (1.0) vs negative (-1.0) class.\n",
      "\tTo explicitly set the positive_class, either rename classes to 1 and 0, or specify positive_class in Predictor init.\n",
      "Using Feature Generators to preprocess the data ...\n",
      "Fitting AutoMLPipelineFeatureGenerator...\n",
      "\tAvailable Memory:                    3657.42 MB\n",
      "\tTrain Data (Original)  Memory Usage: 0.24 MB (0.0% of available memory)\n",
      "\tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n",
      "\tStage 1 Generators:\n",
      "\t\tFitting AsTypeFeatureGenerator...\n",
      "\tStage 2 Generators:\n",
      "\t\tFitting FillNaFeatureGenerator...\n",
      "\tStage 3 Generators:\n",
      "\t\tFitting IdentityFeatureGenerator...\n",
      "\tStage 4 Generators:\n",
      "\t\tFitting DropUniqueFeatureGenerator...\n",
      "\tStage 5 Generators:\n",
      "\t\tFitting DropDuplicatesFeatureGenerator...\n",
      "\tTypes of features in original data (raw dtype, special dtypes):\n",
      "\t\t('float', []) : 20 | ['0', '1', '2', '3', '4', ...]\n",
      "\tTypes of features in processed data (raw dtype, special dtypes):\n",
      "\t\t('float', []) : 20 | ['0', '1', '2', '3', '4', ...]\n",
      "\t0.1s = Fit runtime\n",
      "\t20 features in original data used to generate 20 features in processed data.\n",
      "\tTrain Data (Processed) Memory Usage: 0.24 MB (0.0% of available memory)\n",
      "Data preprocessing and feature engineering runtime = 0.08s ...\n",
      "AutoGluon will gauge predictive performance using evaluation metric: 'balanced_accuracy'\n",
      "\tTo change this, specify the eval_metric parameter of Predictor()\n",
      "User-specified model hyperparameters to be fit:\n",
      "{\n",
      "\t'NN_TORCH': {},\n",
      "\t'GBM': [{'extra_trees': True, 'ag_args': {'name_suffix': 'XT'}}, {}, 'GBMLarge'],\n",
      "\t'CAT': {},\n",
      "\t'XGB': {},\n",
      "\t'FASTAI': {},\n",
      "\t'RF': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
      "\t'XT': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
      "\t'KNN': [{'weights': 'uniform', 'ag_args': {'name_suffix': 'Unif'}}, {'weights': 'distance', 'ag_args': {'name_suffix': 'Dist'}}],\n",
      "}\n",
      "AutoGluon will fit 4 stack levels (L1 to L4) ...\n",
      "Fitting 13 L1 models ...\n",
      "Hyperparameter tuning model: KNeighborsUnif_BAG_L1 ... Tuning model for up to 1.15s of the 49.92s of remaining time.\n",
      "Warning: Exception caused KNeighborsUnif_BAG_L1 to fail during hyperparameter tuning... Skipping this model.\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\JAN_CICHOMSKI\\STUDIA\\STUDIA_SEMESTR_7_2023_ZIMA\\auto_ml\\homeworks\\homework_1\\AUTOML_HM2\\.venv\\lib\\site-packages\\autogluon\\core\\trainer\\abstract_trainer.py\", line 2135, in _train_single_full\n",
      "    hpo_models, hpo_results = model.hyperparameter_tune(\n",
      "  File \"c:\\JAN_CICHOMSKI\\STUDIA\\STUDIA_SEMESTR_7_2023_ZIMA\\auto_ml\\homeworks\\homework_1\\AUTOML_HM2\\.venv\\lib\\site-packages\\autogluon\\core\\models\\abstract\\abstract_model.py\", line 1466, in hyperparameter_tune\n",
      "    hpo_executor.initialize(hyperparameter_tune_kwargs, default_num_trials=default_num_trials, time_limit=time_limit)\n",
      "  File \"c:\\JAN_CICHOMSKI\\STUDIA\\STUDIA_SEMESTR_7_2023_ZIMA\\auto_ml\\homeworks\\homework_1\\AUTOML_HM2\\.venv\\lib\\site-packages\\autogluon\\core\\hpo\\executors.py\", line 489, in initialize\n",
      "    hyperparameter_tune_kwargs = scheduler_factory(hyperparameter_tune_kwargs, num_trials=num_trials, nthreads_per_trial=\"auto\", ngpus_per_trial=\"auto\")\n",
      "  File \"c:\\JAN_CICHOMSKI\\STUDIA\\STUDIA_SEMESTR_7_2023_ZIMA\\auto_ml\\homeworks\\homework_1\\AUTOML_HM2\\.venv\\lib\\site-packages\\autogluon\\core\\scheduler\\scheduler_factory.py\", line 191, in scheduler_factory\n",
      "    raise ValueError(f\"Required key 'scheduler' is not present in hyperparameter_tune_kwargs: {hyperparameter_tune_kwargs}\")\n",
      "ValueError: Required key 'scheduler' is not present in hyperparameter_tune_kwargs: {'searcher': 'auto', 'time_out': 1200, 'num_trials': 30}\n",
      "Required key 'scheduler' is not present in hyperparameter_tune_kwargs: {'searcher': 'auto', 'time_out': 1200, 'num_trials': 30}\n",
      "Hyperparameter tuning model: KNeighborsDist_BAG_L1 ... Tuning model for up to 1.15s of the 49.91s of remaining time.\n",
      "Warning: Exception caused KNeighborsDist_BAG_L1 to fail during hyperparameter tuning... Skipping this model.\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\JAN_CICHOMSKI\\STUDIA\\STUDIA_SEMESTR_7_2023_ZIMA\\auto_ml\\homeworks\\homework_1\\AUTOML_HM2\\.venv\\lib\\site-packages\\autogluon\\core\\trainer\\abstract_trainer.py\", line 2135, in _train_single_full\n",
      "    hpo_models, hpo_results = model.hyperparameter_tune(\n",
      "  File \"c:\\JAN_CICHOMSKI\\STUDIA\\STUDIA_SEMESTR_7_2023_ZIMA\\auto_ml\\homeworks\\homework_1\\AUTOML_HM2\\.venv\\lib\\site-packages\\autogluon\\core\\models\\abstract\\abstract_model.py\", line 1466, in hyperparameter_tune\n",
      "    hpo_executor.initialize(hyperparameter_tune_kwargs, default_num_trials=default_num_trials, time_limit=time_limit)\n",
      "  File \"c:\\JAN_CICHOMSKI\\STUDIA\\STUDIA_SEMESTR_7_2023_ZIMA\\auto_ml\\homeworks\\homework_1\\AUTOML_HM2\\.venv\\lib\\site-packages\\autogluon\\core\\hpo\\executors.py\", line 489, in initialize\n",
      "    hyperparameter_tune_kwargs = scheduler_factory(hyperparameter_tune_kwargs, num_trials=num_trials, nthreads_per_trial=\"auto\", ngpus_per_trial=\"auto\")\n",
      "  File \"c:\\JAN_CICHOMSKI\\STUDIA\\STUDIA_SEMESTR_7_2023_ZIMA\\auto_ml\\homeworks\\homework_1\\AUTOML_HM2\\.venv\\lib\\site-packages\\autogluon\\core\\scheduler\\scheduler_factory.py\", line 191, in scheduler_factory\n",
      "    raise ValueError(f\"Required key 'scheduler' is not present in hyperparameter_tune_kwargs: {hyperparameter_tune_kwargs}\")\n",
      "ValueError: Required key 'scheduler' is not present in hyperparameter_tune_kwargs: {'searcher': 'auto', 'time_out': 1200, 'num_trials': 30}\n",
      "Required key 'scheduler' is not present in hyperparameter_tune_kwargs: {'searcher': 'auto', 'time_out': 1200, 'num_trials': 30}\n",
      "Hyperparameter tuning model: LightGBMXT_BAG_L1 ... Tuning model for up to 1.15s of the 49.91s of remaining time.\n",
      "Warning: Exception caused LightGBMXT_BAG_L1 to fail during hyperparameter tuning... Skipping this model.\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\JAN_CICHOMSKI\\STUDIA\\STUDIA_SEMESTR_7_2023_ZIMA\\auto_ml\\homeworks\\homework_1\\AUTOML_HM2\\.venv\\lib\\site-packages\\autogluon\\core\\trainer\\abstract_trainer.py\", line 2135, in _train_single_full\n",
      "    hpo_models, hpo_results = model.hyperparameter_tune(\n",
      "  File \"c:\\JAN_CICHOMSKI\\STUDIA\\STUDIA_SEMESTR_7_2023_ZIMA\\auto_ml\\homeworks\\homework_1\\AUTOML_HM2\\.venv\\lib\\site-packages\\autogluon\\core\\models\\abstract\\abstract_model.py\", line 1466, in hyperparameter_tune\n",
      "    hpo_executor.initialize(hyperparameter_tune_kwargs, default_num_trials=default_num_trials, time_limit=time_limit)\n",
      "  File \"c:\\JAN_CICHOMSKI\\STUDIA\\STUDIA_SEMESTR_7_2023_ZIMA\\auto_ml\\homeworks\\homework_1\\AUTOML_HM2\\.venv\\lib\\site-packages\\autogluon\\core\\hpo\\executors.py\", line 489, in initialize\n",
      "    hyperparameter_tune_kwargs = scheduler_factory(hyperparameter_tune_kwargs, num_trials=num_trials, nthreads_per_trial=\"auto\", ngpus_per_trial=\"auto\")\n",
      "  File \"c:\\JAN_CICHOMSKI\\STUDIA\\STUDIA_SEMESTR_7_2023_ZIMA\\auto_ml\\homeworks\\homework_1\\AUTOML_HM2\\.venv\\lib\\site-packages\\autogluon\\core\\scheduler\\scheduler_factory.py\", line 191, in scheduler_factory\n",
      "    raise ValueError(f\"Required key 'scheduler' is not present in hyperparameter_tune_kwargs: {hyperparameter_tune_kwargs}\")\n",
      "ValueError: Required key 'scheduler' is not present in hyperparameter_tune_kwargs: {'searcher': 'auto', 'time_out': 1200, 'num_trials': 30}\n",
      "Required key 'scheduler' is not present in hyperparameter_tune_kwargs: {'searcher': 'auto', 'time_out': 1200, 'num_trials': 30}\n",
      "Hyperparameter tuning model: LightGBM_BAG_L1 ... Tuning model for up to 1.15s of the 49.9s of remaining time.\n",
      "Warning: Exception caused LightGBM_BAG_L1 to fail during hyperparameter tuning... Skipping this model.\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\JAN_CICHOMSKI\\STUDIA\\STUDIA_SEMESTR_7_2023_ZIMA\\auto_ml\\homeworks\\homework_1\\AUTOML_HM2\\.venv\\lib\\site-packages\\autogluon\\core\\trainer\\abstract_trainer.py\", line 2135, in _train_single_full\n",
      "    hpo_models, hpo_results = model.hyperparameter_tune(\n",
      "  File \"c:\\JAN_CICHOMSKI\\STUDIA\\STUDIA_SEMESTR_7_2023_ZIMA\\auto_ml\\homeworks\\homework_1\\AUTOML_HM2\\.venv\\lib\\site-packages\\autogluon\\core\\models\\abstract\\abstract_model.py\", line 1466, in hyperparameter_tune\n",
      "    hpo_executor.initialize(hyperparameter_tune_kwargs, default_num_trials=default_num_trials, time_limit=time_limit)\n",
      "  File \"c:\\JAN_CICHOMSKI\\STUDIA\\STUDIA_SEMESTR_7_2023_ZIMA\\auto_ml\\homeworks\\homework_1\\AUTOML_HM2\\.venv\\lib\\site-packages\\autogluon\\core\\hpo\\executors.py\", line 489, in initialize\n",
      "    hyperparameter_tune_kwargs = scheduler_factory(hyperparameter_tune_kwargs, num_trials=num_trials, nthreads_per_trial=\"auto\", ngpus_per_trial=\"auto\")\n",
      "  File \"c:\\JAN_CICHOMSKI\\STUDIA\\STUDIA_SEMESTR_7_2023_ZIMA\\auto_ml\\homeworks\\homework_1\\AUTOML_HM2\\.venv\\lib\\site-packages\\autogluon\\core\\scheduler\\scheduler_factory.py\", line 191, in scheduler_factory\n",
      "    raise ValueError(f\"Required key 'scheduler' is not present in hyperparameter_tune_kwargs: {hyperparameter_tune_kwargs}\")\n",
      "ValueError: Required key 'scheduler' is not present in hyperparameter_tune_kwargs: {'searcher': 'auto', 'time_out': 1200, 'num_trials': 30}\n",
      "Required key 'scheduler' is not present in hyperparameter_tune_kwargs: {'searcher': 'auto', 'time_out': 1200, 'num_trials': 30}\n",
      "Hyperparameter tuning model: RandomForestGini_BAG_L1 ... Tuning model for up to 1.15s of the 49.89s of remaining time.\n",
      "Warning: Exception caused RandomForestGini_BAG_L1 to fail during hyperparameter tuning... Skipping this model.\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\JAN_CICHOMSKI\\STUDIA\\STUDIA_SEMESTR_7_2023_ZIMA\\auto_ml\\homeworks\\homework_1\\AUTOML_HM2\\.venv\\lib\\site-packages\\autogluon\\core\\trainer\\abstract_trainer.py\", line 2135, in _train_single_full\n",
      "    hpo_models, hpo_results = model.hyperparameter_tune(\n",
      "  File \"c:\\JAN_CICHOMSKI\\STUDIA\\STUDIA_SEMESTR_7_2023_ZIMA\\auto_ml\\homeworks\\homework_1\\AUTOML_HM2\\.venv\\lib\\site-packages\\autogluon\\core\\models\\abstract\\abstract_model.py\", line 1466, in hyperparameter_tune\n",
      "    hpo_executor.initialize(hyperparameter_tune_kwargs, default_num_trials=default_num_trials, time_limit=time_limit)\n",
      "  File \"c:\\JAN_CICHOMSKI\\STUDIA\\STUDIA_SEMESTR_7_2023_ZIMA\\auto_ml\\homeworks\\homework_1\\AUTOML_HM2\\.venv\\lib\\site-packages\\autogluon\\core\\hpo\\executors.py\", line 489, in initialize\n",
      "    hyperparameter_tune_kwargs = scheduler_factory(hyperparameter_tune_kwargs, num_trials=num_trials, nthreads_per_trial=\"auto\", ngpus_per_trial=\"auto\")\n",
      "  File \"c:\\JAN_CICHOMSKI\\STUDIA\\STUDIA_SEMESTR_7_2023_ZIMA\\auto_ml\\homeworks\\homework_1\\AUTOML_HM2\\.venv\\lib\\site-packages\\autogluon\\core\\scheduler\\scheduler_factory.py\", line 191, in scheduler_factory\n",
      "    raise ValueError(f\"Required key 'scheduler' is not present in hyperparameter_tune_kwargs: {hyperparameter_tune_kwargs}\")\n",
      "ValueError: Required key 'scheduler' is not present in hyperparameter_tune_kwargs: {'searcher': 'auto', 'time_out': 1200, 'num_trials': 30}\n",
      "Required key 'scheduler' is not present in hyperparameter_tune_kwargs: {'searcher': 'auto', 'time_out': 1200, 'num_trials': 30}\n",
      "Hyperparameter tuning model: RandomForestEntr_BAG_L1 ... Tuning model for up to 1.15s of the 49.89s of remaining time.\n",
      "Warning: Exception caused RandomForestEntr_BAG_L1 to fail during hyperparameter tuning... Skipping this model.\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\JAN_CICHOMSKI\\STUDIA\\STUDIA_SEMESTR_7_2023_ZIMA\\auto_ml\\homeworks\\homework_1\\AUTOML_HM2\\.venv\\lib\\site-packages\\autogluon\\core\\trainer\\abstract_trainer.py\", line 2135, in _train_single_full\n",
      "    hpo_models, hpo_results = model.hyperparameter_tune(\n",
      "  File \"c:\\JAN_CICHOMSKI\\STUDIA\\STUDIA_SEMESTR_7_2023_ZIMA\\auto_ml\\homeworks\\homework_1\\AUTOML_HM2\\.venv\\lib\\site-packages\\autogluon\\core\\models\\abstract\\abstract_model.py\", line 1466, in hyperparameter_tune\n",
      "    hpo_executor.initialize(hyperparameter_tune_kwargs, default_num_trials=default_num_trials, time_limit=time_limit)\n",
      "  File \"c:\\JAN_CICHOMSKI\\STUDIA\\STUDIA_SEMESTR_7_2023_ZIMA\\auto_ml\\homeworks\\homework_1\\AUTOML_HM2\\.venv\\lib\\site-packages\\autogluon\\core\\hpo\\executors.py\", line 489, in initialize\n",
      "    hyperparameter_tune_kwargs = scheduler_factory(hyperparameter_tune_kwargs, num_trials=num_trials, nthreads_per_trial=\"auto\", ngpus_per_trial=\"auto\")\n",
      "  File \"c:\\JAN_CICHOMSKI\\STUDIA\\STUDIA_SEMESTR_7_2023_ZIMA\\auto_ml\\homeworks\\homework_1\\AUTOML_HM2\\.venv\\lib\\site-packages\\autogluon\\core\\scheduler\\scheduler_factory.py\", line 191, in scheduler_factory\n",
      "    raise ValueError(f\"Required key 'scheduler' is not present in hyperparameter_tune_kwargs: {hyperparameter_tune_kwargs}\")\n",
      "ValueError: Required key 'scheduler' is not present in hyperparameter_tune_kwargs: {'searcher': 'auto', 'time_out': 1200, 'num_trials': 30}\n",
      "Required key 'scheduler' is not present in hyperparameter_tune_kwargs: {'searcher': 'auto', 'time_out': 1200, 'num_trials': 30}\n",
      "Hyperparameter tuning model: CatBoost_BAG_L1 ... Tuning model for up to 1.15s of the 49.88s of remaining time.\n",
      "Warning: Exception caused CatBoost_BAG_L1 to fail during hyperparameter tuning... Skipping this model.\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\JAN_CICHOMSKI\\STUDIA\\STUDIA_SEMESTR_7_2023_ZIMA\\auto_ml\\homeworks\\homework_1\\AUTOML_HM2\\.venv\\lib\\site-packages\\autogluon\\core\\trainer\\abstract_trainer.py\", line 2135, in _train_single_full\n",
      "    hpo_models, hpo_results = model.hyperparameter_tune(\n",
      "  File \"c:\\JAN_CICHOMSKI\\STUDIA\\STUDIA_SEMESTR_7_2023_ZIMA\\auto_ml\\homeworks\\homework_1\\AUTOML_HM2\\.venv\\lib\\site-packages\\autogluon\\core\\models\\abstract\\abstract_model.py\", line 1466, in hyperparameter_tune\n",
      "    hpo_executor.initialize(hyperparameter_tune_kwargs, default_num_trials=default_num_trials, time_limit=time_limit)\n",
      "  File \"c:\\JAN_CICHOMSKI\\STUDIA\\STUDIA_SEMESTR_7_2023_ZIMA\\auto_ml\\homeworks\\homework_1\\AUTOML_HM2\\.venv\\lib\\site-packages\\autogluon\\core\\hpo\\executors.py\", line 489, in initialize\n",
      "    hyperparameter_tune_kwargs = scheduler_factory(hyperparameter_tune_kwargs, num_trials=num_trials, nthreads_per_trial=\"auto\", ngpus_per_trial=\"auto\")\n",
      "  File \"c:\\JAN_CICHOMSKI\\STUDIA\\STUDIA_SEMESTR_7_2023_ZIMA\\auto_ml\\homeworks\\homework_1\\AUTOML_HM2\\.venv\\lib\\site-packages\\autogluon\\core\\scheduler\\scheduler_factory.py\", line 191, in scheduler_factory\n",
      "    raise ValueError(f\"Required key 'scheduler' is not present in hyperparameter_tune_kwargs: {hyperparameter_tune_kwargs}\")\n",
      "ValueError: Required key 'scheduler' is not present in hyperparameter_tune_kwargs: {'searcher': 'auto', 'time_out': 1200, 'num_trials': 30}\n",
      "Required key 'scheduler' is not present in hyperparameter_tune_kwargs: {'searcher': 'auto', 'time_out': 1200, 'num_trials': 30}\n",
      "Hyperparameter tuning model: ExtraTreesGini_BAG_L1 ... Tuning model for up to 1.15s of the 49.88s of remaining time.\n",
      "Warning: Exception caused ExtraTreesGini_BAG_L1 to fail during hyperparameter tuning... Skipping this model.\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\JAN_CICHOMSKI\\STUDIA\\STUDIA_SEMESTR_7_2023_ZIMA\\auto_ml\\homeworks\\homework_1\\AUTOML_HM2\\.venv\\lib\\site-packages\\autogluon\\core\\trainer\\abstract_trainer.py\", line 2135, in _train_single_full\n",
      "    hpo_models, hpo_results = model.hyperparameter_tune(\n",
      "  File \"c:\\JAN_CICHOMSKI\\STUDIA\\STUDIA_SEMESTR_7_2023_ZIMA\\auto_ml\\homeworks\\homework_1\\AUTOML_HM2\\.venv\\lib\\site-packages\\autogluon\\core\\models\\abstract\\abstract_model.py\", line 1466, in hyperparameter_tune\n",
      "    hpo_executor.initialize(hyperparameter_tune_kwargs, default_num_trials=default_num_trials, time_limit=time_limit)\n",
      "  File \"c:\\JAN_CICHOMSKI\\STUDIA\\STUDIA_SEMESTR_7_2023_ZIMA\\auto_ml\\homeworks\\homework_1\\AUTOML_HM2\\.venv\\lib\\site-packages\\autogluon\\core\\hpo\\executors.py\", line 489, in initialize\n",
      "    hyperparameter_tune_kwargs = scheduler_factory(hyperparameter_tune_kwargs, num_trials=num_trials, nthreads_per_trial=\"auto\", ngpus_per_trial=\"auto\")\n",
      "  File \"c:\\JAN_CICHOMSKI\\STUDIA\\STUDIA_SEMESTR_7_2023_ZIMA\\auto_ml\\homeworks\\homework_1\\AUTOML_HM2\\.venv\\lib\\site-packages\\autogluon\\core\\scheduler\\scheduler_factory.py\", line 191, in scheduler_factory\n",
      "    raise ValueError(f\"Required key 'scheduler' is not present in hyperparameter_tune_kwargs: {hyperparameter_tune_kwargs}\")\n",
      "ValueError: Required key 'scheduler' is not present in hyperparameter_tune_kwargs: {'searcher': 'auto', 'time_out': 1200, 'num_trials': 30}\n",
      "Required key 'scheduler' is not present in hyperparameter_tune_kwargs: {'searcher': 'auto', 'time_out': 1200, 'num_trials': 30}\n",
      "Hyperparameter tuning model: ExtraTreesEntr_BAG_L1 ... Tuning model for up to 1.15s of the 49.88s of remaining time.\n",
      "Warning: Exception caused ExtraTreesEntr_BAG_L1 to fail during hyperparameter tuning... Skipping this model.\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\JAN_CICHOMSKI\\STUDIA\\STUDIA_SEMESTR_7_2023_ZIMA\\auto_ml\\homeworks\\homework_1\\AUTOML_HM2\\.venv\\lib\\site-packages\\autogluon\\core\\trainer\\abstract_trainer.py\", line 2135, in _train_single_full\n",
      "    hpo_models, hpo_results = model.hyperparameter_tune(\n",
      "  File \"c:\\JAN_CICHOMSKI\\STUDIA\\STUDIA_SEMESTR_7_2023_ZIMA\\auto_ml\\homeworks\\homework_1\\AUTOML_HM2\\.venv\\lib\\site-packages\\autogluon\\core\\models\\abstract\\abstract_model.py\", line 1466, in hyperparameter_tune\n",
      "    hpo_executor.initialize(hyperparameter_tune_kwargs, default_num_trials=default_num_trials, time_limit=time_limit)\n",
      "  File \"c:\\JAN_CICHOMSKI\\STUDIA\\STUDIA_SEMESTR_7_2023_ZIMA\\auto_ml\\homeworks\\homework_1\\AUTOML_HM2\\.venv\\lib\\site-packages\\autogluon\\core\\hpo\\executors.py\", line 489, in initialize\n",
      "    hyperparameter_tune_kwargs = scheduler_factory(hyperparameter_tune_kwargs, num_trials=num_trials, nthreads_per_trial=\"auto\", ngpus_per_trial=\"auto\")\n",
      "  File \"c:\\JAN_CICHOMSKI\\STUDIA\\STUDIA_SEMESTR_7_2023_ZIMA\\auto_ml\\homeworks\\homework_1\\AUTOML_HM2\\.venv\\lib\\site-packages\\autogluon\\core\\scheduler\\scheduler_factory.py\", line 191, in scheduler_factory\n",
      "    raise ValueError(f\"Required key 'scheduler' is not present in hyperparameter_tune_kwargs: {hyperparameter_tune_kwargs}\")\n",
      "ValueError: Required key 'scheduler' is not present in hyperparameter_tune_kwargs: {'searcher': 'auto', 'time_out': 1200, 'num_trials': 30}\n",
      "Required key 'scheduler' is not present in hyperparameter_tune_kwargs: {'searcher': 'auto', 'time_out': 1200, 'num_trials': 30}\n",
      "Hyperparameter tuning model: NeuralNetFastAI_BAG_L1 ... Tuning model for up to 1.15s of the 49.88s of remaining time.\n",
      "Warning: Exception caused NeuralNetFastAI_BAG_L1 to fail during hyperparameter tuning... Skipping this model.\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\JAN_CICHOMSKI\\STUDIA\\STUDIA_SEMESTR_7_2023_ZIMA\\auto_ml\\homeworks\\homework_1\\AUTOML_HM2\\.venv\\lib\\site-packages\\autogluon\\core\\trainer\\abstract_trainer.py\", line 2135, in _train_single_full\n",
      "    hpo_models, hpo_results = model.hyperparameter_tune(\n",
      "  File \"c:\\JAN_CICHOMSKI\\STUDIA\\STUDIA_SEMESTR_7_2023_ZIMA\\auto_ml\\homeworks\\homework_1\\AUTOML_HM2\\.venv\\lib\\site-packages\\autogluon\\core\\models\\abstract\\abstract_model.py\", line 1466, in hyperparameter_tune\n",
      "    hpo_executor.initialize(hyperparameter_tune_kwargs, default_num_trials=default_num_trials, time_limit=time_limit)\n",
      "  File \"c:\\JAN_CICHOMSKI\\STUDIA\\STUDIA_SEMESTR_7_2023_ZIMA\\auto_ml\\homeworks\\homework_1\\AUTOML_HM2\\.venv\\lib\\site-packages\\autogluon\\core\\hpo\\executors.py\", line 336, in initialize\n",
      "    hyperparameter_tune_kwargs[\"scheduler\"], hyperparameter_tune_kwargs[\"scheduler\"]\n",
      "KeyError: 'scheduler'\n",
      "'scheduler'\n",
      "Hyperparameter tuning model: XGBoost_BAG_L1 ... Tuning model for up to 1.15s of the 49.86s of remaining time.\n",
      "Warning: Exception caused XGBoost_BAG_L1 to fail during hyperparameter tuning... Skipping this model.\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\JAN_CICHOMSKI\\STUDIA\\STUDIA_SEMESTR_7_2023_ZIMA\\auto_ml\\homeworks\\homework_1\\AUTOML_HM2\\.venv\\lib\\site-packages\\autogluon\\core\\trainer\\abstract_trainer.py\", line 2135, in _train_single_full\n",
      "    hpo_models, hpo_results = model.hyperparameter_tune(\n",
      "  File \"c:\\JAN_CICHOMSKI\\STUDIA\\STUDIA_SEMESTR_7_2023_ZIMA\\auto_ml\\homeworks\\homework_1\\AUTOML_HM2\\.venv\\lib\\site-packages\\autogluon\\core\\models\\abstract\\abstract_model.py\", line 1466, in hyperparameter_tune\n",
      "    hpo_executor.initialize(hyperparameter_tune_kwargs, default_num_trials=default_num_trials, time_limit=time_limit)\n",
      "  File \"c:\\JAN_CICHOMSKI\\STUDIA\\STUDIA_SEMESTR_7_2023_ZIMA\\auto_ml\\homeworks\\homework_1\\AUTOML_HM2\\.venv\\lib\\site-packages\\autogluon\\core\\hpo\\executors.py\", line 489, in initialize\n",
      "    hyperparameter_tune_kwargs = scheduler_factory(hyperparameter_tune_kwargs, num_trials=num_trials, nthreads_per_trial=\"auto\", ngpus_per_trial=\"auto\")\n",
      "  File \"c:\\JAN_CICHOMSKI\\STUDIA\\STUDIA_SEMESTR_7_2023_ZIMA\\auto_ml\\homeworks\\homework_1\\AUTOML_HM2\\.venv\\lib\\site-packages\\autogluon\\core\\scheduler\\scheduler_factory.py\", line 191, in scheduler_factory\n",
      "    raise ValueError(f\"Required key 'scheduler' is not present in hyperparameter_tune_kwargs: {hyperparameter_tune_kwargs}\")\n",
      "ValueError: Required key 'scheduler' is not present in hyperparameter_tune_kwargs: {'searcher': 'auto', 'time_out': 1200, 'num_trials': 30}\n",
      "Required key 'scheduler' is not present in hyperparameter_tune_kwargs: {'searcher': 'auto', 'time_out': 1200, 'num_trials': 30}\n",
      "Hyperparameter tuning model: NeuralNetTorch_BAG_L1 ... Tuning model for up to 1.15s of the 49.86s of remaining time.\n",
      "Warning: Exception caused NeuralNetTorch_BAG_L1 to fail during hyperparameter tuning... Skipping this model.\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\JAN_CICHOMSKI\\STUDIA\\STUDIA_SEMESTR_7_2023_ZIMA\\auto_ml\\homeworks\\homework_1\\AUTOML_HM2\\.venv\\lib\\site-packages\\autogluon\\core\\trainer\\abstract_trainer.py\", line 2135, in _train_single_full\n",
      "    hpo_models, hpo_results = model.hyperparameter_tune(\n",
      "  File \"c:\\JAN_CICHOMSKI\\STUDIA\\STUDIA_SEMESTR_7_2023_ZIMA\\auto_ml\\homeworks\\homework_1\\AUTOML_HM2\\.venv\\lib\\site-packages\\autogluon\\core\\models\\abstract\\abstract_model.py\", line 1466, in hyperparameter_tune\n",
      "    hpo_executor.initialize(hyperparameter_tune_kwargs, default_num_trials=default_num_trials, time_limit=time_limit)\n",
      "  File \"c:\\JAN_CICHOMSKI\\STUDIA\\STUDIA_SEMESTR_7_2023_ZIMA\\auto_ml\\homeworks\\homework_1\\AUTOML_HM2\\.venv\\lib\\site-packages\\autogluon\\core\\hpo\\executors.py\", line 336, in initialize\n",
      "    hyperparameter_tune_kwargs[\"scheduler\"], hyperparameter_tune_kwargs[\"scheduler\"]\n",
      "KeyError: 'scheduler'\n",
      "'scheduler'\n",
      "Fitting model: LightGBMLarge_BAG_L1 ... Training model for up to 1.15s of the 49.86s of remaining time.\n",
      "\tFitting 15 child models (S1F1 - S1F15) | Fitting with ParallelLocalFoldFittingStrategy (8 workers, per: cpus=1, gpus=0, memory=0.45%)\n",
      "\t0.8587\t = Validation score   (balanced_accuracy)\n",
      "\t6.73s\t = Training   runtime\n",
      "\t0.02s\t = Validation runtime\n",
      "Completed 1/25 k-fold bagging repeats ...\n",
      "Fitting model: WeightedEnsemble_L2 ... Training model for up to 49.92s of the 40.4s of remaining time.\n",
      "\tEnsemble Weights: {'LightGBMLarge_BAG_L1': 1.0}\n",
      "\t0.8587\t = Validation score   (balanced_accuracy)\n",
      "\t0.01s\t = Training   runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "Fitting 11 L2 models ...\n",
      "Hyperparameter tuning model: LightGBMXT_BAG_L2 ... Tuning model for up to 1.47s of the 40.37s of remaining time.\n",
      "Warning: Exception caused LightGBMXT_BAG_L2 to fail during hyperparameter tuning... Skipping this model.\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\JAN_CICHOMSKI\\STUDIA\\STUDIA_SEMESTR_7_2023_ZIMA\\auto_ml\\homeworks\\homework_1\\AUTOML_HM2\\.venv\\lib\\site-packages\\autogluon\\core\\trainer\\abstract_trainer.py\", line 2135, in _train_single_full\n",
      "    hpo_models, hpo_results = model.hyperparameter_tune(\n",
      "  File \"c:\\JAN_CICHOMSKI\\STUDIA\\STUDIA_SEMESTR_7_2023_ZIMA\\auto_ml\\homeworks\\homework_1\\AUTOML_HM2\\.venv\\lib\\site-packages\\autogluon\\core\\models\\abstract\\abstract_model.py\", line 1466, in hyperparameter_tune\n",
      "    hpo_executor.initialize(hyperparameter_tune_kwargs, default_num_trials=default_num_trials, time_limit=time_limit)\n",
      "  File \"c:\\JAN_CICHOMSKI\\STUDIA\\STUDIA_SEMESTR_7_2023_ZIMA\\auto_ml\\homeworks\\homework_1\\AUTOML_HM2\\.venv\\lib\\site-packages\\autogluon\\core\\hpo\\executors.py\", line 489, in initialize\n",
      "    hyperparameter_tune_kwargs = scheduler_factory(hyperparameter_tune_kwargs, num_trials=num_trials, nthreads_per_trial=\"auto\", ngpus_per_trial=\"auto\")\n",
      "  File \"c:\\JAN_CICHOMSKI\\STUDIA\\STUDIA_SEMESTR_7_2023_ZIMA\\auto_ml\\homeworks\\homework_1\\AUTOML_HM2\\.venv\\lib\\site-packages\\autogluon\\core\\scheduler\\scheduler_factory.py\", line 191, in scheduler_factory\n",
      "    raise ValueError(f\"Required key 'scheduler' is not present in hyperparameter_tune_kwargs: {hyperparameter_tune_kwargs}\")\n",
      "ValueError: Required key 'scheduler' is not present in hyperparameter_tune_kwargs: {'searcher': 'auto', 'time_out': 1200, 'num_trials': 30}\n",
      "Required key 'scheduler' is not present in hyperparameter_tune_kwargs: {'searcher': 'auto', 'time_out': 1200, 'num_trials': 30}\n",
      "Hyperparameter tuning model: LightGBM_BAG_L2 ... Tuning model for up to 1.47s of the 40.36s of remaining time.\n",
      "Warning: Exception caused LightGBM_BAG_L2 to fail during hyperparameter tuning... Skipping this model.\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\JAN_CICHOMSKI\\STUDIA\\STUDIA_SEMESTR_7_2023_ZIMA\\auto_ml\\homeworks\\homework_1\\AUTOML_HM2\\.venv\\lib\\site-packages\\autogluon\\core\\trainer\\abstract_trainer.py\", line 2135, in _train_single_full\n",
      "    hpo_models, hpo_results = model.hyperparameter_tune(\n",
      "  File \"c:\\JAN_CICHOMSKI\\STUDIA\\STUDIA_SEMESTR_7_2023_ZIMA\\auto_ml\\homeworks\\homework_1\\AUTOML_HM2\\.venv\\lib\\site-packages\\autogluon\\core\\models\\abstract\\abstract_model.py\", line 1466, in hyperparameter_tune\n",
      "    hpo_executor.initialize(hyperparameter_tune_kwargs, default_num_trials=default_num_trials, time_limit=time_limit)\n",
      "  File \"c:\\JAN_CICHOMSKI\\STUDIA\\STUDIA_SEMESTR_7_2023_ZIMA\\auto_ml\\homeworks\\homework_1\\AUTOML_HM2\\.venv\\lib\\site-packages\\autogluon\\core\\hpo\\executors.py\", line 489, in initialize\n",
      "    hyperparameter_tune_kwargs = scheduler_factory(hyperparameter_tune_kwargs, num_trials=num_trials, nthreads_per_trial=\"auto\", ngpus_per_trial=\"auto\")\n",
      "  File \"c:\\JAN_CICHOMSKI\\STUDIA\\STUDIA_SEMESTR_7_2023_ZIMA\\auto_ml\\homeworks\\homework_1\\AUTOML_HM2\\.venv\\lib\\site-packages\\autogluon\\core\\scheduler\\scheduler_factory.py\", line 191, in scheduler_factory\n",
      "    raise ValueError(f\"Required key 'scheduler' is not present in hyperparameter_tune_kwargs: {hyperparameter_tune_kwargs}\")\n",
      "ValueError: Required key 'scheduler' is not present in hyperparameter_tune_kwargs: {'searcher': 'auto', 'time_out': 1200, 'num_trials': 30}\n",
      "Required key 'scheduler' is not present in hyperparameter_tune_kwargs: {'searcher': 'auto', 'time_out': 1200, 'num_trials': 30}\n",
      "Hyperparameter tuning model: RandomForestGini_BAG_L2 ... Tuning model for up to 1.47s of the 40.35s of remaining time.\n",
      "Warning: Exception caused RandomForestGini_BAG_L2 to fail during hyperparameter tuning... Skipping this model.\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\JAN_CICHOMSKI\\STUDIA\\STUDIA_SEMESTR_7_2023_ZIMA\\auto_ml\\homeworks\\homework_1\\AUTOML_HM2\\.venv\\lib\\site-packages\\autogluon\\core\\trainer\\abstract_trainer.py\", line 2135, in _train_single_full\n",
      "    hpo_models, hpo_results = model.hyperparameter_tune(\n",
      "  File \"c:\\JAN_CICHOMSKI\\STUDIA\\STUDIA_SEMESTR_7_2023_ZIMA\\auto_ml\\homeworks\\homework_1\\AUTOML_HM2\\.venv\\lib\\site-packages\\autogluon\\core\\models\\abstract\\abstract_model.py\", line 1466, in hyperparameter_tune\n",
      "    hpo_executor.initialize(hyperparameter_tune_kwargs, default_num_trials=default_num_trials, time_limit=time_limit)\n",
      "  File \"c:\\JAN_CICHOMSKI\\STUDIA\\STUDIA_SEMESTR_7_2023_ZIMA\\auto_ml\\homeworks\\homework_1\\AUTOML_HM2\\.venv\\lib\\site-packages\\autogluon\\core\\hpo\\executors.py\", line 489, in initialize\n",
      "    hyperparameter_tune_kwargs = scheduler_factory(hyperparameter_tune_kwargs, num_trials=num_trials, nthreads_per_trial=\"auto\", ngpus_per_trial=\"auto\")\n",
      "  File \"c:\\JAN_CICHOMSKI\\STUDIA\\STUDIA_SEMESTR_7_2023_ZIMA\\auto_ml\\homeworks\\homework_1\\AUTOML_HM2\\.venv\\lib\\site-packages\\autogluon\\core\\scheduler\\scheduler_factory.py\", line 191, in scheduler_factory\n",
      "    raise ValueError(f\"Required key 'scheduler' is not present in hyperparameter_tune_kwargs: {hyperparameter_tune_kwargs}\")\n",
      "ValueError: Required key 'scheduler' is not present in hyperparameter_tune_kwargs: {'searcher': 'auto', 'time_out': 1200, 'num_trials': 30}\n",
      "Required key 'scheduler' is not present in hyperparameter_tune_kwargs: {'searcher': 'auto', 'time_out': 1200, 'num_trials': 30}\n",
      "Hyperparameter tuning model: RandomForestEntr_BAG_L2 ... Tuning model for up to 1.47s of the 40.34s of remaining time.\n",
      "Warning: Exception caused RandomForestEntr_BAG_L2 to fail during hyperparameter tuning... Skipping this model.\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\JAN_CICHOMSKI\\STUDIA\\STUDIA_SEMESTR_7_2023_ZIMA\\auto_ml\\homeworks\\homework_1\\AUTOML_HM2\\.venv\\lib\\site-packages\\autogluon\\core\\trainer\\abstract_trainer.py\", line 2135, in _train_single_full\n",
      "    hpo_models, hpo_results = model.hyperparameter_tune(\n",
      "  File \"c:\\JAN_CICHOMSKI\\STUDIA\\STUDIA_SEMESTR_7_2023_ZIMA\\auto_ml\\homeworks\\homework_1\\AUTOML_HM2\\.venv\\lib\\site-packages\\autogluon\\core\\models\\abstract\\abstract_model.py\", line 1466, in hyperparameter_tune\n",
      "    hpo_executor.initialize(hyperparameter_tune_kwargs, default_num_trials=default_num_trials, time_limit=time_limit)\n",
      "  File \"c:\\JAN_CICHOMSKI\\STUDIA\\STUDIA_SEMESTR_7_2023_ZIMA\\auto_ml\\homeworks\\homework_1\\AUTOML_HM2\\.venv\\lib\\site-packages\\autogluon\\core\\hpo\\executors.py\", line 489, in initialize\n",
      "    hyperparameter_tune_kwargs = scheduler_factory(hyperparameter_tune_kwargs, num_trials=num_trials, nthreads_per_trial=\"auto\", ngpus_per_trial=\"auto\")\n",
      "  File \"c:\\JAN_CICHOMSKI\\STUDIA\\STUDIA_SEMESTR_7_2023_ZIMA\\auto_ml\\homeworks\\homework_1\\AUTOML_HM2\\.venv\\lib\\site-packages\\autogluon\\core\\scheduler\\scheduler_factory.py\", line 191, in scheduler_factory\n",
      "    raise ValueError(f\"Required key 'scheduler' is not present in hyperparameter_tune_kwargs: {hyperparameter_tune_kwargs}\")\n",
      "ValueError: Required key 'scheduler' is not present in hyperparameter_tune_kwargs: {'searcher': 'auto', 'time_out': 1200, 'num_trials': 30}\n",
      "Required key 'scheduler' is not present in hyperparameter_tune_kwargs: {'searcher': 'auto', 'time_out': 1200, 'num_trials': 30}\n",
      "Hyperparameter tuning model: CatBoost_BAG_L2 ... Tuning model for up to 1.47s of the 40.34s of remaining time.\n",
      "Warning: Exception caused CatBoost_BAG_L2 to fail during hyperparameter tuning... Skipping this model.\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\JAN_CICHOMSKI\\STUDIA\\STUDIA_SEMESTR_7_2023_ZIMA\\auto_ml\\homeworks\\homework_1\\AUTOML_HM2\\.venv\\lib\\site-packages\\autogluon\\core\\trainer\\abstract_trainer.py\", line 2135, in _train_single_full\n",
      "    hpo_models, hpo_results = model.hyperparameter_tune(\n",
      "  File \"c:\\JAN_CICHOMSKI\\STUDIA\\STUDIA_SEMESTR_7_2023_ZIMA\\auto_ml\\homeworks\\homework_1\\AUTOML_HM2\\.venv\\lib\\site-packages\\autogluon\\core\\models\\abstract\\abstract_model.py\", line 1466, in hyperparameter_tune\n",
      "    hpo_executor.initialize(hyperparameter_tune_kwargs, default_num_trials=default_num_trials, time_limit=time_limit)\n",
      "  File \"c:\\JAN_CICHOMSKI\\STUDIA\\STUDIA_SEMESTR_7_2023_ZIMA\\auto_ml\\homeworks\\homework_1\\AUTOML_HM2\\.venv\\lib\\site-packages\\autogluon\\core\\hpo\\executors.py\", line 489, in initialize\n",
      "    hyperparameter_tune_kwargs = scheduler_factory(hyperparameter_tune_kwargs, num_trials=num_trials, nthreads_per_trial=\"auto\", ngpus_per_trial=\"auto\")\n",
      "  File \"c:\\JAN_CICHOMSKI\\STUDIA\\STUDIA_SEMESTR_7_2023_ZIMA\\auto_ml\\homeworks\\homework_1\\AUTOML_HM2\\.venv\\lib\\site-packages\\autogluon\\core\\scheduler\\scheduler_factory.py\", line 191, in scheduler_factory\n",
      "    raise ValueError(f\"Required key 'scheduler' is not present in hyperparameter_tune_kwargs: {hyperparameter_tune_kwargs}\")\n",
      "ValueError: Required key 'scheduler' is not present in hyperparameter_tune_kwargs: {'searcher': 'auto', 'time_out': 1200, 'num_trials': 30}\n",
      "Required key 'scheduler' is not present in hyperparameter_tune_kwargs: {'searcher': 'auto', 'time_out': 1200, 'num_trials': 30}\n",
      "Hyperparameter tuning model: ExtraTreesGini_BAG_L2 ... Tuning model for up to 1.47s of the 40.33s of remaining time.\n",
      "Warning: Exception caused ExtraTreesGini_BAG_L2 to fail during hyperparameter tuning... Skipping this model.\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\JAN_CICHOMSKI\\STUDIA\\STUDIA_SEMESTR_7_2023_ZIMA\\auto_ml\\homeworks\\homework_1\\AUTOML_HM2\\.venv\\lib\\site-packages\\autogluon\\core\\trainer\\abstract_trainer.py\", line 2135, in _train_single_full\n",
      "    hpo_models, hpo_results = model.hyperparameter_tune(\n",
      "  File \"c:\\JAN_CICHOMSKI\\STUDIA\\STUDIA_SEMESTR_7_2023_ZIMA\\auto_ml\\homeworks\\homework_1\\AUTOML_HM2\\.venv\\lib\\site-packages\\autogluon\\core\\models\\abstract\\abstract_model.py\", line 1466, in hyperparameter_tune\n",
      "    hpo_executor.initialize(hyperparameter_tune_kwargs, default_num_trials=default_num_trials, time_limit=time_limit)\n",
      "  File \"c:\\JAN_CICHOMSKI\\STUDIA\\STUDIA_SEMESTR_7_2023_ZIMA\\auto_ml\\homeworks\\homework_1\\AUTOML_HM2\\.venv\\lib\\site-packages\\autogluon\\core\\hpo\\executors.py\", line 489, in initialize\n",
      "    hyperparameter_tune_kwargs = scheduler_factory(hyperparameter_tune_kwargs, num_trials=num_trials, nthreads_per_trial=\"auto\", ngpus_per_trial=\"auto\")\n",
      "  File \"c:\\JAN_CICHOMSKI\\STUDIA\\STUDIA_SEMESTR_7_2023_ZIMA\\auto_ml\\homeworks\\homework_1\\AUTOML_HM2\\.venv\\lib\\site-packages\\autogluon\\core\\scheduler\\scheduler_factory.py\", line 191, in scheduler_factory\n",
      "    raise ValueError(f\"Required key 'scheduler' is not present in hyperparameter_tune_kwargs: {hyperparameter_tune_kwargs}\")\n",
      "ValueError: Required key 'scheduler' is not present in hyperparameter_tune_kwargs: {'searcher': 'auto', 'time_out': 1200, 'num_trials': 30}\n",
      "Required key 'scheduler' is not present in hyperparameter_tune_kwargs: {'searcher': 'auto', 'time_out': 1200, 'num_trials': 30}\n",
      "Hyperparameter tuning model: ExtraTreesEntr_BAG_L2 ... Tuning model for up to 1.47s of the 40.32s of remaining time.\n",
      "Warning: Exception caused ExtraTreesEntr_BAG_L2 to fail during hyperparameter tuning... Skipping this model.\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\JAN_CICHOMSKI\\STUDIA\\STUDIA_SEMESTR_7_2023_ZIMA\\auto_ml\\homeworks\\homework_1\\AUTOML_HM2\\.venv\\lib\\site-packages\\autogluon\\core\\trainer\\abstract_trainer.py\", line 2135, in _train_single_full\n",
      "    hpo_models, hpo_results = model.hyperparameter_tune(\n",
      "  File \"c:\\JAN_CICHOMSKI\\STUDIA\\STUDIA_SEMESTR_7_2023_ZIMA\\auto_ml\\homeworks\\homework_1\\AUTOML_HM2\\.venv\\lib\\site-packages\\autogluon\\core\\models\\abstract\\abstract_model.py\", line 1466, in hyperparameter_tune\n",
      "    hpo_executor.initialize(hyperparameter_tune_kwargs, default_num_trials=default_num_trials, time_limit=time_limit)\n",
      "  File \"c:\\JAN_CICHOMSKI\\STUDIA\\STUDIA_SEMESTR_7_2023_ZIMA\\auto_ml\\homeworks\\homework_1\\AUTOML_HM2\\.venv\\lib\\site-packages\\autogluon\\core\\hpo\\executors.py\", line 489, in initialize\n",
      "    hyperparameter_tune_kwargs = scheduler_factory(hyperparameter_tune_kwargs, num_trials=num_trials, nthreads_per_trial=\"auto\", ngpus_per_trial=\"auto\")\n",
      "  File \"c:\\JAN_CICHOMSKI\\STUDIA\\STUDIA_SEMESTR_7_2023_ZIMA\\auto_ml\\homeworks\\homework_1\\AUTOML_HM2\\.venv\\lib\\site-packages\\autogluon\\core\\scheduler\\scheduler_factory.py\", line 191, in scheduler_factory\n",
      "    raise ValueError(f\"Required key 'scheduler' is not present in hyperparameter_tune_kwargs: {hyperparameter_tune_kwargs}\")\n",
      "ValueError: Required key 'scheduler' is not present in hyperparameter_tune_kwargs: {'searcher': 'auto', 'time_out': 1200, 'num_trials': 30}\n",
      "Required key 'scheduler' is not present in hyperparameter_tune_kwargs: {'searcher': 'auto', 'time_out': 1200, 'num_trials': 30}\n",
      "Hyperparameter tuning model: NeuralNetFastAI_BAG_L2 ... Tuning model for up to 1.47s of the 40.32s of remaining time.\n",
      "Warning: Exception caused NeuralNetFastAI_BAG_L2 to fail during hyperparameter tuning... Skipping this model.\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\JAN_CICHOMSKI\\STUDIA\\STUDIA_SEMESTR_7_2023_ZIMA\\auto_ml\\homeworks\\homework_1\\AUTOML_HM2\\.venv\\lib\\site-packages\\autogluon\\core\\trainer\\abstract_trainer.py\", line 2135, in _train_single_full\n",
      "    hpo_models, hpo_results = model.hyperparameter_tune(\n",
      "  File \"c:\\JAN_CICHOMSKI\\STUDIA\\STUDIA_SEMESTR_7_2023_ZIMA\\auto_ml\\homeworks\\homework_1\\AUTOML_HM2\\.venv\\lib\\site-packages\\autogluon\\core\\models\\abstract\\abstract_model.py\", line 1466, in hyperparameter_tune\n",
      "    hpo_executor.initialize(hyperparameter_tune_kwargs, default_num_trials=default_num_trials, time_limit=time_limit)\n",
      "  File \"c:\\JAN_CICHOMSKI\\STUDIA\\STUDIA_SEMESTR_7_2023_ZIMA\\auto_ml\\homeworks\\homework_1\\AUTOML_HM2\\.venv\\lib\\site-packages\\autogluon\\core\\hpo\\executors.py\", line 336, in initialize\n",
      "    hyperparameter_tune_kwargs[\"scheduler\"], hyperparameter_tune_kwargs[\"scheduler\"]\n",
      "KeyError: 'scheduler'\n",
      "'scheduler'\n",
      "Hyperparameter tuning model: XGBoost_BAG_L2 ... Tuning model for up to 1.47s of the 40.3s of remaining time.\n",
      "Warning: Exception caused XGBoost_BAG_L2 to fail during hyperparameter tuning... Skipping this model.\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\JAN_CICHOMSKI\\STUDIA\\STUDIA_SEMESTR_7_2023_ZIMA\\auto_ml\\homeworks\\homework_1\\AUTOML_HM2\\.venv\\lib\\site-packages\\autogluon\\core\\trainer\\abstract_trainer.py\", line 2135, in _train_single_full\n",
      "    hpo_models, hpo_results = model.hyperparameter_tune(\n",
      "  File \"c:\\JAN_CICHOMSKI\\STUDIA\\STUDIA_SEMESTR_7_2023_ZIMA\\auto_ml\\homeworks\\homework_1\\AUTOML_HM2\\.venv\\lib\\site-packages\\autogluon\\core\\models\\abstract\\abstract_model.py\", line 1466, in hyperparameter_tune\n",
      "    hpo_executor.initialize(hyperparameter_tune_kwargs, default_num_trials=default_num_trials, time_limit=time_limit)\n",
      "  File \"c:\\JAN_CICHOMSKI\\STUDIA\\STUDIA_SEMESTR_7_2023_ZIMA\\auto_ml\\homeworks\\homework_1\\AUTOML_HM2\\.venv\\lib\\site-packages\\autogluon\\core\\hpo\\executors.py\", line 489, in initialize\n",
      "    hyperparameter_tune_kwargs = scheduler_factory(hyperparameter_tune_kwargs, num_trials=num_trials, nthreads_per_trial=\"auto\", ngpus_per_trial=\"auto\")\n",
      "  File \"c:\\JAN_CICHOMSKI\\STUDIA\\STUDIA_SEMESTR_7_2023_ZIMA\\auto_ml\\homeworks\\homework_1\\AUTOML_HM2\\.venv\\lib\\site-packages\\autogluon\\core\\scheduler\\scheduler_factory.py\", line 191, in scheduler_factory\n",
      "    raise ValueError(f\"Required key 'scheduler' is not present in hyperparameter_tune_kwargs: {hyperparameter_tune_kwargs}\")\n",
      "ValueError: Required key 'scheduler' is not present in hyperparameter_tune_kwargs: {'searcher': 'auto', 'time_out': 1200, 'num_trials': 30}\n",
      "Required key 'scheduler' is not present in hyperparameter_tune_kwargs: {'searcher': 'auto', 'time_out': 1200, 'num_trials': 30}\n",
      "Hyperparameter tuning model: NeuralNetTorch_BAG_L2 ... Tuning model for up to 1.47s of the 40.3s of remaining time.\n",
      "Warning: Exception caused NeuralNetTorch_BAG_L2 to fail during hyperparameter tuning... Skipping this model.\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\JAN_CICHOMSKI\\STUDIA\\STUDIA_SEMESTR_7_2023_ZIMA\\auto_ml\\homeworks\\homework_1\\AUTOML_HM2\\.venv\\lib\\site-packages\\autogluon\\core\\trainer\\abstract_trainer.py\", line 2135, in _train_single_full\n",
      "    hpo_models, hpo_results = model.hyperparameter_tune(\n",
      "  File \"c:\\JAN_CICHOMSKI\\STUDIA\\STUDIA_SEMESTR_7_2023_ZIMA\\auto_ml\\homeworks\\homework_1\\AUTOML_HM2\\.venv\\lib\\site-packages\\autogluon\\core\\models\\abstract\\abstract_model.py\", line 1466, in hyperparameter_tune\n",
      "    hpo_executor.initialize(hyperparameter_tune_kwargs, default_num_trials=default_num_trials, time_limit=time_limit)\n",
      "  File \"c:\\JAN_CICHOMSKI\\STUDIA\\STUDIA_SEMESTR_7_2023_ZIMA\\auto_ml\\homeworks\\homework_1\\AUTOML_HM2\\.venv\\lib\\site-packages\\autogluon\\core\\hpo\\executors.py\", line 336, in initialize\n",
      "    hyperparameter_tune_kwargs[\"scheduler\"], hyperparameter_tune_kwargs[\"scheduler\"]\n",
      "KeyError: 'scheduler'\n",
      "'scheduler'\n",
      "Fitting model: LightGBMLarge_BAG_L2 ... Training model for up to 1.47s of the 40.29s of remaining time.\n",
      "\tFitting 15 child models (S1F1 - S1F15) | Fitting with ParallelLocalFoldFittingStrategy (8 workers, per: cpus=1, gpus=0, memory=0.50%)\n",
      "\t0.8718\t = Validation score   (balanced_accuracy)\n",
      "\t8.53s\t = Training   runtime\n",
      "\t0.05s\t = Validation runtime\n",
      "Completed 1/25 k-fold bagging repeats ...\n",
      "Fitting model: WeightedEnsemble_L3 ... Training model for up to 49.92s of the 26.46s of remaining time.\n",
      "\tEnsemble Weights: {'LightGBMLarge_BAG_L2': 1.0}\n",
      "\t0.8718\t = Validation score   (balanced_accuracy)\n",
      "\t0.02s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "Fitting 11 L3 models ...\n",
      "Hyperparameter tuning model: LightGBMXT_BAG_L3 ... Tuning model for up to 1.44s of the 26.42s of remaining time.\n",
      "Warning: Exception caused LightGBMXT_BAG_L3 to fail during hyperparameter tuning... Skipping this model.\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\JAN_CICHOMSKI\\STUDIA\\STUDIA_SEMESTR_7_2023_ZIMA\\auto_ml\\homeworks\\homework_1\\AUTOML_HM2\\.venv\\lib\\site-packages\\autogluon\\core\\trainer\\abstract_trainer.py\", line 2135, in _train_single_full\n",
      "    hpo_models, hpo_results = model.hyperparameter_tune(\n",
      "  File \"c:\\JAN_CICHOMSKI\\STUDIA\\STUDIA_SEMESTR_7_2023_ZIMA\\auto_ml\\homeworks\\homework_1\\AUTOML_HM2\\.venv\\lib\\site-packages\\autogluon\\core\\models\\abstract\\abstract_model.py\", line 1466, in hyperparameter_tune\n",
      "    hpo_executor.initialize(hyperparameter_tune_kwargs, default_num_trials=default_num_trials, time_limit=time_limit)\n",
      "  File \"c:\\JAN_CICHOMSKI\\STUDIA\\STUDIA_SEMESTR_7_2023_ZIMA\\auto_ml\\homeworks\\homework_1\\AUTOML_HM2\\.venv\\lib\\site-packages\\autogluon\\core\\hpo\\executors.py\", line 489, in initialize\n",
      "    hyperparameter_tune_kwargs = scheduler_factory(hyperparameter_tune_kwargs, num_trials=num_trials, nthreads_per_trial=\"auto\", ngpus_per_trial=\"auto\")\n",
      "  File \"c:\\JAN_CICHOMSKI\\STUDIA\\STUDIA_SEMESTR_7_2023_ZIMA\\auto_ml\\homeworks\\homework_1\\AUTOML_HM2\\.venv\\lib\\site-packages\\autogluon\\core\\scheduler\\scheduler_factory.py\", line 191, in scheduler_factory\n",
      "    raise ValueError(f\"Required key 'scheduler' is not present in hyperparameter_tune_kwargs: {hyperparameter_tune_kwargs}\")\n",
      "ValueError: Required key 'scheduler' is not present in hyperparameter_tune_kwargs: {'searcher': 'auto', 'time_out': 1200, 'num_trials': 30}\n",
      "Required key 'scheduler' is not present in hyperparameter_tune_kwargs: {'searcher': 'auto', 'time_out': 1200, 'num_trials': 30}\n",
      "Hyperparameter tuning model: LightGBM_BAG_L3 ... Tuning model for up to 1.44s of the 26.41s of remaining time.\n",
      "Warning: Exception caused LightGBM_BAG_L3 to fail during hyperparameter tuning... Skipping this model.\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\JAN_CICHOMSKI\\STUDIA\\STUDIA_SEMESTR_7_2023_ZIMA\\auto_ml\\homeworks\\homework_1\\AUTOML_HM2\\.venv\\lib\\site-packages\\autogluon\\core\\trainer\\abstract_trainer.py\", line 2135, in _train_single_full\n",
      "    hpo_models, hpo_results = model.hyperparameter_tune(\n",
      "  File \"c:\\JAN_CICHOMSKI\\STUDIA\\STUDIA_SEMESTR_7_2023_ZIMA\\auto_ml\\homeworks\\homework_1\\AUTOML_HM2\\.venv\\lib\\site-packages\\autogluon\\core\\models\\abstract\\abstract_model.py\", line 1466, in hyperparameter_tune\n",
      "    hpo_executor.initialize(hyperparameter_tune_kwargs, default_num_trials=default_num_trials, time_limit=time_limit)\n",
      "  File \"c:\\JAN_CICHOMSKI\\STUDIA\\STUDIA_SEMESTR_7_2023_ZIMA\\auto_ml\\homeworks\\homework_1\\AUTOML_HM2\\.venv\\lib\\site-packages\\autogluon\\core\\hpo\\executors.py\", line 489, in initialize\n",
      "    hyperparameter_tune_kwargs = scheduler_factory(hyperparameter_tune_kwargs, num_trials=num_trials, nthreads_per_trial=\"auto\", ngpus_per_trial=\"auto\")\n",
      "  File \"c:\\JAN_CICHOMSKI\\STUDIA\\STUDIA_SEMESTR_7_2023_ZIMA\\auto_ml\\homeworks\\homework_1\\AUTOML_HM2\\.venv\\lib\\site-packages\\autogluon\\core\\scheduler\\scheduler_factory.py\", line 191, in scheduler_factory\n",
      "    raise ValueError(f\"Required key 'scheduler' is not present in hyperparameter_tune_kwargs: {hyperparameter_tune_kwargs}\")\n",
      "ValueError: Required key 'scheduler' is not present in hyperparameter_tune_kwargs: {'searcher': 'auto', 'time_out': 1200, 'num_trials': 30}\n",
      "Required key 'scheduler' is not present in hyperparameter_tune_kwargs: {'searcher': 'auto', 'time_out': 1200, 'num_trials': 30}\n",
      "Hyperparameter tuning model: RandomForestGini_BAG_L3 ... Tuning model for up to 1.44s of the 26.4s of remaining time.\n",
      "Warning: Exception caused RandomForestGini_BAG_L3 to fail during hyperparameter tuning... Skipping this model.\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\JAN_CICHOMSKI\\STUDIA\\STUDIA_SEMESTR_7_2023_ZIMA\\auto_ml\\homeworks\\homework_1\\AUTOML_HM2\\.venv\\lib\\site-packages\\autogluon\\core\\trainer\\abstract_trainer.py\", line 2135, in _train_single_full\n",
      "    hpo_models, hpo_results = model.hyperparameter_tune(\n",
      "  File \"c:\\JAN_CICHOMSKI\\STUDIA\\STUDIA_SEMESTR_7_2023_ZIMA\\auto_ml\\homeworks\\homework_1\\AUTOML_HM2\\.venv\\lib\\site-packages\\autogluon\\core\\models\\abstract\\abstract_model.py\", line 1466, in hyperparameter_tune\n",
      "    hpo_executor.initialize(hyperparameter_tune_kwargs, default_num_trials=default_num_trials, time_limit=time_limit)\n",
      "  File \"c:\\JAN_CICHOMSKI\\STUDIA\\STUDIA_SEMESTR_7_2023_ZIMA\\auto_ml\\homeworks\\homework_1\\AUTOML_HM2\\.venv\\lib\\site-packages\\autogluon\\core\\hpo\\executors.py\", line 489, in initialize\n",
      "    hyperparameter_tune_kwargs = scheduler_factory(hyperparameter_tune_kwargs, num_trials=num_trials, nthreads_per_trial=\"auto\", ngpus_per_trial=\"auto\")\n",
      "  File \"c:\\JAN_CICHOMSKI\\STUDIA\\STUDIA_SEMESTR_7_2023_ZIMA\\auto_ml\\homeworks\\homework_1\\AUTOML_HM2\\.venv\\lib\\site-packages\\autogluon\\core\\scheduler\\scheduler_factory.py\", line 191, in scheduler_factory\n",
      "    raise ValueError(f\"Required key 'scheduler' is not present in hyperparameter_tune_kwargs: {hyperparameter_tune_kwargs}\")\n",
      "ValueError: Required key 'scheduler' is not present in hyperparameter_tune_kwargs: {'searcher': 'auto', 'time_out': 1200, 'num_trials': 30}\n",
      "Required key 'scheduler' is not present in hyperparameter_tune_kwargs: {'searcher': 'auto', 'time_out': 1200, 'num_trials': 30}\n",
      "Hyperparameter tuning model: RandomForestEntr_BAG_L3 ... Tuning model for up to 1.44s of the 26.39s of remaining time.\n",
      "Warning: Exception caused RandomForestEntr_BAG_L3 to fail during hyperparameter tuning... Skipping this model.\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\JAN_CICHOMSKI\\STUDIA\\STUDIA_SEMESTR_7_2023_ZIMA\\auto_ml\\homeworks\\homework_1\\AUTOML_HM2\\.venv\\lib\\site-packages\\autogluon\\core\\trainer\\abstract_trainer.py\", line 2135, in _train_single_full\n",
      "    hpo_models, hpo_results = model.hyperparameter_tune(\n",
      "  File \"c:\\JAN_CICHOMSKI\\STUDIA\\STUDIA_SEMESTR_7_2023_ZIMA\\auto_ml\\homeworks\\homework_1\\AUTOML_HM2\\.venv\\lib\\site-packages\\autogluon\\core\\models\\abstract\\abstract_model.py\", line 1466, in hyperparameter_tune\n",
      "    hpo_executor.initialize(hyperparameter_tune_kwargs, default_num_trials=default_num_trials, time_limit=time_limit)\n",
      "  File \"c:\\JAN_CICHOMSKI\\STUDIA\\STUDIA_SEMESTR_7_2023_ZIMA\\auto_ml\\homeworks\\homework_1\\AUTOML_HM2\\.venv\\lib\\site-packages\\autogluon\\core\\hpo\\executors.py\", line 489, in initialize\n",
      "    hyperparameter_tune_kwargs = scheduler_factory(hyperparameter_tune_kwargs, num_trials=num_trials, nthreads_per_trial=\"auto\", ngpus_per_trial=\"auto\")\n",
      "  File \"c:\\JAN_CICHOMSKI\\STUDIA\\STUDIA_SEMESTR_7_2023_ZIMA\\auto_ml\\homeworks\\homework_1\\AUTOML_HM2\\.venv\\lib\\site-packages\\autogluon\\core\\scheduler\\scheduler_factory.py\", line 191, in scheduler_factory\n",
      "    raise ValueError(f\"Required key 'scheduler' is not present in hyperparameter_tune_kwargs: {hyperparameter_tune_kwargs}\")\n",
      "ValueError: Required key 'scheduler' is not present in hyperparameter_tune_kwargs: {'searcher': 'auto', 'time_out': 1200, 'num_trials': 30}\n",
      "Required key 'scheduler' is not present in hyperparameter_tune_kwargs: {'searcher': 'auto', 'time_out': 1200, 'num_trials': 30}\n",
      "Hyperparameter tuning model: CatBoost_BAG_L3 ... Tuning model for up to 1.44s of the 26.38s of remaining time.\n",
      "Warning: Exception caused CatBoost_BAG_L3 to fail during hyperparameter tuning... Skipping this model.\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\JAN_CICHOMSKI\\STUDIA\\STUDIA_SEMESTR_7_2023_ZIMA\\auto_ml\\homeworks\\homework_1\\AUTOML_HM2\\.venv\\lib\\site-packages\\autogluon\\core\\trainer\\abstract_trainer.py\", line 2135, in _train_single_full\n",
      "    hpo_models, hpo_results = model.hyperparameter_tune(\n",
      "  File \"c:\\JAN_CICHOMSKI\\STUDIA\\STUDIA_SEMESTR_7_2023_ZIMA\\auto_ml\\homeworks\\homework_1\\AUTOML_HM2\\.venv\\lib\\site-packages\\autogluon\\core\\models\\abstract\\abstract_model.py\", line 1466, in hyperparameter_tune\n",
      "    hpo_executor.initialize(hyperparameter_tune_kwargs, default_num_trials=default_num_trials, time_limit=time_limit)\n",
      "  File \"c:\\JAN_CICHOMSKI\\STUDIA\\STUDIA_SEMESTR_7_2023_ZIMA\\auto_ml\\homeworks\\homework_1\\AUTOML_HM2\\.venv\\lib\\site-packages\\autogluon\\core\\hpo\\executors.py\", line 489, in initialize\n",
      "    hyperparameter_tune_kwargs = scheduler_factory(hyperparameter_tune_kwargs, num_trials=num_trials, nthreads_per_trial=\"auto\", ngpus_per_trial=\"auto\")\n",
      "  File \"c:\\JAN_CICHOMSKI\\STUDIA\\STUDIA_SEMESTR_7_2023_ZIMA\\auto_ml\\homeworks\\homework_1\\AUTOML_HM2\\.venv\\lib\\site-packages\\autogluon\\core\\scheduler\\scheduler_factory.py\", line 191, in scheduler_factory\n",
      "    raise ValueError(f\"Required key 'scheduler' is not present in hyperparameter_tune_kwargs: {hyperparameter_tune_kwargs}\")\n",
      "ValueError: Required key 'scheduler' is not present in hyperparameter_tune_kwargs: {'searcher': 'auto', 'time_out': 1200, 'num_trials': 30}\n",
      "Required key 'scheduler' is not present in hyperparameter_tune_kwargs: {'searcher': 'auto', 'time_out': 1200, 'num_trials': 30}\n",
      "Hyperparameter tuning model: ExtraTreesGini_BAG_L3 ... Tuning model for up to 1.44s of the 26.37s of remaining time.\n",
      "Warning: Exception caused ExtraTreesGini_BAG_L3 to fail during hyperparameter tuning... Skipping this model.\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\JAN_CICHOMSKI\\STUDIA\\STUDIA_SEMESTR_7_2023_ZIMA\\auto_ml\\homeworks\\homework_1\\AUTOML_HM2\\.venv\\lib\\site-packages\\autogluon\\core\\trainer\\abstract_trainer.py\", line 2135, in _train_single_full\n",
      "    hpo_models, hpo_results = model.hyperparameter_tune(\n",
      "  File \"c:\\JAN_CICHOMSKI\\STUDIA\\STUDIA_SEMESTR_7_2023_ZIMA\\auto_ml\\homeworks\\homework_1\\AUTOML_HM2\\.venv\\lib\\site-packages\\autogluon\\core\\models\\abstract\\abstract_model.py\", line 1466, in hyperparameter_tune\n",
      "    hpo_executor.initialize(hyperparameter_tune_kwargs, default_num_trials=default_num_trials, time_limit=time_limit)\n",
      "  File \"c:\\JAN_CICHOMSKI\\STUDIA\\STUDIA_SEMESTR_7_2023_ZIMA\\auto_ml\\homeworks\\homework_1\\AUTOML_HM2\\.venv\\lib\\site-packages\\autogluon\\core\\hpo\\executors.py\", line 489, in initialize\n",
      "    hyperparameter_tune_kwargs = scheduler_factory(hyperparameter_tune_kwargs, num_trials=num_trials, nthreads_per_trial=\"auto\", ngpus_per_trial=\"auto\")\n",
      "  File \"c:\\JAN_CICHOMSKI\\STUDIA\\STUDIA_SEMESTR_7_2023_ZIMA\\auto_ml\\homeworks\\homework_1\\AUTOML_HM2\\.venv\\lib\\site-packages\\autogluon\\core\\scheduler\\scheduler_factory.py\", line 191, in scheduler_factory\n",
      "    raise ValueError(f\"Required key 'scheduler' is not present in hyperparameter_tune_kwargs: {hyperparameter_tune_kwargs}\")\n",
      "ValueError: Required key 'scheduler' is not present in hyperparameter_tune_kwargs: {'searcher': 'auto', 'time_out': 1200, 'num_trials': 30}\n",
      "Required key 'scheduler' is not present in hyperparameter_tune_kwargs: {'searcher': 'auto', 'time_out': 1200, 'num_trials': 30}\n",
      "Hyperparameter tuning model: ExtraTreesEntr_BAG_L3 ... Tuning model for up to 1.44s of the 26.36s of remaining time.\n",
      "Warning: Exception caused ExtraTreesEntr_BAG_L3 to fail during hyperparameter tuning... Skipping this model.\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\JAN_CICHOMSKI\\STUDIA\\STUDIA_SEMESTR_7_2023_ZIMA\\auto_ml\\homeworks\\homework_1\\AUTOML_HM2\\.venv\\lib\\site-packages\\autogluon\\core\\trainer\\abstract_trainer.py\", line 2135, in _train_single_full\n",
      "    hpo_models, hpo_results = model.hyperparameter_tune(\n",
      "  File \"c:\\JAN_CICHOMSKI\\STUDIA\\STUDIA_SEMESTR_7_2023_ZIMA\\auto_ml\\homeworks\\homework_1\\AUTOML_HM2\\.venv\\lib\\site-packages\\autogluon\\core\\models\\abstract\\abstract_model.py\", line 1466, in hyperparameter_tune\n",
      "    hpo_executor.initialize(hyperparameter_tune_kwargs, default_num_trials=default_num_trials, time_limit=time_limit)\n",
      "  File \"c:\\JAN_CICHOMSKI\\STUDIA\\STUDIA_SEMESTR_7_2023_ZIMA\\auto_ml\\homeworks\\homework_1\\AUTOML_HM2\\.venv\\lib\\site-packages\\autogluon\\core\\hpo\\executors.py\", line 489, in initialize\n",
      "    hyperparameter_tune_kwargs = scheduler_factory(hyperparameter_tune_kwargs, num_trials=num_trials, nthreads_per_trial=\"auto\", ngpus_per_trial=\"auto\")\n",
      "  File \"c:\\JAN_CICHOMSKI\\STUDIA\\STUDIA_SEMESTR_7_2023_ZIMA\\auto_ml\\homeworks\\homework_1\\AUTOML_HM2\\.venv\\lib\\site-packages\\autogluon\\core\\scheduler\\scheduler_factory.py\", line 191, in scheduler_factory\n",
      "    raise ValueError(f\"Required key 'scheduler' is not present in hyperparameter_tune_kwargs: {hyperparameter_tune_kwargs}\")\n",
      "ValueError: Required key 'scheduler' is not present in hyperparameter_tune_kwargs: {'searcher': 'auto', 'time_out': 1200, 'num_trials': 30}\n",
      "Required key 'scheduler' is not present in hyperparameter_tune_kwargs: {'searcher': 'auto', 'time_out': 1200, 'num_trials': 30}\n",
      "Hyperparameter tuning model: NeuralNetFastAI_BAG_L3 ... Tuning model for up to 1.44s of the 26.35s of remaining time.\n",
      "Warning: Exception caused NeuralNetFastAI_BAG_L3 to fail during hyperparameter tuning... Skipping this model.\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\JAN_CICHOMSKI\\STUDIA\\STUDIA_SEMESTR_7_2023_ZIMA\\auto_ml\\homeworks\\homework_1\\AUTOML_HM2\\.venv\\lib\\site-packages\\autogluon\\core\\trainer\\abstract_trainer.py\", line 2135, in _train_single_full\n",
      "    hpo_models, hpo_results = model.hyperparameter_tune(\n",
      "  File \"c:\\JAN_CICHOMSKI\\STUDIA\\STUDIA_SEMESTR_7_2023_ZIMA\\auto_ml\\homeworks\\homework_1\\AUTOML_HM2\\.venv\\lib\\site-packages\\autogluon\\core\\models\\abstract\\abstract_model.py\", line 1466, in hyperparameter_tune\n",
      "    hpo_executor.initialize(hyperparameter_tune_kwargs, default_num_trials=default_num_trials, time_limit=time_limit)\n",
      "  File \"c:\\JAN_CICHOMSKI\\STUDIA\\STUDIA_SEMESTR_7_2023_ZIMA\\auto_ml\\homeworks\\homework_1\\AUTOML_HM2\\.venv\\lib\\site-packages\\autogluon\\core\\hpo\\executors.py\", line 336, in initialize\n",
      "    hyperparameter_tune_kwargs[\"scheduler\"], hyperparameter_tune_kwargs[\"scheduler\"]\n",
      "KeyError: 'scheduler'\n",
      "'scheduler'\n",
      "Hyperparameter tuning model: XGBoost_BAG_L3 ... Tuning model for up to 1.44s of the 26.34s of remaining time.\n",
      "Warning: Exception caused XGBoost_BAG_L3 to fail during hyperparameter tuning... Skipping this model.\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\JAN_CICHOMSKI\\STUDIA\\STUDIA_SEMESTR_7_2023_ZIMA\\auto_ml\\homeworks\\homework_1\\AUTOML_HM2\\.venv\\lib\\site-packages\\autogluon\\core\\trainer\\abstract_trainer.py\", line 2135, in _train_single_full\n",
      "    hpo_models, hpo_results = model.hyperparameter_tune(\n",
      "  File \"c:\\JAN_CICHOMSKI\\STUDIA\\STUDIA_SEMESTR_7_2023_ZIMA\\auto_ml\\homeworks\\homework_1\\AUTOML_HM2\\.venv\\lib\\site-packages\\autogluon\\core\\models\\abstract\\abstract_model.py\", line 1466, in hyperparameter_tune\n",
      "    hpo_executor.initialize(hyperparameter_tune_kwargs, default_num_trials=default_num_trials, time_limit=time_limit)\n",
      "  File \"c:\\JAN_CICHOMSKI\\STUDIA\\STUDIA_SEMESTR_7_2023_ZIMA\\auto_ml\\homeworks\\homework_1\\AUTOML_HM2\\.venv\\lib\\site-packages\\autogluon\\core\\hpo\\executors.py\", line 489, in initialize\n",
      "    hyperparameter_tune_kwargs = scheduler_factory(hyperparameter_tune_kwargs, num_trials=num_trials, nthreads_per_trial=\"auto\", ngpus_per_trial=\"auto\")\n",
      "  File \"c:\\JAN_CICHOMSKI\\STUDIA\\STUDIA_SEMESTR_7_2023_ZIMA\\auto_ml\\homeworks\\homework_1\\AUTOML_HM2\\.venv\\lib\\site-packages\\autogluon\\core\\scheduler\\scheduler_factory.py\", line 191, in scheduler_factory\n",
      "    raise ValueError(f\"Required key 'scheduler' is not present in hyperparameter_tune_kwargs: {hyperparameter_tune_kwargs}\")\n",
      "ValueError: Required key 'scheduler' is not present in hyperparameter_tune_kwargs: {'searcher': 'auto', 'time_out': 1200, 'num_trials': 30}\n",
      "Required key 'scheduler' is not present in hyperparameter_tune_kwargs: {'searcher': 'auto', 'time_out': 1200, 'num_trials': 30}\n",
      "Hyperparameter tuning model: NeuralNetTorch_BAG_L3 ... Tuning model for up to 1.44s of the 26.33s of remaining time.\n",
      "Warning: Exception caused NeuralNetTorch_BAG_L3 to fail during hyperparameter tuning... Skipping this model.\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\JAN_CICHOMSKI\\STUDIA\\STUDIA_SEMESTR_7_2023_ZIMA\\auto_ml\\homeworks\\homework_1\\AUTOML_HM2\\.venv\\lib\\site-packages\\autogluon\\core\\trainer\\abstract_trainer.py\", line 2135, in _train_single_full\n",
      "    hpo_models, hpo_results = model.hyperparameter_tune(\n",
      "  File \"c:\\JAN_CICHOMSKI\\STUDIA\\STUDIA_SEMESTR_7_2023_ZIMA\\auto_ml\\homeworks\\homework_1\\AUTOML_HM2\\.venv\\lib\\site-packages\\autogluon\\core\\models\\abstract\\abstract_model.py\", line 1466, in hyperparameter_tune\n",
      "    hpo_executor.initialize(hyperparameter_tune_kwargs, default_num_trials=default_num_trials, time_limit=time_limit)\n",
      "  File \"c:\\JAN_CICHOMSKI\\STUDIA\\STUDIA_SEMESTR_7_2023_ZIMA\\auto_ml\\homeworks\\homework_1\\AUTOML_HM2\\.venv\\lib\\site-packages\\autogluon\\core\\hpo\\executors.py\", line 336, in initialize\n",
      "    hyperparameter_tune_kwargs[\"scheduler\"], hyperparameter_tune_kwargs[\"scheduler\"]\n",
      "KeyError: 'scheduler'\n",
      "'scheduler'\n",
      "Fitting model: LightGBMLarge_BAG_L3 ... Training model for up to 1.44s of the 26.31s of remaining time.\n",
      "\tFitting 15 child models (S1F1 - S1F15) | Fitting with ParallelLocalFoldFittingStrategy (8 workers, per: cpus=1, gpus=0, memory=0.52%)\n",
      "\t0.8719\t = Validation score   (balanced_accuracy)\n",
      "\t8.74s\t = Training   runtime\n",
      "\t0.04s\t = Validation runtime\n",
      "Completed 1/25 k-fold bagging repeats ...\n",
      "Fitting model: WeightedEnsemble_L4 ... Training model for up to 49.92s of the 11.99s of remaining time.\n",
      "\tEnsemble Weights: {'LightGBMLarge_BAG_L3': 1.0}\n",
      "\t0.8719\t = Validation score   (balanced_accuracy)\n",
      "\t0.02s\t = Training   runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "Fitting 11 L4 models ...\n",
      "Hyperparameter tuning model: LightGBMXT_BAG_L4 ... Tuning model for up to 0.98s of the 11.93s of remaining time.\n",
      "Warning: Exception caused LightGBMXT_BAG_L4 to fail during hyperparameter tuning... Skipping this model.\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\JAN_CICHOMSKI\\STUDIA\\STUDIA_SEMESTR_7_2023_ZIMA\\auto_ml\\homeworks\\homework_1\\AUTOML_HM2\\.venv\\lib\\site-packages\\autogluon\\core\\trainer\\abstract_trainer.py\", line 2135, in _train_single_full\n",
      "    hpo_models, hpo_results = model.hyperparameter_tune(\n",
      "  File \"c:\\JAN_CICHOMSKI\\STUDIA\\STUDIA_SEMESTR_7_2023_ZIMA\\auto_ml\\homeworks\\homework_1\\AUTOML_HM2\\.venv\\lib\\site-packages\\autogluon\\core\\models\\abstract\\abstract_model.py\", line 1466, in hyperparameter_tune\n",
      "    hpo_executor.initialize(hyperparameter_tune_kwargs, default_num_trials=default_num_trials, time_limit=time_limit)\n",
      "  File \"c:\\JAN_CICHOMSKI\\STUDIA\\STUDIA_SEMESTR_7_2023_ZIMA\\auto_ml\\homeworks\\homework_1\\AUTOML_HM2\\.venv\\lib\\site-packages\\autogluon\\core\\hpo\\executors.py\", line 489, in initialize\n",
      "    hyperparameter_tune_kwargs = scheduler_factory(hyperparameter_tune_kwargs, num_trials=num_trials, nthreads_per_trial=\"auto\", ngpus_per_trial=\"auto\")\n",
      "  File \"c:\\JAN_CICHOMSKI\\STUDIA\\STUDIA_SEMESTR_7_2023_ZIMA\\auto_ml\\homeworks\\homework_1\\AUTOML_HM2\\.venv\\lib\\site-packages\\autogluon\\core\\scheduler\\scheduler_factory.py\", line 191, in scheduler_factory\n",
      "    raise ValueError(f\"Required key 'scheduler' is not present in hyperparameter_tune_kwargs: {hyperparameter_tune_kwargs}\")\n",
      "ValueError: Required key 'scheduler' is not present in hyperparameter_tune_kwargs: {'searcher': 'auto', 'time_out': 1200, 'num_trials': 30}\n",
      "Required key 'scheduler' is not present in hyperparameter_tune_kwargs: {'searcher': 'auto', 'time_out': 1200, 'num_trials': 30}\n",
      "Hyperparameter tuning model: LightGBM_BAG_L4 ... Tuning model for up to 0.98s of the 11.92s of remaining time.\n",
      "Warning: Exception caused LightGBM_BAG_L4 to fail during hyperparameter tuning... Skipping this model.\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\JAN_CICHOMSKI\\STUDIA\\STUDIA_SEMESTR_7_2023_ZIMA\\auto_ml\\homeworks\\homework_1\\AUTOML_HM2\\.venv\\lib\\site-packages\\autogluon\\core\\trainer\\abstract_trainer.py\", line 2135, in _train_single_full\n",
      "    hpo_models, hpo_results = model.hyperparameter_tune(\n",
      "  File \"c:\\JAN_CICHOMSKI\\STUDIA\\STUDIA_SEMESTR_7_2023_ZIMA\\auto_ml\\homeworks\\homework_1\\AUTOML_HM2\\.venv\\lib\\site-packages\\autogluon\\core\\models\\abstract\\abstract_model.py\", line 1466, in hyperparameter_tune\n",
      "    hpo_executor.initialize(hyperparameter_tune_kwargs, default_num_trials=default_num_trials, time_limit=time_limit)\n",
      "  File \"c:\\JAN_CICHOMSKI\\STUDIA\\STUDIA_SEMESTR_7_2023_ZIMA\\auto_ml\\homeworks\\homework_1\\AUTOML_HM2\\.venv\\lib\\site-packages\\autogluon\\core\\hpo\\executors.py\", line 489, in initialize\n",
      "    hyperparameter_tune_kwargs = scheduler_factory(hyperparameter_tune_kwargs, num_trials=num_trials, nthreads_per_trial=\"auto\", ngpus_per_trial=\"auto\")\n",
      "  File \"c:\\JAN_CICHOMSKI\\STUDIA\\STUDIA_SEMESTR_7_2023_ZIMA\\auto_ml\\homeworks\\homework_1\\AUTOML_HM2\\.venv\\lib\\site-packages\\autogluon\\core\\scheduler\\scheduler_factory.py\", line 191, in scheduler_factory\n",
      "    raise ValueError(f\"Required key 'scheduler' is not present in hyperparameter_tune_kwargs: {hyperparameter_tune_kwargs}\")\n",
      "ValueError: Required key 'scheduler' is not present in hyperparameter_tune_kwargs: {'searcher': 'auto', 'time_out': 1200, 'num_trials': 30}\n",
      "Required key 'scheduler' is not present in hyperparameter_tune_kwargs: {'searcher': 'auto', 'time_out': 1200, 'num_trials': 30}\n",
      "Hyperparameter tuning model: RandomForestGini_BAG_L4 ... Tuning model for up to 0.98s of the 11.9s of remaining time.\n",
      "Warning: Exception caused RandomForestGini_BAG_L4 to fail during hyperparameter tuning... Skipping this model.\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\JAN_CICHOMSKI\\STUDIA\\STUDIA_SEMESTR_7_2023_ZIMA\\auto_ml\\homeworks\\homework_1\\AUTOML_HM2\\.venv\\lib\\site-packages\\autogluon\\core\\trainer\\abstract_trainer.py\", line 2135, in _train_single_full\n",
      "    hpo_models, hpo_results = model.hyperparameter_tune(\n",
      "  File \"c:\\JAN_CICHOMSKI\\STUDIA\\STUDIA_SEMESTR_7_2023_ZIMA\\auto_ml\\homeworks\\homework_1\\AUTOML_HM2\\.venv\\lib\\site-packages\\autogluon\\core\\models\\abstract\\abstract_model.py\", line 1466, in hyperparameter_tune\n",
      "    hpo_executor.initialize(hyperparameter_tune_kwargs, default_num_trials=default_num_trials, time_limit=time_limit)\n",
      "  File \"c:\\JAN_CICHOMSKI\\STUDIA\\STUDIA_SEMESTR_7_2023_ZIMA\\auto_ml\\homeworks\\homework_1\\AUTOML_HM2\\.venv\\lib\\site-packages\\autogluon\\core\\hpo\\executors.py\", line 489, in initialize\n",
      "    hyperparameter_tune_kwargs = scheduler_factory(hyperparameter_tune_kwargs, num_trials=num_trials, nthreads_per_trial=\"auto\", ngpus_per_trial=\"auto\")\n",
      "  File \"c:\\JAN_CICHOMSKI\\STUDIA\\STUDIA_SEMESTR_7_2023_ZIMA\\auto_ml\\homeworks\\homework_1\\AUTOML_HM2\\.venv\\lib\\site-packages\\autogluon\\core\\scheduler\\scheduler_factory.py\", line 191, in scheduler_factory\n",
      "    raise ValueError(f\"Required key 'scheduler' is not present in hyperparameter_tune_kwargs: {hyperparameter_tune_kwargs}\")\n",
      "ValueError: Required key 'scheduler' is not present in hyperparameter_tune_kwargs: {'searcher': 'auto', 'time_out': 1200, 'num_trials': 30}\n",
      "Required key 'scheduler' is not present in hyperparameter_tune_kwargs: {'searcher': 'auto', 'time_out': 1200, 'num_trials': 30}\n",
      "Hyperparameter tuning model: RandomForestEntr_BAG_L4 ... Tuning model for up to 0.98s of the 11.9s of remaining time.\n",
      "Warning: Exception caused RandomForestEntr_BAG_L4 to fail during hyperparameter tuning... Skipping this model.\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\JAN_CICHOMSKI\\STUDIA\\STUDIA_SEMESTR_7_2023_ZIMA\\auto_ml\\homeworks\\homework_1\\AUTOML_HM2\\.venv\\lib\\site-packages\\autogluon\\core\\trainer\\abstract_trainer.py\", line 2135, in _train_single_full\n",
      "    hpo_models, hpo_results = model.hyperparameter_tune(\n",
      "  File \"c:\\JAN_CICHOMSKI\\STUDIA\\STUDIA_SEMESTR_7_2023_ZIMA\\auto_ml\\homeworks\\homework_1\\AUTOML_HM2\\.venv\\lib\\site-packages\\autogluon\\core\\models\\abstract\\abstract_model.py\", line 1466, in hyperparameter_tune\n",
      "    hpo_executor.initialize(hyperparameter_tune_kwargs, default_num_trials=default_num_trials, time_limit=time_limit)\n",
      "  File \"c:\\JAN_CICHOMSKI\\STUDIA\\STUDIA_SEMESTR_7_2023_ZIMA\\auto_ml\\homeworks\\homework_1\\AUTOML_HM2\\.venv\\lib\\site-packages\\autogluon\\core\\hpo\\executors.py\", line 489, in initialize\n",
      "    hyperparameter_tune_kwargs = scheduler_factory(hyperparameter_tune_kwargs, num_trials=num_trials, nthreads_per_trial=\"auto\", ngpus_per_trial=\"auto\")\n",
      "  File \"c:\\JAN_CICHOMSKI\\STUDIA\\STUDIA_SEMESTR_7_2023_ZIMA\\auto_ml\\homeworks\\homework_1\\AUTOML_HM2\\.venv\\lib\\site-packages\\autogluon\\core\\scheduler\\scheduler_factory.py\", line 191, in scheduler_factory\n",
      "    raise ValueError(f\"Required key 'scheduler' is not present in hyperparameter_tune_kwargs: {hyperparameter_tune_kwargs}\")\n",
      "ValueError: Required key 'scheduler' is not present in hyperparameter_tune_kwargs: {'searcher': 'auto', 'time_out': 1200, 'num_trials': 30}\n",
      "Required key 'scheduler' is not present in hyperparameter_tune_kwargs: {'searcher': 'auto', 'time_out': 1200, 'num_trials': 30}\n",
      "Hyperparameter tuning model: CatBoost_BAG_L4 ... Tuning model for up to 0.98s of the 11.88s of remaining time.\n",
      "Warning: Exception caused CatBoost_BAG_L4 to fail during hyperparameter tuning... Skipping this model.\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\JAN_CICHOMSKI\\STUDIA\\STUDIA_SEMESTR_7_2023_ZIMA\\auto_ml\\homeworks\\homework_1\\AUTOML_HM2\\.venv\\lib\\site-packages\\autogluon\\core\\trainer\\abstract_trainer.py\", line 2135, in _train_single_full\n",
      "    hpo_models, hpo_results = model.hyperparameter_tune(\n",
      "  File \"c:\\JAN_CICHOMSKI\\STUDIA\\STUDIA_SEMESTR_7_2023_ZIMA\\auto_ml\\homeworks\\homework_1\\AUTOML_HM2\\.venv\\lib\\site-packages\\autogluon\\core\\models\\abstract\\abstract_model.py\", line 1466, in hyperparameter_tune\n",
      "    hpo_executor.initialize(hyperparameter_tune_kwargs, default_num_trials=default_num_trials, time_limit=time_limit)\n",
      "  File \"c:\\JAN_CICHOMSKI\\STUDIA\\STUDIA_SEMESTR_7_2023_ZIMA\\auto_ml\\homeworks\\homework_1\\AUTOML_HM2\\.venv\\lib\\site-packages\\autogluon\\core\\hpo\\executors.py\", line 489, in initialize\n",
      "    hyperparameter_tune_kwargs = scheduler_factory(hyperparameter_tune_kwargs, num_trials=num_trials, nthreads_per_trial=\"auto\", ngpus_per_trial=\"auto\")\n",
      "  File \"c:\\JAN_CICHOMSKI\\STUDIA\\STUDIA_SEMESTR_7_2023_ZIMA\\auto_ml\\homeworks\\homework_1\\AUTOML_HM2\\.venv\\lib\\site-packages\\autogluon\\core\\scheduler\\scheduler_factory.py\", line 191, in scheduler_factory\n",
      "    raise ValueError(f\"Required key 'scheduler' is not present in hyperparameter_tune_kwargs: {hyperparameter_tune_kwargs}\")\n",
      "ValueError: Required key 'scheduler' is not present in hyperparameter_tune_kwargs: {'searcher': 'auto', 'time_out': 1200, 'num_trials': 30}\n",
      "Required key 'scheduler' is not present in hyperparameter_tune_kwargs: {'searcher': 'auto', 'time_out': 1200, 'num_trials': 30}\n",
      "Hyperparameter tuning model: ExtraTreesGini_BAG_L4 ... Tuning model for up to 0.98s of the 11.88s of remaining time.\n",
      "Warning: Exception caused ExtraTreesGini_BAG_L4 to fail during hyperparameter tuning... Skipping this model.\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\JAN_CICHOMSKI\\STUDIA\\STUDIA_SEMESTR_7_2023_ZIMA\\auto_ml\\homeworks\\homework_1\\AUTOML_HM2\\.venv\\lib\\site-packages\\autogluon\\core\\trainer\\abstract_trainer.py\", line 2135, in _train_single_full\n",
      "    hpo_models, hpo_results = model.hyperparameter_tune(\n",
      "  File \"c:\\JAN_CICHOMSKI\\STUDIA\\STUDIA_SEMESTR_7_2023_ZIMA\\auto_ml\\homeworks\\homework_1\\AUTOML_HM2\\.venv\\lib\\site-packages\\autogluon\\core\\models\\abstract\\abstract_model.py\", line 1466, in hyperparameter_tune\n",
      "    hpo_executor.initialize(hyperparameter_tune_kwargs, default_num_trials=default_num_trials, time_limit=time_limit)\n",
      "  File \"c:\\JAN_CICHOMSKI\\STUDIA\\STUDIA_SEMESTR_7_2023_ZIMA\\auto_ml\\homeworks\\homework_1\\AUTOML_HM2\\.venv\\lib\\site-packages\\autogluon\\core\\hpo\\executors.py\", line 489, in initialize\n",
      "    hyperparameter_tune_kwargs = scheduler_factory(hyperparameter_tune_kwargs, num_trials=num_trials, nthreads_per_trial=\"auto\", ngpus_per_trial=\"auto\")\n",
      "  File \"c:\\JAN_CICHOMSKI\\STUDIA\\STUDIA_SEMESTR_7_2023_ZIMA\\auto_ml\\homeworks\\homework_1\\AUTOML_HM2\\.venv\\lib\\site-packages\\autogluon\\core\\scheduler\\scheduler_factory.py\", line 191, in scheduler_factory\n",
      "    raise ValueError(f\"Required key 'scheduler' is not present in hyperparameter_tune_kwargs: {hyperparameter_tune_kwargs}\")\n",
      "ValueError: Required key 'scheduler' is not present in hyperparameter_tune_kwargs: {'searcher': 'auto', 'time_out': 1200, 'num_trials': 30}\n",
      "Required key 'scheduler' is not present in hyperparameter_tune_kwargs: {'searcher': 'auto', 'time_out': 1200, 'num_trials': 30}\n",
      "Hyperparameter tuning model: ExtraTreesEntr_BAG_L4 ... Tuning model for up to 0.98s of the 11.86s of remaining time.\n",
      "Warning: Exception caused ExtraTreesEntr_BAG_L4 to fail during hyperparameter tuning... Skipping this model.\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\JAN_CICHOMSKI\\STUDIA\\STUDIA_SEMESTR_7_2023_ZIMA\\auto_ml\\homeworks\\homework_1\\AUTOML_HM2\\.venv\\lib\\site-packages\\autogluon\\core\\trainer\\abstract_trainer.py\", line 2135, in _train_single_full\n",
      "    hpo_models, hpo_results = model.hyperparameter_tune(\n",
      "  File \"c:\\JAN_CICHOMSKI\\STUDIA\\STUDIA_SEMESTR_7_2023_ZIMA\\auto_ml\\homeworks\\homework_1\\AUTOML_HM2\\.venv\\lib\\site-packages\\autogluon\\core\\models\\abstract\\abstract_model.py\", line 1466, in hyperparameter_tune\n",
      "    hpo_executor.initialize(hyperparameter_tune_kwargs, default_num_trials=default_num_trials, time_limit=time_limit)\n",
      "  File \"c:\\JAN_CICHOMSKI\\STUDIA\\STUDIA_SEMESTR_7_2023_ZIMA\\auto_ml\\homeworks\\homework_1\\AUTOML_HM2\\.venv\\lib\\site-packages\\autogluon\\core\\hpo\\executors.py\", line 489, in initialize\n",
      "    hyperparameter_tune_kwargs = scheduler_factory(hyperparameter_tune_kwargs, num_trials=num_trials, nthreads_per_trial=\"auto\", ngpus_per_trial=\"auto\")\n",
      "  File \"c:\\JAN_CICHOMSKI\\STUDIA\\STUDIA_SEMESTR_7_2023_ZIMA\\auto_ml\\homeworks\\homework_1\\AUTOML_HM2\\.venv\\lib\\site-packages\\autogluon\\core\\scheduler\\scheduler_factory.py\", line 191, in scheduler_factory\n",
      "    raise ValueError(f\"Required key 'scheduler' is not present in hyperparameter_tune_kwargs: {hyperparameter_tune_kwargs}\")\n",
      "ValueError: Required key 'scheduler' is not present in hyperparameter_tune_kwargs: {'searcher': 'auto', 'time_out': 1200, 'num_trials': 30}\n",
      "Required key 'scheduler' is not present in hyperparameter_tune_kwargs: {'searcher': 'auto', 'time_out': 1200, 'num_trials': 30}\n",
      "Hyperparameter tuning model: NeuralNetFastAI_BAG_L4 ... Tuning model for up to 0.98s of the 11.85s of remaining time.\n",
      "Warning: Exception caused NeuralNetFastAI_BAG_L4 to fail during hyperparameter tuning... Skipping this model.\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\JAN_CICHOMSKI\\STUDIA\\STUDIA_SEMESTR_7_2023_ZIMA\\auto_ml\\homeworks\\homework_1\\AUTOML_HM2\\.venv\\lib\\site-packages\\autogluon\\core\\trainer\\abstract_trainer.py\", line 2135, in _train_single_full\n",
      "    hpo_models, hpo_results = model.hyperparameter_tune(\n",
      "  File \"c:\\JAN_CICHOMSKI\\STUDIA\\STUDIA_SEMESTR_7_2023_ZIMA\\auto_ml\\homeworks\\homework_1\\AUTOML_HM2\\.venv\\lib\\site-packages\\autogluon\\core\\models\\abstract\\abstract_model.py\", line 1466, in hyperparameter_tune\n",
      "    hpo_executor.initialize(hyperparameter_tune_kwargs, default_num_trials=default_num_trials, time_limit=time_limit)\n",
      "  File \"c:\\JAN_CICHOMSKI\\STUDIA\\STUDIA_SEMESTR_7_2023_ZIMA\\auto_ml\\homeworks\\homework_1\\AUTOML_HM2\\.venv\\lib\\site-packages\\autogluon\\core\\hpo\\executors.py\", line 336, in initialize\n",
      "    hyperparameter_tune_kwargs[\"scheduler\"], hyperparameter_tune_kwargs[\"scheduler\"]\n",
      "KeyError: 'scheduler'\n",
      "'scheduler'\n",
      "Hyperparameter tuning model: XGBoost_BAG_L4 ... Tuning model for up to 0.98s of the 11.85s of remaining time.\n",
      "Warning: Exception caused XGBoost_BAG_L4 to fail during hyperparameter tuning... Skipping this model.\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\JAN_CICHOMSKI\\STUDIA\\STUDIA_SEMESTR_7_2023_ZIMA\\auto_ml\\homeworks\\homework_1\\AUTOML_HM2\\.venv\\lib\\site-packages\\autogluon\\core\\trainer\\abstract_trainer.py\", line 2135, in _train_single_full\n",
      "    hpo_models, hpo_results = model.hyperparameter_tune(\n",
      "  File \"c:\\JAN_CICHOMSKI\\STUDIA\\STUDIA_SEMESTR_7_2023_ZIMA\\auto_ml\\homeworks\\homework_1\\AUTOML_HM2\\.venv\\lib\\site-packages\\autogluon\\core\\models\\abstract\\abstract_model.py\", line 1466, in hyperparameter_tune\n",
      "    hpo_executor.initialize(hyperparameter_tune_kwargs, default_num_trials=default_num_trials, time_limit=time_limit)\n",
      "  File \"c:\\JAN_CICHOMSKI\\STUDIA\\STUDIA_SEMESTR_7_2023_ZIMA\\auto_ml\\homeworks\\homework_1\\AUTOML_HM2\\.venv\\lib\\site-packages\\autogluon\\core\\hpo\\executors.py\", line 489, in initialize\n",
      "    hyperparameter_tune_kwargs = scheduler_factory(hyperparameter_tune_kwargs, num_trials=num_trials, nthreads_per_trial=\"auto\", ngpus_per_trial=\"auto\")\n",
      "  File \"c:\\JAN_CICHOMSKI\\STUDIA\\STUDIA_SEMESTR_7_2023_ZIMA\\auto_ml\\homeworks\\homework_1\\AUTOML_HM2\\.venv\\lib\\site-packages\\autogluon\\core\\scheduler\\scheduler_factory.py\", line 191, in scheduler_factory\n",
      "    raise ValueError(f\"Required key 'scheduler' is not present in hyperparameter_tune_kwargs: {hyperparameter_tune_kwargs}\")\n",
      "ValueError: Required key 'scheduler' is not present in hyperparameter_tune_kwargs: {'searcher': 'auto', 'time_out': 1200, 'num_trials': 30}\n",
      "Required key 'scheduler' is not present in hyperparameter_tune_kwargs: {'searcher': 'auto', 'time_out': 1200, 'num_trials': 30}\n",
      "Hyperparameter tuning model: NeuralNetTorch_BAG_L4 ... Tuning model for up to 0.98s of the 11.84s of remaining time.\n",
      "Warning: Exception caused NeuralNetTorch_BAG_L4 to fail during hyperparameter tuning... Skipping this model.\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\JAN_CICHOMSKI\\STUDIA\\STUDIA_SEMESTR_7_2023_ZIMA\\auto_ml\\homeworks\\homework_1\\AUTOML_HM2\\.venv\\lib\\site-packages\\autogluon\\core\\trainer\\abstract_trainer.py\", line 2135, in _train_single_full\n",
      "    hpo_models, hpo_results = model.hyperparameter_tune(\n",
      "  File \"c:\\JAN_CICHOMSKI\\STUDIA\\STUDIA_SEMESTR_7_2023_ZIMA\\auto_ml\\homeworks\\homework_1\\AUTOML_HM2\\.venv\\lib\\site-packages\\autogluon\\core\\models\\abstract\\abstract_model.py\", line 1466, in hyperparameter_tune\n",
      "    hpo_executor.initialize(hyperparameter_tune_kwargs, default_num_trials=default_num_trials, time_limit=time_limit)\n",
      "  File \"c:\\JAN_CICHOMSKI\\STUDIA\\STUDIA_SEMESTR_7_2023_ZIMA\\auto_ml\\homeworks\\homework_1\\AUTOML_HM2\\.venv\\lib\\site-packages\\autogluon\\core\\hpo\\executors.py\", line 336, in initialize\n",
      "    hyperparameter_tune_kwargs[\"scheduler\"], hyperparameter_tune_kwargs[\"scheduler\"]\n",
      "KeyError: 'scheduler'\n",
      "'scheduler'\n",
      "Fitting model: LightGBMLarge_BAG_L4 ... Training model for up to 0.98s of the 11.83s of remaining time.\n",
      "\tFitting 15 child models (S1F1 - S1F15) | Fitting with ParallelLocalFoldFittingStrategy (8 workers, per: cpus=1, gpus=0, memory=0.47%)\n",
      "\t0.8687\t = Validation score   (balanced_accuracy)\n",
      "\t7.04s\t = Training   runtime\n",
      "\t0.02s\t = Validation runtime\n",
      "Completed 1/25 k-fold bagging repeats ...\n",
      "Fitting model: WeightedEnsemble_ALL_L5 ... Training model for up to 49.92s of the -0.16s of remaining time.\n",
      "\tEnsemble Weights: {'LightGBMLarge_BAG_L3': 1.0}\n",
      "\t0.8719\t = Validation score   (balanced_accuracy)\n",
      "\t0.55s\t = Training   runtime\n",
      "\t0.02s\t = Validation runtime\n",
      "Fitting model: WeightedEnsemble_L5 ... Training model for up to 49.92s of the -0.75s of remaining time.\n",
      "\tEnsemble Weights: {'LightGBMLarge_BAG_L4': 1.0}\n",
      "\t0.8687\t = Validation score   (balanced_accuracy)\n",
      "\t0.0s\t = Training   runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "AutoGluon training complete, total runtime = 50.81s ... Best model: \"WeightedEnsemble_L4\"\n",
      "TabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"output\\autogluon\\20240114_144651\")\n"
     ]
    }
   ],
   "source": [
    "save_path = path.join(OUTPUT_DIR_AUTOGLUON, UNIQUE_ID)\n",
    "predictor = TabularPredictor(\n",
    "    label=\"class\",\n",
    "    path=save_path,\n",
    "    eval_metric=\"balanced_accuracy\",\n",
    "    problem_type=\"binary\",\n",
    ").fit(\n",
    "    train_data=train_data_pd,\n",
    "    time_limit=TRAIN_TIME_LIMIT_AUTOGLUON,\n",
    "    presets=\"best_quality\",\n",
    "    hyperparameters=\"default\",\n",
    "    fit_weighted_ensemble=True,\n",
    "    fit_full_last_level_weighted_ensemble=True,\n",
    "    full_weighted_ensemble_additionally=True,\n",
    "    num_bag_folds=15,\n",
    "    num_bag_sets=25,\n",
    "    num_stack_levels=3,\n",
    "    auto_stack=True,\n",
    "    dynamic_stacking=True,\n",
    "    feature_generator=\"auto\",\n",
    "    hyperparameter_tune_kwargs={\n",
    "        \"scheduler\": \"local\",\n",
    "        \"searcher\": \"auto\",\n",
    "        \"time_out\": 1200,\n",
    "        \"num_trials\": 30,\n",
    "    },\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>model</th>\n",
       "      <th>score_val</th>\n",
       "      <th>eval_metric</th>\n",
       "      <th>pred_time_val</th>\n",
       "      <th>fit_time</th>\n",
       "      <th>pred_time_val_marginal</th>\n",
       "      <th>fit_time_marginal</th>\n",
       "      <th>stack_level</th>\n",
       "      <th>can_infer</th>\n",
       "      <th>fit_order</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>LightGBMLarge_BAG_L3</td>\n",
       "      <td>0.871871</td>\n",
       "      <td>balanced_accuracy</td>\n",
       "      <td>0.104815</td>\n",
       "      <td>23.994780</td>\n",
       "      <td>0.041661</td>\n",
       "      <td>8.735041</td>\n",
       "      <td>3</td>\n",
       "      <td>True</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>WeightedEnsemble_L4</td>\n",
       "      <td>0.871871</td>\n",
       "      <td>balanced_accuracy</td>\n",
       "      <td>0.111595</td>\n",
       "      <td>24.011844</td>\n",
       "      <td>0.006779</td>\n",
       "      <td>0.017064</td>\n",
       "      <td>4</td>\n",
       "      <td>True</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>WeightedEnsemble_ALL_L5</td>\n",
       "      <td>0.871871</td>\n",
       "      <td>balanced_accuracy</td>\n",
       "      <td>0.121702</td>\n",
       "      <td>24.541931</td>\n",
       "      <td>0.016887</td>\n",
       "      <td>0.547150</td>\n",
       "      <td>5</td>\n",
       "      <td>True</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>LightGBMLarge_BAG_L2</td>\n",
       "      <td>0.871838</td>\n",
       "      <td>balanced_accuracy</td>\n",
       "      <td>0.063155</td>\n",
       "      <td>15.259739</td>\n",
       "      <td>0.047872</td>\n",
       "      <td>8.527765</td>\n",
       "      <td>2</td>\n",
       "      <td>True</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>WeightedEnsemble_L3</td>\n",
       "      <td>0.871838</td>\n",
       "      <td>balanced_accuracy</td>\n",
       "      <td>0.066155</td>\n",
       "      <td>15.278766</td>\n",
       "      <td>0.003000</td>\n",
       "      <td>0.019027</td>\n",
       "      <td>3</td>\n",
       "      <td>True</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>LightGBMLarge_BAG_L4</td>\n",
       "      <td>0.868706</td>\n",
       "      <td>balanced_accuracy</td>\n",
       "      <td>0.124007</td>\n",
       "      <td>31.039169</td>\n",
       "      <td>0.019192</td>\n",
       "      <td>7.044388</td>\n",
       "      <td>4</td>\n",
       "      <td>True</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>WeightedEnsemble_L5</td>\n",
       "      <td>0.868706</td>\n",
       "      <td>balanced_accuracy</td>\n",
       "      <td>0.138849</td>\n",
       "      <td>31.039169</td>\n",
       "      <td>0.014841</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>5</td>\n",
       "      <td>True</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>LightGBMLarge_BAG_L1</td>\n",
       "      <td>0.858720</td>\n",
       "      <td>balanced_accuracy</td>\n",
       "      <td>0.015282</td>\n",
       "      <td>6.731974</td>\n",
       "      <td>0.015282</td>\n",
       "      <td>6.731974</td>\n",
       "      <td>1</td>\n",
       "      <td>True</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>WeightedEnsemble_L2</td>\n",
       "      <td>0.858720</td>\n",
       "      <td>balanced_accuracy</td>\n",
       "      <td>0.021303</td>\n",
       "      <td>6.739744</td>\n",
       "      <td>0.006020</td>\n",
       "      <td>0.007770</td>\n",
       "      <td>2</td>\n",
       "      <td>True</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                     model  score_val        eval_metric  pred_time_val  \\\n",
       "0     LightGBMLarge_BAG_L3   0.871871  balanced_accuracy       0.104815   \n",
       "1      WeightedEnsemble_L4   0.871871  balanced_accuracy       0.111595   \n",
       "2  WeightedEnsemble_ALL_L5   0.871871  balanced_accuracy       0.121702   \n",
       "3     LightGBMLarge_BAG_L2   0.871838  balanced_accuracy       0.063155   \n",
       "4      WeightedEnsemble_L3   0.871838  balanced_accuracy       0.066155   \n",
       "5     LightGBMLarge_BAG_L4   0.868706  balanced_accuracy       0.124007   \n",
       "6      WeightedEnsemble_L5   0.868706  balanced_accuracy       0.138849   \n",
       "7     LightGBMLarge_BAG_L1   0.858720  balanced_accuracy       0.015282   \n",
       "8      WeightedEnsemble_L2   0.858720  balanced_accuracy       0.021303   \n",
       "\n",
       "    fit_time  pred_time_val_marginal  fit_time_marginal  stack_level  \\\n",
       "0  23.994780                0.041661           8.735041            3   \n",
       "1  24.011844                0.006779           0.017064            4   \n",
       "2  24.541931                0.016887           0.547150            5   \n",
       "3  15.259739                0.047872           8.527765            2   \n",
       "4  15.278766                0.003000           0.019027            3   \n",
       "5  31.039169                0.019192           7.044388            4   \n",
       "6  31.039169                0.014841           0.000000            5   \n",
       "7   6.731974                0.015282           6.731974            1   \n",
       "8   6.739744                0.006020           0.007770            2   \n",
       "\n",
       "   can_infer  fit_order  \n",
       "0       True          5  \n",
       "1       True          6  \n",
       "2       True          8  \n",
       "3       True          3  \n",
       "4       True          4  \n",
       "5       True          7  \n",
       "6       True          9  \n",
       "7       True          1  \n",
       "8       True          2  "
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictor.leaderboard()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'balanced_accuracy': 0.8404140931709635,\n",
       " 'accuracy': 0.84,\n",
       " 'mcc': 0.681441898430591,\n",
       " 'roc_auc': 0.9231076992323273,\n",
       " 'f1': 0.8423645320197045,\n",
       " 'precision': 0.8181818181818182,\n",
       " 'recall': 0.868020304568528}"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictor.evaluate(valid_data_pd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_path_proba = path.join(\n",
    "    OUTPUT_DIR_AUTOGLUON, UNIQUE_ID, \"autogluon_model_proba.txt\"\n",
    ")\n",
    "dump_proba(predictor, pd.DataFrame(test_x), output_path_proba)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MLJar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 1.  1.  1. ...  1. -1. -1.]\n",
      "AutoML directory: output\\mljar\\20240114_114214\\tmp\n",
      "The task is binary_classification with evaluation metric f1\n",
      "AutoML will use algorithms: ['Decision Tree', 'Linear', 'Random Forest', 'Extra Trees', 'LightGBM', 'Xgboost', 'CatBoost', 'Neural Network', 'Nearest Neighbors']\n",
      "AutoML will stack models\n",
      "AutoML will ensemble available models\n",
      "AutoML steps: ['adjust_validation', 'simple_algorithms', 'default_algorithms', 'not_so_random', 'golden_features', 'kmeans_features', 'insert_random_feature', 'features_selection', 'hill_climbing_1', 'hill_climbing_2', 'boost_on_errors', 'ensemble', 'stack', 'ensemble_stacked']\n",
      "* Step adjust_validation will try to check up to 1 model\n",
      "1_DecisionTree f1 0.742515 trained in 3.76 seconds\n",
      "Adjust validation. Remove: 1_DecisionTree\n",
      "Validation strategy: 10-fold CV Shuffle,Stratify\n",
      "* Step simple_algorithms will try to check up to 4 models\n",
      "1_DecisionTree f1 0.743881 trained in 6.52 seconds\n",
      "2_DecisionTree f1 0.660644 trained in 6.99 seconds\n",
      "3_DecisionTree f1 0.638272 trained in 6.24 seconds\n",
      "4_Linear f1 0.63586 trained in 15.42 seconds\n",
      "* Step default_algorithms will try to check up to 7 models\n",
      "5_Default_LightGBM f1 0.83549 trained in 18.9 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\YanPC\\Downloads\\automl_hw2-master\\.venv\\lib\\site-packages\\xgboost\\training.py:38: UserWarning: `feval` is deprecated, use `custom_metric` instead.  They have different behavior when custom objective is also used.See https://xgboost.readthedocs.io/en/latest/tutorials/custom_metric_obj.html for details on the `custom_metric`.\n",
      "c:\\Users\\YanPC\\Downloads\\automl_hw2-master\\.venv\\lib\\site-packages\\xgboost\\training.py:38: UserWarning: `feval` is deprecated, use `custom_metric` instead.  They have different behavior when custom objective is also used.See https://xgboost.readthedocs.io/en/latest/tutorials/custom_metric_obj.html for details on the `custom_metric`.\n",
      "c:\\Users\\YanPC\\Downloads\\automl_hw2-master\\.venv\\lib\\site-packages\\xgboost\\training.py:38: UserWarning: `feval` is deprecated, use `custom_metric` instead.  They have different behavior when custom objective is also used.See https://xgboost.readthedocs.io/en/latest/tutorials/custom_metric_obj.html for details on the `custom_metric`.\n",
      "c:\\Users\\YanPC\\Downloads\\automl_hw2-master\\.venv\\lib\\site-packages\\xgboost\\training.py:38: UserWarning: `feval` is deprecated, use `custom_metric` instead.  They have different behavior when custom objective is also used.See https://xgboost.readthedocs.io/en/latest/tutorials/custom_metric_obj.html for details on the `custom_metric`.\n",
      "c:\\Users\\YanPC\\Downloads\\automl_hw2-master\\.venv\\lib\\site-packages\\xgboost\\training.py:38: UserWarning: `feval` is deprecated, use `custom_metric` instead.  They have different behavior when custom objective is also used.See https://xgboost.readthedocs.io/en/latest/tutorials/custom_metric_obj.html for details on the `custom_metric`.\n",
      "c:\\Users\\YanPC\\Downloads\\automl_hw2-master\\.venv\\lib\\site-packages\\xgboost\\training.py:38: UserWarning: `feval` is deprecated, use `custom_metric` instead.  They have different behavior when custom objective is also used.See https://xgboost.readthedocs.io/en/latest/tutorials/custom_metric_obj.html for details on the `custom_metric`.\n",
      "c:\\Users\\YanPC\\Downloads\\automl_hw2-master\\.venv\\lib\\site-packages\\xgboost\\training.py:38: UserWarning: `feval` is deprecated, use `custom_metric` instead.  They have different behavior when custom objective is also used.See https://xgboost.readthedocs.io/en/latest/tutorials/custom_metric_obj.html for details on the `custom_metric`.\n",
      "c:\\Users\\YanPC\\Downloads\\automl_hw2-master\\.venv\\lib\\site-packages\\xgboost\\training.py:38: UserWarning: `feval` is deprecated, use `custom_metric` instead.  They have different behavior when custom objective is also used.See https://xgboost.readthedocs.io/en/latest/tutorials/custom_metric_obj.html for details on the `custom_metric`.\n",
      "c:\\Users\\YanPC\\Downloads\\automl_hw2-master\\.venv\\lib\\site-packages\\xgboost\\training.py:38: UserWarning: `feval` is deprecated, use `custom_metric` instead.  They have different behavior when custom objective is also used.See https://xgboost.readthedocs.io/en/latest/tutorials/custom_metric_obj.html for details on the `custom_metric`.\n",
      "c:\\Users\\YanPC\\Downloads\\automl_hw2-master\\.venv\\lib\\site-packages\\xgboost\\training.py:38: UserWarning: `feval` is deprecated, use `custom_metric` instead.  They have different behavior when custom objective is also used.See https://xgboost.readthedocs.io/en/latest/tutorials/custom_metric_obj.html for details on the `custom_metric`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6_Default_Xgboost f1 0.815871 trained in 25.23 seconds\n",
      "7_Default_CatBoost f1 0.836836 trained in 16.83 seconds\n",
      "8_Default_NeuralNetwork f1 0.675444 trained in 13.05 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\YanPC\\Downloads\\automl_hw2-master\\.venv\\lib\\site-packages\\supervised\\utils\\learning_curves.py:113: FutureWarning: The behavior of Series.argmax/argmin with skipna=False and NAs, or with all-NAs is deprecated. In a future version this will raise ValueError.\n",
      "c:\\Users\\YanPC\\Downloads\\automl_hw2-master\\.venv\\lib\\site-packages\\supervised\\utils\\learning_curves.py:113: FutureWarning: The behavior of Series.argmax/argmin with skipna=False and NAs, or with all-NAs is deprecated. In a future version this will raise ValueError.\n",
      "c:\\Users\\YanPC\\Downloads\\automl_hw2-master\\.venv\\lib\\site-packages\\supervised\\utils\\learning_curves.py:113: FutureWarning: The behavior of Series.argmax/argmin with skipna=False and NAs, or with all-NAs is deprecated. In a future version this will raise ValueError.\n",
      "c:\\Users\\YanPC\\Downloads\\automl_hw2-master\\.venv\\lib\\site-packages\\supervised\\utils\\learning_curves.py:113: FutureWarning: The behavior of Series.argmax/argmin with skipna=False and NAs, or with all-NAs is deprecated. In a future version this will raise ValueError.\n",
      "c:\\Users\\YanPC\\Downloads\\automl_hw2-master\\.venv\\lib\\site-packages\\supervised\\utils\\learning_curves.py:113: FutureWarning: The behavior of Series.argmax/argmin with skipna=False and NAs, or with all-NAs is deprecated. In a future version this will raise ValueError.\n",
      "c:\\Users\\YanPC\\Downloads\\automl_hw2-master\\.venv\\lib\\site-packages\\supervised\\utils\\learning_curves.py:113: FutureWarning: The behavior of Series.argmax/argmin with skipna=False and NAs, or with all-NAs is deprecated. In a future version this will raise ValueError.\n",
      "c:\\Users\\YanPC\\Downloads\\automl_hw2-master\\.venv\\lib\\site-packages\\supervised\\utils\\learning_curves.py:113: FutureWarning: The behavior of Series.argmax/argmin with skipna=False and NAs, or with all-NAs is deprecated. In a future version this will raise ValueError.\n",
      "c:\\Users\\YanPC\\Downloads\\automl_hw2-master\\.venv\\lib\\site-packages\\supervised\\utils\\learning_curves.py:113: FutureWarning: The behavior of Series.argmax/argmin with skipna=False and NAs, or with all-NAs is deprecated. In a future version this will raise ValueError.\n",
      "c:\\Users\\YanPC\\Downloads\\automl_hw2-master\\.venv\\lib\\site-packages\\supervised\\utils\\learning_curves.py:113: FutureWarning: The behavior of Series.argmax/argmin with skipna=False and NAs, or with all-NAs is deprecated. In a future version this will raise ValueError.\n",
      "c:\\Users\\YanPC\\Downloads\\automl_hw2-master\\.venv\\lib\\site-packages\\supervised\\utils\\learning_curves.py:113: FutureWarning: The behavior of Series.argmax/argmin with skipna=False and NAs, or with all-NAs is deprecated. In a future version this will raise ValueError.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9_Default_RandomForest f1 0.78765 trained in 23.48 seconds\n",
      "10_Default_ExtraTrees f1 0.743227 trained in 16.59 seconds\n",
      "11_Default_NearestNeighbors f1 0.639112 trained in 9.4 seconds\n",
      "* Step not_so_random will try to check up to 61 models\n",
      "21_LightGBM f1 0.815142 trained in 18.61 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\YanPC\\Downloads\\automl_hw2-master\\.venv\\lib\\site-packages\\xgboost\\training.py:38: UserWarning: `feval` is deprecated, use `custom_metric` instead.  They have different behavior when custom objective is also used.See https://xgboost.readthedocs.io/en/latest/tutorials/custom_metric_obj.html for details on the `custom_metric`.\n",
      "c:\\Users\\YanPC\\Downloads\\automl_hw2-master\\.venv\\lib\\site-packages\\xgboost\\training.py:38: UserWarning: `feval` is deprecated, use `custom_metric` instead.  They have different behavior when custom objective is also used.See https://xgboost.readthedocs.io/en/latest/tutorials/custom_metric_obj.html for details on the `custom_metric`.\n",
      "c:\\Users\\YanPC\\Downloads\\automl_hw2-master\\.venv\\lib\\site-packages\\xgboost\\training.py:38: UserWarning: `feval` is deprecated, use `custom_metric` instead.  They have different behavior when custom objective is also used.See https://xgboost.readthedocs.io/en/latest/tutorials/custom_metric_obj.html for details on the `custom_metric`.\n",
      "c:\\Users\\YanPC\\Downloads\\automl_hw2-master\\.venv\\lib\\site-packages\\xgboost\\training.py:38: UserWarning: `feval` is deprecated, use `custom_metric` instead.  They have different behavior when custom objective is also used.See https://xgboost.readthedocs.io/en/latest/tutorials/custom_metric_obj.html for details on the `custom_metric`.\n",
      "c:\\Users\\YanPC\\Downloads\\automl_hw2-master\\.venv\\lib\\site-packages\\xgboost\\training.py:38: UserWarning: `feval` is deprecated, use `custom_metric` instead.  They have different behavior when custom objective is also used.See https://xgboost.readthedocs.io/en/latest/tutorials/custom_metric_obj.html for details on the `custom_metric`.\n",
      "c:\\Users\\YanPC\\Downloads\\automl_hw2-master\\.venv\\lib\\site-packages\\xgboost\\training.py:38: UserWarning: `feval` is deprecated, use `custom_metric` instead.  They have different behavior when custom objective is also used.See https://xgboost.readthedocs.io/en/latest/tutorials/custom_metric_obj.html for details on the `custom_metric`.\n",
      "c:\\Users\\YanPC\\Downloads\\automl_hw2-master\\.venv\\lib\\site-packages\\xgboost\\training.py:38: UserWarning: `feval` is deprecated, use `custom_metric` instead.  They have different behavior when custom objective is also used.See https://xgboost.readthedocs.io/en/latest/tutorials/custom_metric_obj.html for details on the `custom_metric`.\n",
      "c:\\Users\\YanPC\\Downloads\\automl_hw2-master\\.venv\\lib\\site-packages\\xgboost\\training.py:38: UserWarning: `feval` is deprecated, use `custom_metric` instead.  They have different behavior when custom objective is also used.See https://xgboost.readthedocs.io/en/latest/tutorials/custom_metric_obj.html for details on the `custom_metric`.\n",
      "c:\\Users\\YanPC\\Downloads\\automl_hw2-master\\.venv\\lib\\site-packages\\xgboost\\training.py:38: UserWarning: `feval` is deprecated, use `custom_metric` instead.  They have different behavior when custom objective is also used.See https://xgboost.readthedocs.io/en/latest/tutorials/custom_metric_obj.html for details on the `custom_metric`.\n",
      "c:\\Users\\YanPC\\Downloads\\automl_hw2-master\\.venv\\lib\\site-packages\\xgboost\\training.py:38: UserWarning: `feval` is deprecated, use `custom_metric` instead.  They have different behavior when custom objective is also used.See https://xgboost.readthedocs.io/en/latest/tutorials/custom_metric_obj.html for details on the `custom_metric`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12_Xgboost f1 0.794984 trained in 23.32 seconds\n",
      "30_CatBoost f1 0.841133 trained in 20.51 seconds\n",
      "39_RandomForest f1 0.811321 trained in 28.48 seconds\n",
      "48_ExtraTrees f1 0.783558 trained in 20.52 seconds\n",
      "57_NeuralNetwork f1 0.559256 trained in 14.77 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\YanPC\\Downloads\\automl_hw2-master\\.venv\\lib\\site-packages\\supervised\\utils\\learning_curves.py:113: FutureWarning: The behavior of Series.argmax/argmin with skipna=False and NAs, or with all-NAs is deprecated. In a future version this will raise ValueError.\n",
      "c:\\Users\\YanPC\\Downloads\\automl_hw2-master\\.venv\\lib\\site-packages\\supervised\\utils\\learning_curves.py:113: FutureWarning: The behavior of Series.argmax/argmin with skipna=False and NAs, or with all-NAs is deprecated. In a future version this will raise ValueError.\n",
      "c:\\Users\\YanPC\\Downloads\\automl_hw2-master\\.venv\\lib\\site-packages\\supervised\\utils\\learning_curves.py:113: FutureWarning: The behavior of Series.argmax/argmin with skipna=False and NAs, or with all-NAs is deprecated. In a future version this will raise ValueError.\n",
      "c:\\Users\\YanPC\\Downloads\\automl_hw2-master\\.venv\\lib\\site-packages\\supervised\\utils\\learning_curves.py:113: FutureWarning: The behavior of Series.argmax/argmin with skipna=False and NAs, or with all-NAs is deprecated. In a future version this will raise ValueError.\n",
      "c:\\Users\\YanPC\\Downloads\\automl_hw2-master\\.venv\\lib\\site-packages\\supervised\\utils\\learning_curves.py:113: FutureWarning: The behavior of Series.argmax/argmin with skipna=False and NAs, or with all-NAs is deprecated. In a future version this will raise ValueError.\n",
      "c:\\Users\\YanPC\\Downloads\\automl_hw2-master\\.venv\\lib\\site-packages\\supervised\\utils\\learning_curves.py:113: FutureWarning: The behavior of Series.argmax/argmin with skipna=False and NAs, or with all-NAs is deprecated. In a future version this will raise ValueError.\n",
      "c:\\Users\\YanPC\\Downloads\\automl_hw2-master\\.venv\\lib\\site-packages\\supervised\\utils\\learning_curves.py:113: FutureWarning: The behavior of Series.argmax/argmin with skipna=False and NAs, or with all-NAs is deprecated. In a future version this will raise ValueError.\n",
      "c:\\Users\\YanPC\\Downloads\\automl_hw2-master\\.venv\\lib\\site-packages\\supervised\\utils\\learning_curves.py:113: FutureWarning: The behavior of Series.argmax/argmin with skipna=False and NAs, or with all-NAs is deprecated. In a future version this will raise ValueError.\n",
      "c:\\Users\\YanPC\\Downloads\\automl_hw2-master\\.venv\\lib\\site-packages\\supervised\\utils\\learning_curves.py:113: FutureWarning: The behavior of Series.argmax/argmin with skipna=False and NAs, or with all-NAs is deprecated. In a future version this will raise ValueError.\n",
      "c:\\Users\\YanPC\\Downloads\\automl_hw2-master\\.venv\\lib\\site-packages\\supervised\\utils\\learning_curves.py:113: FutureWarning: The behavior of Series.argmax/argmin with skipna=False and NAs, or with all-NAs is deprecated. In a future version this will raise ValueError.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "66_NearestNeighbors f1 0.652527 trained in 10.59 seconds\n",
      "22_LightGBM f1 0.823965 trained in 18.99 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\YanPC\\Downloads\\automl_hw2-master\\.venv\\lib\\site-packages\\xgboost\\training.py:38: UserWarning: `feval` is deprecated, use `custom_metric` instead.  They have different behavior when custom objective is also used.See https://xgboost.readthedocs.io/en/latest/tutorials/custom_metric_obj.html for details on the `custom_metric`.\n",
      "c:\\Users\\YanPC\\Downloads\\automl_hw2-master\\.venv\\lib\\site-packages\\xgboost\\training.py:38: UserWarning: `feval` is deprecated, use `custom_metric` instead.  They have different behavior when custom objective is also used.See https://xgboost.readthedocs.io/en/latest/tutorials/custom_metric_obj.html for details on the `custom_metric`.\n",
      "c:\\Users\\YanPC\\Downloads\\automl_hw2-master\\.venv\\lib\\site-packages\\xgboost\\training.py:38: UserWarning: `feval` is deprecated, use `custom_metric` instead.  They have different behavior when custom objective is also used.See https://xgboost.readthedocs.io/en/latest/tutorials/custom_metric_obj.html for details on the `custom_metric`.\n",
      "c:\\Users\\YanPC\\Downloads\\automl_hw2-master\\.venv\\lib\\site-packages\\xgboost\\training.py:38: UserWarning: `feval` is deprecated, use `custom_metric` instead.  They have different behavior when custom objective is also used.See https://xgboost.readthedocs.io/en/latest/tutorials/custom_metric_obj.html for details on the `custom_metric`.\n",
      "c:\\Users\\YanPC\\Downloads\\automl_hw2-master\\.venv\\lib\\site-packages\\xgboost\\training.py:38: UserWarning: `feval` is deprecated, use `custom_metric` instead.  They have different behavior when custom objective is also used.See https://xgboost.readthedocs.io/en/latest/tutorials/custom_metric_obj.html for details on the `custom_metric`.\n",
      "c:\\Users\\YanPC\\Downloads\\automl_hw2-master\\.venv\\lib\\site-packages\\xgboost\\training.py:38: UserWarning: `feval` is deprecated, use `custom_metric` instead.  They have different behavior when custom objective is also used.See https://xgboost.readthedocs.io/en/latest/tutorials/custom_metric_obj.html for details on the `custom_metric`.\n",
      "c:\\Users\\YanPC\\Downloads\\automl_hw2-master\\.venv\\lib\\site-packages\\xgboost\\training.py:38: UserWarning: `feval` is deprecated, use `custom_metric` instead.  They have different behavior when custom objective is also used.See https://xgboost.readthedocs.io/en/latest/tutorials/custom_metric_obj.html for details on the `custom_metric`.\n",
      "c:\\Users\\YanPC\\Downloads\\automl_hw2-master\\.venv\\lib\\site-packages\\xgboost\\training.py:38: UserWarning: `feval` is deprecated, use `custom_metric` instead.  They have different behavior when custom objective is also used.See https://xgboost.readthedocs.io/en/latest/tutorials/custom_metric_obj.html for details on the `custom_metric`.\n",
      "c:\\Users\\YanPC\\Downloads\\automl_hw2-master\\.venv\\lib\\site-packages\\xgboost\\training.py:38: UserWarning: `feval` is deprecated, use `custom_metric` instead.  They have different behavior when custom objective is also used.See https://xgboost.readthedocs.io/en/latest/tutorials/custom_metric_obj.html for details on the `custom_metric`.\n",
      "c:\\Users\\YanPC\\Downloads\\automl_hw2-master\\.venv\\lib\\site-packages\\xgboost\\training.py:38: UserWarning: `feval` is deprecated, use `custom_metric` instead.  They have different behavior when custom objective is also used.See https://xgboost.readthedocs.io/en/latest/tutorials/custom_metric_obj.html for details on the `custom_metric`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "13_Xgboost f1 0.714286 trained in 27.42 seconds\n",
      "31_CatBoost f1 0.838471 trained in 19.26 seconds\n",
      "40_RandomForest f1 0.753433 trained in 23.1 seconds\n",
      "49_ExtraTrees f1 0.712846 trained in 19.3 seconds\n",
      "58_NeuralNetwork f1 0.708504 trained in 18.35 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\YanPC\\Downloads\\automl_hw2-master\\.venv\\lib\\site-packages\\supervised\\utils\\learning_curves.py:113: FutureWarning: The behavior of Series.argmax/argmin with skipna=False and NAs, or with all-NAs is deprecated. In a future version this will raise ValueError.\n",
      "c:\\Users\\YanPC\\Downloads\\automl_hw2-master\\.venv\\lib\\site-packages\\supervised\\utils\\learning_curves.py:113: FutureWarning: The behavior of Series.argmax/argmin with skipna=False and NAs, or with all-NAs is deprecated. In a future version this will raise ValueError.\n",
      "c:\\Users\\YanPC\\Downloads\\automl_hw2-master\\.venv\\lib\\site-packages\\supervised\\utils\\learning_curves.py:113: FutureWarning: The behavior of Series.argmax/argmin with skipna=False and NAs, or with all-NAs is deprecated. In a future version this will raise ValueError.\n",
      "c:\\Users\\YanPC\\Downloads\\automl_hw2-master\\.venv\\lib\\site-packages\\supervised\\utils\\learning_curves.py:113: FutureWarning: The behavior of Series.argmax/argmin with skipna=False and NAs, or with all-NAs is deprecated. In a future version this will raise ValueError.\n",
      "c:\\Users\\YanPC\\Downloads\\automl_hw2-master\\.venv\\lib\\site-packages\\supervised\\utils\\learning_curves.py:113: FutureWarning: The behavior of Series.argmax/argmin with skipna=False and NAs, or with all-NAs is deprecated. In a future version this will raise ValueError.\n",
      "c:\\Users\\YanPC\\Downloads\\automl_hw2-master\\.venv\\lib\\site-packages\\supervised\\utils\\learning_curves.py:113: FutureWarning: The behavior of Series.argmax/argmin with skipna=False and NAs, or with all-NAs is deprecated. In a future version this will raise ValueError.\n",
      "c:\\Users\\YanPC\\Downloads\\automl_hw2-master\\.venv\\lib\\site-packages\\supervised\\utils\\learning_curves.py:113: FutureWarning: The behavior of Series.argmax/argmin with skipna=False and NAs, or with all-NAs is deprecated. In a future version this will raise ValueError.\n",
      "c:\\Users\\YanPC\\Downloads\\automl_hw2-master\\.venv\\lib\\site-packages\\supervised\\utils\\learning_curves.py:113: FutureWarning: The behavior of Series.argmax/argmin with skipna=False and NAs, or with all-NAs is deprecated. In a future version this will raise ValueError.\n",
      "c:\\Users\\YanPC\\Downloads\\automl_hw2-master\\.venv\\lib\\site-packages\\supervised\\utils\\learning_curves.py:113: FutureWarning: The behavior of Series.argmax/argmin with skipna=False and NAs, or with all-NAs is deprecated. In a future version this will raise ValueError.\n",
      "c:\\Users\\YanPC\\Downloads\\automl_hw2-master\\.venv\\lib\\site-packages\\supervised\\utils\\learning_curves.py:113: FutureWarning: The behavior of Series.argmax/argmin with skipna=False and NAs, or with all-NAs is deprecated. In a future version this will raise ValueError.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "67_NearestNeighbors f1 0.620647 trained in 11.26 seconds\n",
      "23_LightGBM f1 0.838988 trained in 20.68 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\YanPC\\Downloads\\automl_hw2-master\\.venv\\lib\\site-packages\\xgboost\\training.py:38: UserWarning: `feval` is deprecated, use `custom_metric` instead.  They have different behavior when custom objective is also used.See https://xgboost.readthedocs.io/en/latest/tutorials/custom_metric_obj.html for details on the `custom_metric`.\n",
      "c:\\Users\\YanPC\\Downloads\\automl_hw2-master\\.venv\\lib\\site-packages\\xgboost\\training.py:38: UserWarning: `feval` is deprecated, use `custom_metric` instead.  They have different behavior when custom objective is also used.See https://xgboost.readthedocs.io/en/latest/tutorials/custom_metric_obj.html for details on the `custom_metric`.\n",
      "c:\\Users\\YanPC\\Downloads\\automl_hw2-master\\.venv\\lib\\site-packages\\xgboost\\training.py:38: UserWarning: `feval` is deprecated, use `custom_metric` instead.  They have different behavior when custom objective is also used.See https://xgboost.readthedocs.io/en/latest/tutorials/custom_metric_obj.html for details on the `custom_metric`.\n",
      "c:\\Users\\YanPC\\Downloads\\automl_hw2-master\\.venv\\lib\\site-packages\\xgboost\\training.py:38: UserWarning: `feval` is deprecated, use `custom_metric` instead.  They have different behavior when custom objective is also used.See https://xgboost.readthedocs.io/en/latest/tutorials/custom_metric_obj.html for details on the `custom_metric`.\n",
      "c:\\Users\\YanPC\\Downloads\\automl_hw2-master\\.venv\\lib\\site-packages\\xgboost\\training.py:38: UserWarning: `feval` is deprecated, use `custom_metric` instead.  They have different behavior when custom objective is also used.See https://xgboost.readthedocs.io/en/latest/tutorials/custom_metric_obj.html for details on the `custom_metric`.\n",
      "c:\\Users\\YanPC\\Downloads\\automl_hw2-master\\.venv\\lib\\site-packages\\xgboost\\training.py:38: UserWarning: `feval` is deprecated, use `custom_metric` instead.  They have different behavior when custom objective is also used.See https://xgboost.readthedocs.io/en/latest/tutorials/custom_metric_obj.html for details on the `custom_metric`.\n",
      "c:\\Users\\YanPC\\Downloads\\automl_hw2-master\\.venv\\lib\\site-packages\\xgboost\\training.py:38: UserWarning: `feval` is deprecated, use `custom_metric` instead.  They have different behavior when custom objective is also used.See https://xgboost.readthedocs.io/en/latest/tutorials/custom_metric_obj.html for details on the `custom_metric`.\n",
      "c:\\Users\\YanPC\\Downloads\\automl_hw2-master\\.venv\\lib\\site-packages\\xgboost\\training.py:38: UserWarning: `feval` is deprecated, use `custom_metric` instead.  They have different behavior when custom objective is also used.See https://xgboost.readthedocs.io/en/latest/tutorials/custom_metric_obj.html for details on the `custom_metric`.\n",
      "c:\\Users\\YanPC\\Downloads\\automl_hw2-master\\.venv\\lib\\site-packages\\xgboost\\training.py:38: UserWarning: `feval` is deprecated, use `custom_metric` instead.  They have different behavior when custom objective is also used.See https://xgboost.readthedocs.io/en/latest/tutorials/custom_metric_obj.html for details on the `custom_metric`.\n",
      "c:\\Users\\YanPC\\Downloads\\automl_hw2-master\\.venv\\lib\\site-packages\\xgboost\\training.py:38: UserWarning: `feval` is deprecated, use `custom_metric` instead.  They have different behavior when custom objective is also used.See https://xgboost.readthedocs.io/en/latest/tutorials/custom_metric_obj.html for details on the `custom_metric`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "14_Xgboost f1 0.783387 trained in 31.94 seconds\n",
      "32_CatBoost f1 0.849148 trained in 44.36 seconds\n",
      "41_RandomForest f1 0.745993 trained in 26.77 seconds\n",
      "50_ExtraTrees f1 0.710237 trained in 19.53 seconds\n",
      "59_NeuralNetwork f1 0.681959 trained in 16.36 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\YanPC\\Downloads\\automl_hw2-master\\.venv\\lib\\site-packages\\supervised\\utils\\learning_curves.py:113: FutureWarning: The behavior of Series.argmax/argmin with skipna=False and NAs, or with all-NAs is deprecated. In a future version this will raise ValueError.\n",
      "c:\\Users\\YanPC\\Downloads\\automl_hw2-master\\.venv\\lib\\site-packages\\supervised\\utils\\learning_curves.py:113: FutureWarning: The behavior of Series.argmax/argmin with skipna=False and NAs, or with all-NAs is deprecated. In a future version this will raise ValueError.\n",
      "c:\\Users\\YanPC\\Downloads\\automl_hw2-master\\.venv\\lib\\site-packages\\supervised\\utils\\learning_curves.py:113: FutureWarning: The behavior of Series.argmax/argmin with skipna=False and NAs, or with all-NAs is deprecated. In a future version this will raise ValueError.\n",
      "c:\\Users\\YanPC\\Downloads\\automl_hw2-master\\.venv\\lib\\site-packages\\supervised\\utils\\learning_curves.py:113: FutureWarning: The behavior of Series.argmax/argmin with skipna=False and NAs, or with all-NAs is deprecated. In a future version this will raise ValueError.\n",
      "c:\\Users\\YanPC\\Downloads\\automl_hw2-master\\.venv\\lib\\site-packages\\supervised\\utils\\learning_curves.py:113: FutureWarning: The behavior of Series.argmax/argmin with skipna=False and NAs, or with all-NAs is deprecated. In a future version this will raise ValueError.\n",
      "c:\\Users\\YanPC\\Downloads\\automl_hw2-master\\.venv\\lib\\site-packages\\supervised\\utils\\learning_curves.py:113: FutureWarning: The behavior of Series.argmax/argmin with skipna=False and NAs, or with all-NAs is deprecated. In a future version this will raise ValueError.\n",
      "c:\\Users\\YanPC\\Downloads\\automl_hw2-master\\.venv\\lib\\site-packages\\supervised\\utils\\learning_curves.py:113: FutureWarning: The behavior of Series.argmax/argmin with skipna=False and NAs, or with all-NAs is deprecated. In a future version this will raise ValueError.\n",
      "c:\\Users\\YanPC\\Downloads\\automl_hw2-master\\.venv\\lib\\site-packages\\supervised\\utils\\learning_curves.py:113: FutureWarning: The behavior of Series.argmax/argmin with skipna=False and NAs, or with all-NAs is deprecated. In a future version this will raise ValueError.\n",
      "c:\\Users\\YanPC\\Downloads\\automl_hw2-master\\.venv\\lib\\site-packages\\supervised\\utils\\learning_curves.py:113: FutureWarning: The behavior of Series.argmax/argmin with skipna=False and NAs, or with all-NAs is deprecated. In a future version this will raise ValueError.\n",
      "c:\\Users\\YanPC\\Downloads\\automl_hw2-master\\.venv\\lib\\site-packages\\supervised\\utils\\learning_curves.py:113: FutureWarning: The behavior of Series.argmax/argmin with skipna=False and NAs, or with all-NAs is deprecated. In a future version this will raise ValueError.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "68_NearestNeighbors f1 0.620647 trained in 12.24 seconds\n",
      "24_LightGBM f1 0.836431 trained in 30.66 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\YanPC\\Downloads\\automl_hw2-master\\.venv\\lib\\site-packages\\xgboost\\training.py:38: UserWarning: `feval` is deprecated, use `custom_metric` instead.  They have different behavior when custom objective is also used.See https://xgboost.readthedocs.io/en/latest/tutorials/custom_metric_obj.html for details on the `custom_metric`.\n",
      "c:\\Users\\YanPC\\Downloads\\automl_hw2-master\\.venv\\lib\\site-packages\\xgboost\\training.py:38: UserWarning: `feval` is deprecated, use `custom_metric` instead.  They have different behavior when custom objective is also used.See https://xgboost.readthedocs.io/en/latest/tutorials/custom_metric_obj.html for details on the `custom_metric`.\n",
      "c:\\Users\\YanPC\\Downloads\\automl_hw2-master\\.venv\\lib\\site-packages\\xgboost\\training.py:38: UserWarning: `feval` is deprecated, use `custom_metric` instead.  They have different behavior when custom objective is also used.See https://xgboost.readthedocs.io/en/latest/tutorials/custom_metric_obj.html for details on the `custom_metric`.\n",
      "c:\\Users\\YanPC\\Downloads\\automl_hw2-master\\.venv\\lib\\site-packages\\xgboost\\training.py:38: UserWarning: `feval` is deprecated, use `custom_metric` instead.  They have different behavior when custom objective is also used.See https://xgboost.readthedocs.io/en/latest/tutorials/custom_metric_obj.html for details on the `custom_metric`.\n",
      "c:\\Users\\YanPC\\Downloads\\automl_hw2-master\\.venv\\lib\\site-packages\\xgboost\\training.py:38: UserWarning: `feval` is deprecated, use `custom_metric` instead.  They have different behavior when custom objective is also used.See https://xgboost.readthedocs.io/en/latest/tutorials/custom_metric_obj.html for details on the `custom_metric`.\n",
      "c:\\Users\\YanPC\\Downloads\\automl_hw2-master\\.venv\\lib\\site-packages\\xgboost\\training.py:38: UserWarning: `feval` is deprecated, use `custom_metric` instead.  They have different behavior when custom objective is also used.See https://xgboost.readthedocs.io/en/latest/tutorials/custom_metric_obj.html for details on the `custom_metric`.\n",
      "c:\\Users\\YanPC\\Downloads\\automl_hw2-master\\.venv\\lib\\site-packages\\xgboost\\training.py:38: UserWarning: `feval` is deprecated, use `custom_metric` instead.  They have different behavior when custom objective is also used.See https://xgboost.readthedocs.io/en/latest/tutorials/custom_metric_obj.html for details on the `custom_metric`.\n",
      "c:\\Users\\YanPC\\Downloads\\automl_hw2-master\\.venv\\lib\\site-packages\\xgboost\\training.py:38: UserWarning: `feval` is deprecated, use `custom_metric` instead.  They have different behavior when custom objective is also used.See https://xgboost.readthedocs.io/en/latest/tutorials/custom_metric_obj.html for details on the `custom_metric`.\n",
      "c:\\Users\\YanPC\\Downloads\\automl_hw2-master\\.venv\\lib\\site-packages\\xgboost\\training.py:38: UserWarning: `feval` is deprecated, use `custom_metric` instead.  They have different behavior when custom objective is also used.See https://xgboost.readthedocs.io/en/latest/tutorials/custom_metric_obj.html for details on the `custom_metric`.\n",
      "c:\\Users\\YanPC\\Downloads\\automl_hw2-master\\.venv\\lib\\site-packages\\xgboost\\training.py:38: UserWarning: `feval` is deprecated, use `custom_metric` instead.  They have different behavior when custom objective is also used.See https://xgboost.readthedocs.io/en/latest/tutorials/custom_metric_obj.html for details on the `custom_metric`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15_Xgboost f1 0.757196 trained in 32.89 seconds\n",
      "33_CatBoost f1 0.814452 trained in 16.11 seconds\n",
      "42_RandomForest f1 0.774436 trained in 26.26 seconds\n",
      "51_ExtraTrees f1 0.721559 trained in 24.57 seconds\n",
      "60_NeuralNetwork f1 0.678727 trained in 16.72 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\YanPC\\Downloads\\automl_hw2-master\\.venv\\lib\\site-packages\\supervised\\utils\\learning_curves.py:113: FutureWarning: The behavior of Series.argmax/argmin with skipna=False and NAs, or with all-NAs is deprecated. In a future version this will raise ValueError.\n",
      "c:\\Users\\YanPC\\Downloads\\automl_hw2-master\\.venv\\lib\\site-packages\\supervised\\utils\\learning_curves.py:113: FutureWarning: The behavior of Series.argmax/argmin with skipna=False and NAs, or with all-NAs is deprecated. In a future version this will raise ValueError.\n",
      "c:\\Users\\YanPC\\Downloads\\automl_hw2-master\\.venv\\lib\\site-packages\\supervised\\utils\\learning_curves.py:113: FutureWarning: The behavior of Series.argmax/argmin with skipna=False and NAs, or with all-NAs is deprecated. In a future version this will raise ValueError.\n",
      "c:\\Users\\YanPC\\Downloads\\automl_hw2-master\\.venv\\lib\\site-packages\\supervised\\utils\\learning_curves.py:113: FutureWarning: The behavior of Series.argmax/argmin with skipna=False and NAs, or with all-NAs is deprecated. In a future version this will raise ValueError.\n",
      "c:\\Users\\YanPC\\Downloads\\automl_hw2-master\\.venv\\lib\\site-packages\\supervised\\utils\\learning_curves.py:113: FutureWarning: The behavior of Series.argmax/argmin with skipna=False and NAs, or with all-NAs is deprecated. In a future version this will raise ValueError.\n",
      "c:\\Users\\YanPC\\Downloads\\automl_hw2-master\\.venv\\lib\\site-packages\\supervised\\utils\\learning_curves.py:113: FutureWarning: The behavior of Series.argmax/argmin with skipna=False and NAs, or with all-NAs is deprecated. In a future version this will raise ValueError.\n",
      "c:\\Users\\YanPC\\Downloads\\automl_hw2-master\\.venv\\lib\\site-packages\\supervised\\utils\\learning_curves.py:113: FutureWarning: The behavior of Series.argmax/argmin with skipna=False and NAs, or with all-NAs is deprecated. In a future version this will raise ValueError.\n",
      "c:\\Users\\YanPC\\Downloads\\automl_hw2-master\\.venv\\lib\\site-packages\\supervised\\utils\\learning_curves.py:113: FutureWarning: The behavior of Series.argmax/argmin with skipna=False and NAs, or with all-NAs is deprecated. In a future version this will raise ValueError.\n",
      "c:\\Users\\YanPC\\Downloads\\automl_hw2-master\\.venv\\lib\\site-packages\\supervised\\utils\\learning_curves.py:113: FutureWarning: The behavior of Series.argmax/argmin with skipna=False and NAs, or with all-NAs is deprecated. In a future version this will raise ValueError.\n",
      "c:\\Users\\YanPC\\Downloads\\automl_hw2-master\\.venv\\lib\\site-packages\\supervised\\utils\\learning_curves.py:113: FutureWarning: The behavior of Series.argmax/argmin with skipna=False and NAs, or with all-NAs is deprecated. In a future version this will raise ValueError.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "69_NearestNeighbors f1 0.620647 trained in 13.17 seconds\n",
      "25_LightGBM f1 0.827329 trained in 24.17 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\YanPC\\Downloads\\automl_hw2-master\\.venv\\lib\\site-packages\\xgboost\\training.py:38: UserWarning: `feval` is deprecated, use `custom_metric` instead.  They have different behavior when custom objective is also used.See https://xgboost.readthedocs.io/en/latest/tutorials/custom_metric_obj.html for details on the `custom_metric`.\n",
      "c:\\Users\\YanPC\\Downloads\\automl_hw2-master\\.venv\\lib\\site-packages\\xgboost\\training.py:38: UserWarning: `feval` is deprecated, use `custom_metric` instead.  They have different behavior when custom objective is also used.See https://xgboost.readthedocs.io/en/latest/tutorials/custom_metric_obj.html for details on the `custom_metric`.\n",
      "c:\\Users\\YanPC\\Downloads\\automl_hw2-master\\.venv\\lib\\site-packages\\xgboost\\training.py:38: UserWarning: `feval` is deprecated, use `custom_metric` instead.  They have different behavior when custom objective is also used.See https://xgboost.readthedocs.io/en/latest/tutorials/custom_metric_obj.html for details on the `custom_metric`.\n",
      "c:\\Users\\YanPC\\Downloads\\automl_hw2-master\\.venv\\lib\\site-packages\\xgboost\\training.py:38: UserWarning: `feval` is deprecated, use `custom_metric` instead.  They have different behavior when custom objective is also used.See https://xgboost.readthedocs.io/en/latest/tutorials/custom_metric_obj.html for details on the `custom_metric`.\n",
      "c:\\Users\\YanPC\\Downloads\\automl_hw2-master\\.venv\\lib\\site-packages\\xgboost\\training.py:38: UserWarning: `feval` is deprecated, use `custom_metric` instead.  They have different behavior when custom objective is also used.See https://xgboost.readthedocs.io/en/latest/tutorials/custom_metric_obj.html for details on the `custom_metric`.\n",
      "c:\\Users\\YanPC\\Downloads\\automl_hw2-master\\.venv\\lib\\site-packages\\xgboost\\training.py:38: UserWarning: `feval` is deprecated, use `custom_metric` instead.  They have different behavior when custom objective is also used.See https://xgboost.readthedocs.io/en/latest/tutorials/custom_metric_obj.html for details on the `custom_metric`.\n",
      "c:\\Users\\YanPC\\Downloads\\automl_hw2-master\\.venv\\lib\\site-packages\\xgboost\\training.py:38: UserWarning: `feval` is deprecated, use `custom_metric` instead.  They have different behavior when custom objective is also used.See https://xgboost.readthedocs.io/en/latest/tutorials/custom_metric_obj.html for details on the `custom_metric`.\n",
      "c:\\Users\\YanPC\\Downloads\\automl_hw2-master\\.venv\\lib\\site-packages\\xgboost\\training.py:38: UserWarning: `feval` is deprecated, use `custom_metric` instead.  They have different behavior when custom objective is also used.See https://xgboost.readthedocs.io/en/latest/tutorials/custom_metric_obj.html for details on the `custom_metric`.\n",
      "c:\\Users\\YanPC\\Downloads\\automl_hw2-master\\.venv\\lib\\site-packages\\xgboost\\training.py:38: UserWarning: `feval` is deprecated, use `custom_metric` instead.  They have different behavior when custom objective is also used.See https://xgboost.readthedocs.io/en/latest/tutorials/custom_metric_obj.html for details on the `custom_metric`.\n",
      "c:\\Users\\YanPC\\Downloads\\automl_hw2-master\\.venv\\lib\\site-packages\\xgboost\\training.py:38: UserWarning: `feval` is deprecated, use `custom_metric` instead.  They have different behavior when custom objective is also used.See https://xgboost.readthedocs.io/en/latest/tutorials/custom_metric_obj.html for details on the `custom_metric`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16_Xgboost f1 0.655194 trained in 33.88 seconds\n",
      "34_CatBoost f1 0.828589 trained in 19.94 seconds\n",
      "43_RandomForest f1 0.741347 trained in 25.22 seconds\n",
      "52_ExtraTrees f1 0.684178 trained in 28.14 seconds\n",
      "61_NeuralNetwork f1 0.705308 trained in 19.52 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\YanPC\\Downloads\\automl_hw2-master\\.venv\\lib\\site-packages\\supervised\\utils\\learning_curves.py:113: FutureWarning: The behavior of Series.argmax/argmin with skipna=False and NAs, or with all-NAs is deprecated. In a future version this will raise ValueError.\n",
      "c:\\Users\\YanPC\\Downloads\\automl_hw2-master\\.venv\\lib\\site-packages\\supervised\\utils\\learning_curves.py:113: FutureWarning: The behavior of Series.argmax/argmin with skipna=False and NAs, or with all-NAs is deprecated. In a future version this will raise ValueError.\n",
      "c:\\Users\\YanPC\\Downloads\\automl_hw2-master\\.venv\\lib\\site-packages\\supervised\\utils\\learning_curves.py:113: FutureWarning: The behavior of Series.argmax/argmin with skipna=False and NAs, or with all-NAs is deprecated. In a future version this will raise ValueError.\n",
      "c:\\Users\\YanPC\\Downloads\\automl_hw2-master\\.venv\\lib\\site-packages\\supervised\\utils\\learning_curves.py:113: FutureWarning: The behavior of Series.argmax/argmin with skipna=False and NAs, or with all-NAs is deprecated. In a future version this will raise ValueError.\n",
      "c:\\Users\\YanPC\\Downloads\\automl_hw2-master\\.venv\\lib\\site-packages\\supervised\\utils\\learning_curves.py:113: FutureWarning: The behavior of Series.argmax/argmin with skipna=False and NAs, or with all-NAs is deprecated. In a future version this will raise ValueError.\n",
      "c:\\Users\\YanPC\\Downloads\\automl_hw2-master\\.venv\\lib\\site-packages\\supervised\\utils\\learning_curves.py:113: FutureWarning: The behavior of Series.argmax/argmin with skipna=False and NAs, or with all-NAs is deprecated. In a future version this will raise ValueError.\n",
      "c:\\Users\\YanPC\\Downloads\\automl_hw2-master\\.venv\\lib\\site-packages\\supervised\\utils\\learning_curves.py:113: FutureWarning: The behavior of Series.argmax/argmin with skipna=False and NAs, or with all-NAs is deprecated. In a future version this will raise ValueError.\n",
      "c:\\Users\\YanPC\\Downloads\\automl_hw2-master\\.venv\\lib\\site-packages\\supervised\\utils\\learning_curves.py:113: FutureWarning: The behavior of Series.argmax/argmin with skipna=False and NAs, or with all-NAs is deprecated. In a future version this will raise ValueError.\n",
      "c:\\Users\\YanPC\\Downloads\\automl_hw2-master\\.venv\\lib\\site-packages\\supervised\\utils\\learning_curves.py:113: FutureWarning: The behavior of Series.argmax/argmin with skipna=False and NAs, or with all-NAs is deprecated. In a future version this will raise ValueError.\n",
      "c:\\Users\\YanPC\\Downloads\\automl_hw2-master\\.venv\\lib\\site-packages\\supervised\\utils\\learning_curves.py:113: FutureWarning: The behavior of Series.argmax/argmin with skipna=False and NAs, or with all-NAs is deprecated. In a future version this will raise ValueError.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "70_NearestNeighbors f1 0.652527 trained in 15.77 seconds\n",
      "26_LightGBM f1 0.836341 trained in 26.44 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\YanPC\\Downloads\\automl_hw2-master\\.venv\\lib\\site-packages\\xgboost\\training.py:38: UserWarning: `feval` is deprecated, use `custom_metric` instead.  They have different behavior when custom objective is also used.See https://xgboost.readthedocs.io/en/latest/tutorials/custom_metric_obj.html for details on the `custom_metric`.\n",
      "c:\\Users\\YanPC\\Downloads\\automl_hw2-master\\.venv\\lib\\site-packages\\xgboost\\training.py:38: UserWarning: `feval` is deprecated, use `custom_metric` instead.  They have different behavior when custom objective is also used.See https://xgboost.readthedocs.io/en/latest/tutorials/custom_metric_obj.html for details on the `custom_metric`.\n",
      "c:\\Users\\YanPC\\Downloads\\automl_hw2-master\\.venv\\lib\\site-packages\\xgboost\\training.py:38: UserWarning: `feval` is deprecated, use `custom_metric` instead.  They have different behavior when custom objective is also used.See https://xgboost.readthedocs.io/en/latest/tutorials/custom_metric_obj.html for details on the `custom_metric`.\n",
      "c:\\Users\\YanPC\\Downloads\\automl_hw2-master\\.venv\\lib\\site-packages\\xgboost\\training.py:38: UserWarning: `feval` is deprecated, use `custom_metric` instead.  They have different behavior when custom objective is also used.See https://xgboost.readthedocs.io/en/latest/tutorials/custom_metric_obj.html for details on the `custom_metric`.\n",
      "c:\\Users\\YanPC\\Downloads\\automl_hw2-master\\.venv\\lib\\site-packages\\xgboost\\training.py:38: UserWarning: `feval` is deprecated, use `custom_metric` instead.  They have different behavior when custom objective is also used.See https://xgboost.readthedocs.io/en/latest/tutorials/custom_metric_obj.html for details on the `custom_metric`.\n",
      "c:\\Users\\YanPC\\Downloads\\automl_hw2-master\\.venv\\lib\\site-packages\\xgboost\\training.py:38: UserWarning: `feval` is deprecated, use `custom_metric` instead.  They have different behavior when custom objective is also used.See https://xgboost.readthedocs.io/en/latest/tutorials/custom_metric_obj.html for details on the `custom_metric`.\n",
      "c:\\Users\\YanPC\\Downloads\\automl_hw2-master\\.venv\\lib\\site-packages\\xgboost\\training.py:38: UserWarning: `feval` is deprecated, use `custom_metric` instead.  They have different behavior when custom objective is also used.See https://xgboost.readthedocs.io/en/latest/tutorials/custom_metric_obj.html for details on the `custom_metric`.\n",
      "c:\\Users\\YanPC\\Downloads\\automl_hw2-master\\.venv\\lib\\site-packages\\xgboost\\training.py:38: UserWarning: `feval` is deprecated, use `custom_metric` instead.  They have different behavior when custom objective is also used.See https://xgboost.readthedocs.io/en/latest/tutorials/custom_metric_obj.html for details on the `custom_metric`.\n",
      "c:\\Users\\YanPC\\Downloads\\automl_hw2-master\\.venv\\lib\\site-packages\\xgboost\\training.py:38: UserWarning: `feval` is deprecated, use `custom_metric` instead.  They have different behavior when custom objective is also used.See https://xgboost.readthedocs.io/en/latest/tutorials/custom_metric_obj.html for details on the `custom_metric`.\n",
      "c:\\Users\\YanPC\\Downloads\\automl_hw2-master\\.venv\\lib\\site-packages\\xgboost\\training.py:38: UserWarning: `feval` is deprecated, use `custom_metric` instead.  They have different behavior when custom objective is also used.See https://xgboost.readthedocs.io/en/latest/tutorials/custom_metric_obj.html for details on the `custom_metric`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "17_Xgboost f1 0.783375 trained in 37.11 seconds\n",
      "35_CatBoost f1 0.819247 trained in 20.33 seconds\n",
      "44_RandomForest f1 0.802218 trained in 30.31 seconds\n",
      "53_ExtraTrees f1 0.762572 trained in 28.6 seconds\n",
      "62_NeuralNetwork f1 0.658278 trained in 25.96 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\YanPC\\Downloads\\automl_hw2-master\\.venv\\lib\\site-packages\\supervised\\utils\\learning_curves.py:113: FutureWarning: The behavior of Series.argmax/argmin with skipna=False and NAs, or with all-NAs is deprecated. In a future version this will raise ValueError.\n",
      "c:\\Users\\YanPC\\Downloads\\automl_hw2-master\\.venv\\lib\\site-packages\\supervised\\utils\\learning_curves.py:113: FutureWarning: The behavior of Series.argmax/argmin with skipna=False and NAs, or with all-NAs is deprecated. In a future version this will raise ValueError.\n",
      "c:\\Users\\YanPC\\Downloads\\automl_hw2-master\\.venv\\lib\\site-packages\\supervised\\utils\\learning_curves.py:113: FutureWarning: The behavior of Series.argmax/argmin with skipna=False and NAs, or with all-NAs is deprecated. In a future version this will raise ValueError.\n",
      "c:\\Users\\YanPC\\Downloads\\automl_hw2-master\\.venv\\lib\\site-packages\\supervised\\utils\\learning_curves.py:113: FutureWarning: The behavior of Series.argmax/argmin with skipna=False and NAs, or with all-NAs is deprecated. In a future version this will raise ValueError.\n",
      "c:\\Users\\YanPC\\Downloads\\automl_hw2-master\\.venv\\lib\\site-packages\\supervised\\utils\\learning_curves.py:113: FutureWarning: The behavior of Series.argmax/argmin with skipna=False and NAs, or with all-NAs is deprecated. In a future version this will raise ValueError.\n",
      "c:\\Users\\YanPC\\Downloads\\automl_hw2-master\\.venv\\lib\\site-packages\\supervised\\utils\\learning_curves.py:113: FutureWarning: The behavior of Series.argmax/argmin with skipna=False and NAs, or with all-NAs is deprecated. In a future version this will raise ValueError.\n",
      "c:\\Users\\YanPC\\Downloads\\automl_hw2-master\\.venv\\lib\\site-packages\\supervised\\utils\\learning_curves.py:113: FutureWarning: The behavior of Series.argmax/argmin with skipna=False and NAs, or with all-NAs is deprecated. In a future version this will raise ValueError.\n",
      "c:\\Users\\YanPC\\Downloads\\automl_hw2-master\\.venv\\lib\\site-packages\\supervised\\utils\\learning_curves.py:113: FutureWarning: The behavior of Series.argmax/argmin with skipna=False and NAs, or with all-NAs is deprecated. In a future version this will raise ValueError.\n",
      "c:\\Users\\YanPC\\Downloads\\automl_hw2-master\\.venv\\lib\\site-packages\\supervised\\utils\\learning_curves.py:113: FutureWarning: The behavior of Series.argmax/argmin with skipna=False and NAs, or with all-NAs is deprecated. In a future version this will raise ValueError.\n",
      "c:\\Users\\YanPC\\Downloads\\automl_hw2-master\\.venv\\lib\\site-packages\\supervised\\utils\\learning_curves.py:113: FutureWarning: The behavior of Series.argmax/argmin with skipna=False and NAs, or with all-NAs is deprecated. In a future version this will raise ValueError.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "71_NearestNeighbors f1 0.620647 trained in 16.67 seconds\n",
      "27_LightGBM f1 0.832718 trained in 28.83 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\YanPC\\Downloads\\automl_hw2-master\\.venv\\lib\\site-packages\\xgboost\\training.py:38: UserWarning: `feval` is deprecated, use `custom_metric` instead.  They have different behavior when custom objective is also used.See https://xgboost.readthedocs.io/en/latest/tutorials/custom_metric_obj.html for details on the `custom_metric`.\n",
      "c:\\Users\\YanPC\\Downloads\\automl_hw2-master\\.venv\\lib\\site-packages\\xgboost\\training.py:38: UserWarning: `feval` is deprecated, use `custom_metric` instead.  They have different behavior when custom objective is also used.See https://xgboost.readthedocs.io/en/latest/tutorials/custom_metric_obj.html for details on the `custom_metric`.\n",
      "c:\\Users\\YanPC\\Downloads\\automl_hw2-master\\.venv\\lib\\site-packages\\xgboost\\training.py:38: UserWarning: `feval` is deprecated, use `custom_metric` instead.  They have different behavior when custom objective is also used.See https://xgboost.readthedocs.io/en/latest/tutorials/custom_metric_obj.html for details on the `custom_metric`.\n",
      "c:\\Users\\YanPC\\Downloads\\automl_hw2-master\\.venv\\lib\\site-packages\\xgboost\\training.py:38: UserWarning: `feval` is deprecated, use `custom_metric` instead.  They have different behavior when custom objective is also used.See https://xgboost.readthedocs.io/en/latest/tutorials/custom_metric_obj.html for details on the `custom_metric`.\n",
      "c:\\Users\\YanPC\\Downloads\\automl_hw2-master\\.venv\\lib\\site-packages\\xgboost\\training.py:38: UserWarning: `feval` is deprecated, use `custom_metric` instead.  They have different behavior when custom objective is also used.See https://xgboost.readthedocs.io/en/latest/tutorials/custom_metric_obj.html for details on the `custom_metric`.\n",
      "c:\\Users\\YanPC\\Downloads\\automl_hw2-master\\.venv\\lib\\site-packages\\xgboost\\training.py:38: UserWarning: `feval` is deprecated, use `custom_metric` instead.  They have different behavior when custom objective is also used.See https://xgboost.readthedocs.io/en/latest/tutorials/custom_metric_obj.html for details on the `custom_metric`.\n",
      "c:\\Users\\YanPC\\Downloads\\automl_hw2-master\\.venv\\lib\\site-packages\\xgboost\\training.py:38: UserWarning: `feval` is deprecated, use `custom_metric` instead.  They have different behavior when custom objective is also used.See https://xgboost.readthedocs.io/en/latest/tutorials/custom_metric_obj.html for details on the `custom_metric`.\n",
      "c:\\Users\\YanPC\\Downloads\\automl_hw2-master\\.venv\\lib\\site-packages\\xgboost\\training.py:38: UserWarning: `feval` is deprecated, use `custom_metric` instead.  They have different behavior when custom objective is also used.See https://xgboost.readthedocs.io/en/latest/tutorials/custom_metric_obj.html for details on the `custom_metric`.\n",
      "c:\\Users\\YanPC\\Downloads\\automl_hw2-master\\.venv\\lib\\site-packages\\xgboost\\training.py:38: UserWarning: `feval` is deprecated, use `custom_metric` instead.  They have different behavior when custom objective is also used.See https://xgboost.readthedocs.io/en/latest/tutorials/custom_metric_obj.html for details on the `custom_metric`.\n",
      "c:\\Users\\YanPC\\Downloads\\automl_hw2-master\\.venv\\lib\\site-packages\\xgboost\\training.py:38: UserWarning: `feval` is deprecated, use `custom_metric` instead.  They have different behavior when custom objective is also used.See https://xgboost.readthedocs.io/en/latest/tutorials/custom_metric_obj.html for details on the `custom_metric`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "18_Xgboost f1 0.734036 trained in 35.27 seconds\n",
      "36_CatBoost f1 0.850277 trained in 27.29 seconds\n",
      "45_RandomForest f1 0.815366 trained in 36.58 seconds\n",
      "54_ExtraTrees f1 0.764059 trained in 29.04 seconds\n",
      "63_NeuralNetwork f1 0.661999 trained in 21.25 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\YanPC\\Downloads\\automl_hw2-master\\.venv\\lib\\site-packages\\supervised\\utils\\learning_curves.py:113: FutureWarning: The behavior of Series.argmax/argmin with skipna=False and NAs, or with all-NAs is deprecated. In a future version this will raise ValueError.\n",
      "c:\\Users\\YanPC\\Downloads\\automl_hw2-master\\.venv\\lib\\site-packages\\supervised\\utils\\learning_curves.py:113: FutureWarning: The behavior of Series.argmax/argmin with skipna=False and NAs, or with all-NAs is deprecated. In a future version this will raise ValueError.\n",
      "c:\\Users\\YanPC\\Downloads\\automl_hw2-master\\.venv\\lib\\site-packages\\supervised\\utils\\learning_curves.py:113: FutureWarning: The behavior of Series.argmax/argmin with skipna=False and NAs, or with all-NAs is deprecated. In a future version this will raise ValueError.\n",
      "c:\\Users\\YanPC\\Downloads\\automl_hw2-master\\.venv\\lib\\site-packages\\supervised\\utils\\learning_curves.py:113: FutureWarning: The behavior of Series.argmax/argmin with skipna=False and NAs, or with all-NAs is deprecated. In a future version this will raise ValueError.\n",
      "c:\\Users\\YanPC\\Downloads\\automl_hw2-master\\.venv\\lib\\site-packages\\supervised\\utils\\learning_curves.py:113: FutureWarning: The behavior of Series.argmax/argmin with skipna=False and NAs, or with all-NAs is deprecated. In a future version this will raise ValueError.\n",
      "c:\\Users\\YanPC\\Downloads\\automl_hw2-master\\.venv\\lib\\site-packages\\supervised\\utils\\learning_curves.py:113: FutureWarning: The behavior of Series.argmax/argmin with skipna=False and NAs, or with all-NAs is deprecated. In a future version this will raise ValueError.\n",
      "c:\\Users\\YanPC\\Downloads\\automl_hw2-master\\.venv\\lib\\site-packages\\supervised\\utils\\learning_curves.py:113: FutureWarning: The behavior of Series.argmax/argmin with skipna=False and NAs, or with all-NAs is deprecated. In a future version this will raise ValueError.\n",
      "c:\\Users\\YanPC\\Downloads\\automl_hw2-master\\.venv\\lib\\site-packages\\supervised\\utils\\learning_curves.py:113: FutureWarning: The behavior of Series.argmax/argmin with skipna=False and NAs, or with all-NAs is deprecated. In a future version this will raise ValueError.\n",
      "c:\\Users\\YanPC\\Downloads\\automl_hw2-master\\.venv\\lib\\site-packages\\supervised\\utils\\learning_curves.py:113: FutureWarning: The behavior of Series.argmax/argmin with skipna=False and NAs, or with all-NAs is deprecated. In a future version this will raise ValueError.\n",
      "c:\\Users\\YanPC\\Downloads\\automl_hw2-master\\.venv\\lib\\site-packages\\supervised\\utils\\learning_curves.py:113: FutureWarning: The behavior of Series.argmax/argmin with skipna=False and NAs, or with all-NAs is deprecated. In a future version this will raise ValueError.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "72_NearestNeighbors f1 0.620647 trained in 16.67 seconds\n",
      "28_LightGBM f1 0.834988 trained in 32.0 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\YanPC\\Downloads\\automl_hw2-master\\.venv\\lib\\site-packages\\xgboost\\training.py:38: UserWarning: `feval` is deprecated, use `custom_metric` instead.  They have different behavior when custom objective is also used.See https://xgboost.readthedocs.io/en/latest/tutorials/custom_metric_obj.html for details on the `custom_metric`.\n",
      "c:\\Users\\YanPC\\Downloads\\automl_hw2-master\\.venv\\lib\\site-packages\\xgboost\\training.py:38: UserWarning: `feval` is deprecated, use `custom_metric` instead.  They have different behavior when custom objective is also used.See https://xgboost.readthedocs.io/en/latest/tutorials/custom_metric_obj.html for details on the `custom_metric`.\n",
      "c:\\Users\\YanPC\\Downloads\\automl_hw2-master\\.venv\\lib\\site-packages\\xgboost\\training.py:38: UserWarning: `feval` is deprecated, use `custom_metric` instead.  They have different behavior when custom objective is also used.See https://xgboost.readthedocs.io/en/latest/tutorials/custom_metric_obj.html for details on the `custom_metric`.\n",
      "c:\\Users\\YanPC\\Downloads\\automl_hw2-master\\.venv\\lib\\site-packages\\xgboost\\training.py:38: UserWarning: `feval` is deprecated, use `custom_metric` instead.  They have different behavior when custom objective is also used.See https://xgboost.readthedocs.io/en/latest/tutorials/custom_metric_obj.html for details on the `custom_metric`.\n",
      "c:\\Users\\YanPC\\Downloads\\automl_hw2-master\\.venv\\lib\\site-packages\\xgboost\\training.py:38: UserWarning: `feval` is deprecated, use `custom_metric` instead.  They have different behavior when custom objective is also used.See https://xgboost.readthedocs.io/en/latest/tutorials/custom_metric_obj.html for details on the `custom_metric`.\n",
      "c:\\Users\\YanPC\\Downloads\\automl_hw2-master\\.venv\\lib\\site-packages\\xgboost\\training.py:38: UserWarning: `feval` is deprecated, use `custom_metric` instead.  They have different behavior when custom objective is also used.See https://xgboost.readthedocs.io/en/latest/tutorials/custom_metric_obj.html for details on the `custom_metric`.\n",
      "c:\\Users\\YanPC\\Downloads\\automl_hw2-master\\.venv\\lib\\site-packages\\xgboost\\training.py:38: UserWarning: `feval` is deprecated, use `custom_metric` instead.  They have different behavior when custom objective is also used.See https://xgboost.readthedocs.io/en/latest/tutorials/custom_metric_obj.html for details on the `custom_metric`.\n",
      "c:\\Users\\YanPC\\Downloads\\automl_hw2-master\\.venv\\lib\\site-packages\\xgboost\\training.py:38: UserWarning: `feval` is deprecated, use `custom_metric` instead.  They have different behavior when custom objective is also used.See https://xgboost.readthedocs.io/en/latest/tutorials/custom_metric_obj.html for details on the `custom_metric`.\n",
      "c:\\Users\\YanPC\\Downloads\\automl_hw2-master\\.venv\\lib\\site-packages\\xgboost\\training.py:38: UserWarning: `feval` is deprecated, use `custom_metric` instead.  They have different behavior when custom objective is also used.See https://xgboost.readthedocs.io/en/latest/tutorials/custom_metric_obj.html for details on the `custom_metric`.\n",
      "c:\\Users\\YanPC\\Downloads\\automl_hw2-master\\.venv\\lib\\site-packages\\xgboost\\training.py:38: UserWarning: `feval` is deprecated, use `custom_metric` instead.  They have different behavior when custom objective is also used.See https://xgboost.readthedocs.io/en/latest/tutorials/custom_metric_obj.html for details on the `custom_metric`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "19_Xgboost f1 0.775126 trained in 33.23 seconds\n",
      "37_CatBoost f1 0.858021 trained in 34.05 seconds\n",
      "46_RandomForest f1 0.745887 trained in 30.38 seconds\n",
      "55_ExtraTrees f1 0.678313 trained in 26.68 seconds\n",
      "64_NeuralNetwork f1 0.658551 trained in 20.94 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\YanPC\\Downloads\\automl_hw2-master\\.venv\\lib\\site-packages\\supervised\\utils\\learning_curves.py:113: FutureWarning: The behavior of Series.argmax/argmin with skipna=False and NAs, or with all-NAs is deprecated. In a future version this will raise ValueError.\n",
      "c:\\Users\\YanPC\\Downloads\\automl_hw2-master\\.venv\\lib\\site-packages\\supervised\\utils\\learning_curves.py:113: FutureWarning: The behavior of Series.argmax/argmin with skipna=False and NAs, or with all-NAs is deprecated. In a future version this will raise ValueError.\n",
      "c:\\Users\\YanPC\\Downloads\\automl_hw2-master\\.venv\\lib\\site-packages\\supervised\\utils\\learning_curves.py:113: FutureWarning: The behavior of Series.argmax/argmin with skipna=False and NAs, or with all-NAs is deprecated. In a future version this will raise ValueError.\n",
      "c:\\Users\\YanPC\\Downloads\\automl_hw2-master\\.venv\\lib\\site-packages\\supervised\\utils\\learning_curves.py:113: FutureWarning: The behavior of Series.argmax/argmin with skipna=False and NAs, or with all-NAs is deprecated. In a future version this will raise ValueError.\n",
      "c:\\Users\\YanPC\\Downloads\\automl_hw2-master\\.venv\\lib\\site-packages\\supervised\\utils\\learning_curves.py:113: FutureWarning: The behavior of Series.argmax/argmin with skipna=False and NAs, or with all-NAs is deprecated. In a future version this will raise ValueError.\n",
      "c:\\Users\\YanPC\\Downloads\\automl_hw2-master\\.venv\\lib\\site-packages\\supervised\\utils\\learning_curves.py:113: FutureWarning: The behavior of Series.argmax/argmin with skipna=False and NAs, or with all-NAs is deprecated. In a future version this will raise ValueError.\n",
      "c:\\Users\\YanPC\\Downloads\\automl_hw2-master\\.venv\\lib\\site-packages\\supervised\\utils\\learning_curves.py:113: FutureWarning: The behavior of Series.argmax/argmin with skipna=False and NAs, or with all-NAs is deprecated. In a future version this will raise ValueError.\n",
      "c:\\Users\\YanPC\\Downloads\\automl_hw2-master\\.venv\\lib\\site-packages\\supervised\\utils\\learning_curves.py:113: FutureWarning: The behavior of Series.argmax/argmin with skipna=False and NAs, or with all-NAs is deprecated. In a future version this will raise ValueError.\n",
      "c:\\Users\\YanPC\\Downloads\\automl_hw2-master\\.venv\\lib\\site-packages\\supervised\\utils\\learning_curves.py:113: FutureWarning: The behavior of Series.argmax/argmin with skipna=False and NAs, or with all-NAs is deprecated. In a future version this will raise ValueError.\n",
      "c:\\Users\\YanPC\\Downloads\\automl_hw2-master\\.venv\\lib\\site-packages\\supervised\\utils\\learning_curves.py:113: FutureWarning: The behavior of Series.argmax/argmin with skipna=False and NAs, or with all-NAs is deprecated. In a future version this will raise ValueError.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "29_LightGBM f1 0.838789 trained in 26.2 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\YanPC\\Downloads\\automl_hw2-master\\.venv\\lib\\site-packages\\xgboost\\training.py:38: UserWarning: `feval` is deprecated, use `custom_metric` instead.  They have different behavior when custom objective is also used.See https://xgboost.readthedocs.io/en/latest/tutorials/custom_metric_obj.html for details on the `custom_metric`.\n",
      "c:\\Users\\YanPC\\Downloads\\automl_hw2-master\\.venv\\lib\\site-packages\\xgboost\\training.py:38: UserWarning: `feval` is deprecated, use `custom_metric` instead.  They have different behavior when custom objective is also used.See https://xgboost.readthedocs.io/en/latest/tutorials/custom_metric_obj.html for details on the `custom_metric`.\n",
      "c:\\Users\\YanPC\\Downloads\\automl_hw2-master\\.venv\\lib\\site-packages\\xgboost\\training.py:38: UserWarning: `feval` is deprecated, use `custom_metric` instead.  They have different behavior when custom objective is also used.See https://xgboost.readthedocs.io/en/latest/tutorials/custom_metric_obj.html for details on the `custom_metric`.\n",
      "c:\\Users\\YanPC\\Downloads\\automl_hw2-master\\.venv\\lib\\site-packages\\xgboost\\training.py:38: UserWarning: `feval` is deprecated, use `custom_metric` instead.  They have different behavior when custom objective is also used.See https://xgboost.readthedocs.io/en/latest/tutorials/custom_metric_obj.html for details on the `custom_metric`.\n",
      "c:\\Users\\YanPC\\Downloads\\automl_hw2-master\\.venv\\lib\\site-packages\\xgboost\\training.py:38: UserWarning: `feval` is deprecated, use `custom_metric` instead.  They have different behavior when custom objective is also used.See https://xgboost.readthedocs.io/en/latest/tutorials/custom_metric_obj.html for details on the `custom_metric`.\n",
      "c:\\Users\\YanPC\\Downloads\\automl_hw2-master\\.venv\\lib\\site-packages\\xgboost\\training.py:38: UserWarning: `feval` is deprecated, use `custom_metric` instead.  They have different behavior when custom objective is also used.See https://xgboost.readthedocs.io/en/latest/tutorials/custom_metric_obj.html for details on the `custom_metric`.\n",
      "c:\\Users\\YanPC\\Downloads\\automl_hw2-master\\.venv\\lib\\site-packages\\xgboost\\training.py:38: UserWarning: `feval` is deprecated, use `custom_metric` instead.  They have different behavior when custom objective is also used.See https://xgboost.readthedocs.io/en/latest/tutorials/custom_metric_obj.html for details on the `custom_metric`.\n",
      "c:\\Users\\YanPC\\Downloads\\automl_hw2-master\\.venv\\lib\\site-packages\\xgboost\\training.py:38: UserWarning: `feval` is deprecated, use `custom_metric` instead.  They have different behavior when custom objective is also used.See https://xgboost.readthedocs.io/en/latest/tutorials/custom_metric_obj.html for details on the `custom_metric`.\n",
      "c:\\Users\\YanPC\\Downloads\\automl_hw2-master\\.venv\\lib\\site-packages\\xgboost\\training.py:38: UserWarning: `feval` is deprecated, use `custom_metric` instead.  They have different behavior when custom objective is also used.See https://xgboost.readthedocs.io/en/latest/tutorials/custom_metric_obj.html for details on the `custom_metric`.\n",
      "c:\\Users\\YanPC\\Downloads\\automl_hw2-master\\.venv\\lib\\site-packages\\xgboost\\training.py:38: UserWarning: `feval` is deprecated, use `custom_metric` instead.  They have different behavior when custom objective is also used.See https://xgboost.readthedocs.io/en/latest/tutorials/custom_metric_obj.html for details on the `custom_metric`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20_Xgboost f1 0.750157 trained in 38.77 seconds\n",
      "38_CatBoost f1 0.824455 trained in 22.71 seconds\n",
      "47_RandomForest f1 0.774074 trained in 27.57 seconds\n",
      "56_ExtraTrees f1 0.734644 trained in 25.97 seconds\n",
      "65_NeuralNetwork f1 0.664935 trained in 21.36 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\YanPC\\Downloads\\automl_hw2-master\\.venv\\lib\\site-packages\\supervised\\utils\\learning_curves.py:113: FutureWarning: The behavior of Series.argmax/argmin with skipna=False and NAs, or with all-NAs is deprecated. In a future version this will raise ValueError.\n",
      "c:\\Users\\YanPC\\Downloads\\automl_hw2-master\\.venv\\lib\\site-packages\\supervised\\utils\\learning_curves.py:113: FutureWarning: The behavior of Series.argmax/argmin with skipna=False and NAs, or with all-NAs is deprecated. In a future version this will raise ValueError.\n",
      "c:\\Users\\YanPC\\Downloads\\automl_hw2-master\\.venv\\lib\\site-packages\\supervised\\utils\\learning_curves.py:113: FutureWarning: The behavior of Series.argmax/argmin with skipna=False and NAs, or with all-NAs is deprecated. In a future version this will raise ValueError.\n",
      "c:\\Users\\YanPC\\Downloads\\automl_hw2-master\\.venv\\lib\\site-packages\\supervised\\utils\\learning_curves.py:113: FutureWarning: The behavior of Series.argmax/argmin with skipna=False and NAs, or with all-NAs is deprecated. In a future version this will raise ValueError.\n",
      "c:\\Users\\YanPC\\Downloads\\automl_hw2-master\\.venv\\lib\\site-packages\\supervised\\utils\\learning_curves.py:113: FutureWarning: The behavior of Series.argmax/argmin with skipna=False and NAs, or with all-NAs is deprecated. In a future version this will raise ValueError.\n",
      "c:\\Users\\YanPC\\Downloads\\automl_hw2-master\\.venv\\lib\\site-packages\\supervised\\utils\\learning_curves.py:113: FutureWarning: The behavior of Series.argmax/argmin with skipna=False and NAs, or with all-NAs is deprecated. In a future version this will raise ValueError.\n",
      "c:\\Users\\YanPC\\Downloads\\automl_hw2-master\\.venv\\lib\\site-packages\\supervised\\utils\\learning_curves.py:113: FutureWarning: The behavior of Series.argmax/argmin with skipna=False and NAs, or with all-NAs is deprecated. In a future version this will raise ValueError.\n",
      "c:\\Users\\YanPC\\Downloads\\automl_hw2-master\\.venv\\lib\\site-packages\\supervised\\utils\\learning_curves.py:113: FutureWarning: The behavior of Series.argmax/argmin with skipna=False and NAs, or with all-NAs is deprecated. In a future version this will raise ValueError.\n",
      "c:\\Users\\YanPC\\Downloads\\automl_hw2-master\\.venv\\lib\\site-packages\\supervised\\utils\\learning_curves.py:113: FutureWarning: The behavior of Series.argmax/argmin with skipna=False and NAs, or with all-NAs is deprecated. In a future version this will raise ValueError.\n",
      "c:\\Users\\YanPC\\Downloads\\automl_hw2-master\\.venv\\lib\\site-packages\\supervised\\utils\\learning_curves.py:113: FutureWarning: The behavior of Series.argmax/argmin with skipna=False and NAs, or with all-NAs is deprecated. In a future version this will raise ValueError.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "* Step golden_features will try to check up to 3 models\n",
      "None 10\n",
      "Add Golden Feature: feature_22_sum_feature_2\n",
      "Add Golden Feature: feature_11_diff_feature_12\n",
      "Add Golden Feature: feature_8_multiply_feature_2\n",
      "Add Golden Feature: feature_21_multiply_feature_3\n",
      "Add Golden Feature: feature_5_diff_feature_21\n",
      "Add Golden Feature: feature_17_diff_feature_20\n",
      "Add Golden Feature: feature_15_ratio_feature_11\n",
      "Add Golden Feature: feature_11_ratio_feature_15\n",
      "Add Golden Feature: feature_4_diff_feature_17\n",
      "Add Golden Feature: feature_11_sum_feature_1\n",
      "Created 10 Golden Features in 15.2 seconds.\n",
      "37_CatBoost_GoldenFeatures f1 0.84469 trained in 51.54 seconds\n",
      "36_CatBoost_GoldenFeatures f1 0.837787 trained in 30.92 seconds\n",
      "32_CatBoost_GoldenFeatures f1 0.83839 trained in 61.59 seconds\n",
      "* Step kmeans_features will try to check up to 3 models\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\YanPC\\Downloads\\automl_hw2-master\\.venv\\lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1934: FutureWarning: The default value of `n_init` will change from 3 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n",
      "c:\\Users\\YanPC\\Downloads\\automl_hw2-master\\.venv\\lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1934: FutureWarning: The default value of `n_init` will change from 3 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n",
      "c:\\Users\\YanPC\\Downloads\\automl_hw2-master\\.venv\\lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1934: FutureWarning: The default value of `n_init` will change from 3 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n",
      "c:\\Users\\YanPC\\Downloads\\automl_hw2-master\\.venv\\lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1934: FutureWarning: The default value of `n_init` will change from 3 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n",
      "c:\\Users\\YanPC\\Downloads\\automl_hw2-master\\.venv\\lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1934: FutureWarning: The default value of `n_init` will change from 3 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n",
      "c:\\Users\\YanPC\\Downloads\\automl_hw2-master\\.venv\\lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1934: FutureWarning: The default value of `n_init` will change from 3 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n",
      "c:\\Users\\YanPC\\Downloads\\automl_hw2-master\\.venv\\lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1934: FutureWarning: The default value of `n_init` will change from 3 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n",
      "c:\\Users\\YanPC\\Downloads\\automl_hw2-master\\.venv\\lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1934: FutureWarning: The default value of `n_init` will change from 3 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n",
      "c:\\Users\\YanPC\\Downloads\\automl_hw2-master\\.venv\\lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1934: FutureWarning: The default value of `n_init` will change from 3 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n",
      "c:\\Users\\YanPC\\Downloads\\automl_hw2-master\\.venv\\lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1934: FutureWarning: The default value of `n_init` will change from 3 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "37_CatBoost_KMeansFeatures f1 0.833542 trained in 44.19 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\YanPC\\Downloads\\automl_hw2-master\\.venv\\lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1934: FutureWarning: The default value of `n_init` will change from 3 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n",
      "c:\\Users\\YanPC\\Downloads\\automl_hw2-master\\.venv\\lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1934: FutureWarning: The default value of `n_init` will change from 3 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n",
      "c:\\Users\\YanPC\\Downloads\\automl_hw2-master\\.venv\\lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1934: FutureWarning: The default value of `n_init` will change from 3 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n",
      "c:\\Users\\YanPC\\Downloads\\automl_hw2-master\\.venv\\lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1934: FutureWarning: The default value of `n_init` will change from 3 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n",
      "c:\\Users\\YanPC\\Downloads\\automl_hw2-master\\.venv\\lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1934: FutureWarning: The default value of `n_init` will change from 3 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n",
      "c:\\Users\\YanPC\\Downloads\\automl_hw2-master\\.venv\\lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1934: FutureWarning: The default value of `n_init` will change from 3 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n",
      "c:\\Users\\YanPC\\Downloads\\automl_hw2-master\\.venv\\lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1934: FutureWarning: The default value of `n_init` will change from 3 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n",
      "c:\\Users\\YanPC\\Downloads\\automl_hw2-master\\.venv\\lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1934: FutureWarning: The default value of `n_init` will change from 3 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n",
      "c:\\Users\\YanPC\\Downloads\\automl_hw2-master\\.venv\\lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1934: FutureWarning: The default value of `n_init` will change from 3 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n",
      "c:\\Users\\YanPC\\Downloads\\automl_hw2-master\\.venv\\lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1934: FutureWarning: The default value of `n_init` will change from 3 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "36_CatBoost_KMeansFeatures f1 0.809435 trained in 37.11 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\YanPC\\Downloads\\automl_hw2-master\\.venv\\lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1934: FutureWarning: The default value of `n_init` will change from 3 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n",
      "c:\\Users\\YanPC\\Downloads\\automl_hw2-master\\.venv\\lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1934: FutureWarning: The default value of `n_init` will change from 3 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n",
      "c:\\Users\\YanPC\\Downloads\\automl_hw2-master\\.venv\\lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1934: FutureWarning: The default value of `n_init` will change from 3 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n",
      "c:\\Users\\YanPC\\Downloads\\automl_hw2-master\\.venv\\lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1934: FutureWarning: The default value of `n_init` will change from 3 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n",
      "c:\\Users\\YanPC\\Downloads\\automl_hw2-master\\.venv\\lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1934: FutureWarning: The default value of `n_init` will change from 3 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n",
      "c:\\Users\\YanPC\\Downloads\\automl_hw2-master\\.venv\\lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1934: FutureWarning: The default value of `n_init` will change from 3 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n",
      "c:\\Users\\YanPC\\Downloads\\automl_hw2-master\\.venv\\lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1934: FutureWarning: The default value of `n_init` will change from 3 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n",
      "c:\\Users\\YanPC\\Downloads\\automl_hw2-master\\.venv\\lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1934: FutureWarning: The default value of `n_init` will change from 3 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n",
      "c:\\Users\\YanPC\\Downloads\\automl_hw2-master\\.venv\\lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1934: FutureWarning: The default value of `n_init` will change from 3 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n",
      "c:\\Users\\YanPC\\Downloads\\automl_hw2-master\\.venv\\lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1934: FutureWarning: The default value of `n_init` will change from 3 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "32_CatBoost_KMeansFeatures f1 0.815546 trained in 85.24 seconds\n",
      "* Step insert_random_feature will try to check up to 1 model\n",
      "37_CatBoost_RandomFeature f1 0.848225 trained in 47.64 seconds\n",
      "Drop features ['feature_25', 'feature_5', 'feature_10', 'feature_12', 'feature_13', 'feature_7', 'feature_17', 'feature_20', 'feature_21', 'random_feature', 'feature_24', 'feature_18', 'feature_8']\n",
      "* Step features_selection will try to check up to 6 models\n",
      "37_CatBoost_SelectedFeatures f1 0.871007 trained in 28.64 seconds\n",
      "23_LightGBM_SelectedFeatures f1 0.85625 trained in 28.9 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\YanPC\\Downloads\\automl_hw2-master\\.venv\\lib\\site-packages\\xgboost\\training.py:38: UserWarning: `feval` is deprecated, use `custom_metric` instead.  They have different behavior when custom objective is also used.See https://xgboost.readthedocs.io/en/latest/tutorials/custom_metric_obj.html for details on the `custom_metric`.\n",
      "c:\\Users\\YanPC\\Downloads\\automl_hw2-master\\.venv\\lib\\site-packages\\xgboost\\training.py:38: UserWarning: `feval` is deprecated, use `custom_metric` instead.  They have different behavior when custom objective is also used.See https://xgboost.readthedocs.io/en/latest/tutorials/custom_metric_obj.html for details on the `custom_metric`.\n",
      "c:\\Users\\YanPC\\Downloads\\automl_hw2-master\\.venv\\lib\\site-packages\\xgboost\\training.py:38: UserWarning: `feval` is deprecated, use `custom_metric` instead.  They have different behavior when custom objective is also used.See https://xgboost.readthedocs.io/en/latest/tutorials/custom_metric_obj.html for details on the `custom_metric`.\n",
      "c:\\Users\\YanPC\\Downloads\\automl_hw2-master\\.venv\\lib\\site-packages\\xgboost\\training.py:38: UserWarning: `feval` is deprecated, use `custom_metric` instead.  They have different behavior when custom objective is also used.See https://xgboost.readthedocs.io/en/latest/tutorials/custom_metric_obj.html for details on the `custom_metric`.\n",
      "c:\\Users\\YanPC\\Downloads\\automl_hw2-master\\.venv\\lib\\site-packages\\xgboost\\training.py:38: UserWarning: `feval` is deprecated, use `custom_metric` instead.  They have different behavior when custom objective is also used.See https://xgboost.readthedocs.io/en/latest/tutorials/custom_metric_obj.html for details on the `custom_metric`.\n",
      "c:\\Users\\YanPC\\Downloads\\automl_hw2-master\\.venv\\lib\\site-packages\\xgboost\\training.py:38: UserWarning: `feval` is deprecated, use `custom_metric` instead.  They have different behavior when custom objective is also used.See https://xgboost.readthedocs.io/en/latest/tutorials/custom_metric_obj.html for details on the `custom_metric`.\n",
      "c:\\Users\\YanPC\\Downloads\\automl_hw2-master\\.venv\\lib\\site-packages\\xgboost\\training.py:38: UserWarning: `feval` is deprecated, use `custom_metric` instead.  They have different behavior when custom objective is also used.See https://xgboost.readthedocs.io/en/latest/tutorials/custom_metric_obj.html for details on the `custom_metric`.\n",
      "c:\\Users\\YanPC\\Downloads\\automl_hw2-master\\.venv\\lib\\site-packages\\xgboost\\training.py:38: UserWarning: `feval` is deprecated, use `custom_metric` instead.  They have different behavior when custom objective is also used.See https://xgboost.readthedocs.io/en/latest/tutorials/custom_metric_obj.html for details on the `custom_metric`.\n",
      "c:\\Users\\YanPC\\Downloads\\automl_hw2-master\\.venv\\lib\\site-packages\\xgboost\\training.py:38: UserWarning: `feval` is deprecated, use `custom_metric` instead.  They have different behavior when custom objective is also used.See https://xgboost.readthedocs.io/en/latest/tutorials/custom_metric_obj.html for details on the `custom_metric`.\n",
      "c:\\Users\\YanPC\\Downloads\\automl_hw2-master\\.venv\\lib\\site-packages\\xgboost\\training.py:38: UserWarning: `feval` is deprecated, use `custom_metric` instead.  They have different behavior when custom objective is also used.See https://xgboost.readthedocs.io/en/latest/tutorials/custom_metric_obj.html for details on the `custom_metric`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6_Default_Xgboost_SelectedFeatures f1 0.827671 trained in 29.82 seconds\n",
      "45_RandomForest_SelectedFeatures f1 0.813559 trained in 32.02 seconds\n",
      "48_ExtraTrees_SelectedFeatures f1 0.789306 trained in 26.51 seconds\n",
      "58_NeuralNetwork_SelectedFeatures f1 0.707535 trained in 23.55 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\YanPC\\Downloads\\automl_hw2-master\\.venv\\lib\\site-packages\\supervised\\utils\\learning_curves.py:113: FutureWarning: The behavior of Series.argmax/argmin with skipna=False and NAs, or with all-NAs is deprecated. In a future version this will raise ValueError.\n",
      "c:\\Users\\YanPC\\Downloads\\automl_hw2-master\\.venv\\lib\\site-packages\\supervised\\utils\\learning_curves.py:113: FutureWarning: The behavior of Series.argmax/argmin with skipna=False and NAs, or with all-NAs is deprecated. In a future version this will raise ValueError.\n",
      "c:\\Users\\YanPC\\Downloads\\automl_hw2-master\\.venv\\lib\\site-packages\\supervised\\utils\\learning_curves.py:113: FutureWarning: The behavior of Series.argmax/argmin with skipna=False and NAs, or with all-NAs is deprecated. In a future version this will raise ValueError.\n",
      "c:\\Users\\YanPC\\Downloads\\automl_hw2-master\\.venv\\lib\\site-packages\\supervised\\utils\\learning_curves.py:113: FutureWarning: The behavior of Series.argmax/argmin with skipna=False and NAs, or with all-NAs is deprecated. In a future version this will raise ValueError.\n",
      "c:\\Users\\YanPC\\Downloads\\automl_hw2-master\\.venv\\lib\\site-packages\\supervised\\utils\\learning_curves.py:113: FutureWarning: The behavior of Series.argmax/argmin with skipna=False and NAs, or with all-NAs is deprecated. In a future version this will raise ValueError.\n",
      "c:\\Users\\YanPC\\Downloads\\automl_hw2-master\\.venv\\lib\\site-packages\\supervised\\utils\\learning_curves.py:113: FutureWarning: The behavior of Series.argmax/argmin with skipna=False and NAs, or with all-NAs is deprecated. In a future version this will raise ValueError.\n",
      "c:\\Users\\YanPC\\Downloads\\automl_hw2-master\\.venv\\lib\\site-packages\\supervised\\utils\\learning_curves.py:113: FutureWarning: The behavior of Series.argmax/argmin with skipna=False and NAs, or with all-NAs is deprecated. In a future version this will raise ValueError.\n",
      "c:\\Users\\YanPC\\Downloads\\automl_hw2-master\\.venv\\lib\\site-packages\\supervised\\utils\\learning_curves.py:113: FutureWarning: The behavior of Series.argmax/argmin with skipna=False and NAs, or with all-NAs is deprecated. In a future version this will raise ValueError.\n",
      "c:\\Users\\YanPC\\Downloads\\automl_hw2-master\\.venv\\lib\\site-packages\\supervised\\utils\\learning_curves.py:113: FutureWarning: The behavior of Series.argmax/argmin with skipna=False and NAs, or with all-NAs is deprecated. In a future version this will raise ValueError.\n",
      "c:\\Users\\YanPC\\Downloads\\automl_hw2-master\\.venv\\lib\\site-packages\\supervised\\utils\\learning_curves.py:113: FutureWarning: The behavior of Series.argmax/argmin with skipna=False and NAs, or with all-NAs is deprecated. In a future version this will raise ValueError.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "* Step hill_climbing_1 will try to check up to 28 models\n",
      "73_CatBoost_SelectedFeatures f1 0.865727 trained in 29.24 seconds\n",
      "74_CatBoost_SelectedFeatures f1 0.874222 trained in 30.58 seconds\n",
      "75_CatBoost f1 0.844743 trained in 32.25 seconds\n",
      "76_CatBoost f1 0.848485 trained in 37.86 seconds\n",
      "77_LightGBM_SelectedFeatures f1 0.85625 trained in 30.1 seconds\n",
      "78_LightGBM_SelectedFeatures f1 0.85625 trained in 30.24 seconds\n",
      "79_CatBoost f1 0.843924 trained in 26.68 seconds\n",
      "80_LightGBM f1 0.838988 trained in 26.92 seconds\n",
      "81_LightGBM f1 0.838988 trained in 26.98 seconds\n",
      "82_LightGBM f1 0.838789 trained in 27.23 seconds\n",
      "83_LightGBM f1 0.838789 trained in 28.38 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\YanPC\\Downloads\\automl_hw2-master\\.venv\\lib\\site-packages\\xgboost\\training.py:38: UserWarning: `feval` is deprecated, use `custom_metric` instead.  They have different behavior when custom objective is also used.See https://xgboost.readthedocs.io/en/latest/tutorials/custom_metric_obj.html for details on the `custom_metric`.\n",
      "c:\\Users\\YanPC\\Downloads\\automl_hw2-master\\.venv\\lib\\site-packages\\xgboost\\training.py:38: UserWarning: `feval` is deprecated, use `custom_metric` instead.  They have different behavior when custom objective is also used.See https://xgboost.readthedocs.io/en/latest/tutorials/custom_metric_obj.html for details on the `custom_metric`.\n",
      "c:\\Users\\YanPC\\Downloads\\automl_hw2-master\\.venv\\lib\\site-packages\\xgboost\\training.py:38: UserWarning: `feval` is deprecated, use `custom_metric` instead.  They have different behavior when custom objective is also used.See https://xgboost.readthedocs.io/en/latest/tutorials/custom_metric_obj.html for details on the `custom_metric`.\n",
      "c:\\Users\\YanPC\\Downloads\\automl_hw2-master\\.venv\\lib\\site-packages\\xgboost\\training.py:38: UserWarning: `feval` is deprecated, use `custom_metric` instead.  They have different behavior when custom objective is also used.See https://xgboost.readthedocs.io/en/latest/tutorials/custom_metric_obj.html for details on the `custom_metric`.\n",
      "c:\\Users\\YanPC\\Downloads\\automl_hw2-master\\.venv\\lib\\site-packages\\xgboost\\training.py:38: UserWarning: `feval` is deprecated, use `custom_metric` instead.  They have different behavior when custom objective is also used.See https://xgboost.readthedocs.io/en/latest/tutorials/custom_metric_obj.html for details on the `custom_metric`.\n",
      "c:\\Users\\YanPC\\Downloads\\automl_hw2-master\\.venv\\lib\\site-packages\\xgboost\\training.py:38: UserWarning: `feval` is deprecated, use `custom_metric` instead.  They have different behavior when custom objective is also used.See https://xgboost.readthedocs.io/en/latest/tutorials/custom_metric_obj.html for details on the `custom_metric`.\n",
      "c:\\Users\\YanPC\\Downloads\\automl_hw2-master\\.venv\\lib\\site-packages\\xgboost\\training.py:38: UserWarning: `feval` is deprecated, use `custom_metric` instead.  They have different behavior when custom objective is also used.See https://xgboost.readthedocs.io/en/latest/tutorials/custom_metric_obj.html for details on the `custom_metric`.\n",
      "c:\\Users\\YanPC\\Downloads\\automl_hw2-master\\.venv\\lib\\site-packages\\xgboost\\training.py:38: UserWarning: `feval` is deprecated, use `custom_metric` instead.  They have different behavior when custom objective is also used.See https://xgboost.readthedocs.io/en/latest/tutorials/custom_metric_obj.html for details on the `custom_metric`.\n",
      "c:\\Users\\YanPC\\Downloads\\automl_hw2-master\\.venv\\lib\\site-packages\\xgboost\\training.py:38: UserWarning: `feval` is deprecated, use `custom_metric` instead.  They have different behavior when custom objective is also used.See https://xgboost.readthedocs.io/en/latest/tutorials/custom_metric_obj.html for details on the `custom_metric`.\n",
      "c:\\Users\\YanPC\\Downloads\\automl_hw2-master\\.venv\\lib\\site-packages\\xgboost\\training.py:38: UserWarning: `feval` is deprecated, use `custom_metric` instead.  They have different behavior when custom objective is also used.See https://xgboost.readthedocs.io/en/latest/tutorials/custom_metric_obj.html for details on the `custom_metric`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "84_Xgboost_SelectedFeatures f1 0.839875 trained in 33.74 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\YanPC\\Downloads\\automl_hw2-master\\.venv\\lib\\site-packages\\xgboost\\training.py:38: UserWarning: `feval` is deprecated, use `custom_metric` instead.  They have different behavior when custom objective is also used.See https://xgboost.readthedocs.io/en/latest/tutorials/custom_metric_obj.html for details on the `custom_metric`.\n",
      "c:\\Users\\YanPC\\Downloads\\automl_hw2-master\\.venv\\lib\\site-packages\\xgboost\\training.py:38: UserWarning: `feval` is deprecated, use `custom_metric` instead.  They have different behavior when custom objective is also used.See https://xgboost.readthedocs.io/en/latest/tutorials/custom_metric_obj.html for details on the `custom_metric`.\n",
      "c:\\Users\\YanPC\\Downloads\\automl_hw2-master\\.venv\\lib\\site-packages\\xgboost\\training.py:38: UserWarning: `feval` is deprecated, use `custom_metric` instead.  They have different behavior when custom objective is also used.See https://xgboost.readthedocs.io/en/latest/tutorials/custom_metric_obj.html for details on the `custom_metric`.\n",
      "c:\\Users\\YanPC\\Downloads\\automl_hw2-master\\.venv\\lib\\site-packages\\xgboost\\training.py:38: UserWarning: `feval` is deprecated, use `custom_metric` instead.  They have different behavior when custom objective is also used.See https://xgboost.readthedocs.io/en/latest/tutorials/custom_metric_obj.html for details on the `custom_metric`.\n",
      "c:\\Users\\YanPC\\Downloads\\automl_hw2-master\\.venv\\lib\\site-packages\\xgboost\\training.py:38: UserWarning: `feval` is deprecated, use `custom_metric` instead.  They have different behavior when custom objective is also used.See https://xgboost.readthedocs.io/en/latest/tutorials/custom_metric_obj.html for details on the `custom_metric`.\n",
      "c:\\Users\\YanPC\\Downloads\\automl_hw2-master\\.venv\\lib\\site-packages\\xgboost\\training.py:38: UserWarning: `feval` is deprecated, use `custom_metric` instead.  They have different behavior when custom objective is also used.See https://xgboost.readthedocs.io/en/latest/tutorials/custom_metric_obj.html for details on the `custom_metric`.\n",
      "c:\\Users\\YanPC\\Downloads\\automl_hw2-master\\.venv\\lib\\site-packages\\xgboost\\training.py:38: UserWarning: `feval` is deprecated, use `custom_metric` instead.  They have different behavior when custom objective is also used.See https://xgboost.readthedocs.io/en/latest/tutorials/custom_metric_obj.html for details on the `custom_metric`.\n",
      "c:\\Users\\YanPC\\Downloads\\automl_hw2-master\\.venv\\lib\\site-packages\\xgboost\\training.py:38: UserWarning: `feval` is deprecated, use `custom_metric` instead.  They have different behavior when custom objective is also used.See https://xgboost.readthedocs.io/en/latest/tutorials/custom_metric_obj.html for details on the `custom_metric`.\n",
      "c:\\Users\\YanPC\\Downloads\\automl_hw2-master\\.venv\\lib\\site-packages\\xgboost\\training.py:38: UserWarning: `feval` is deprecated, use `custom_metric` instead.  They have different behavior when custom objective is also used.See https://xgboost.readthedocs.io/en/latest/tutorials/custom_metric_obj.html for details on the `custom_metric`.\n",
      "c:\\Users\\YanPC\\Downloads\\automl_hw2-master\\.venv\\lib\\site-packages\\xgboost\\training.py:38: UserWarning: `feval` is deprecated, use `custom_metric` instead.  They have different behavior when custom objective is also used.See https://xgboost.readthedocs.io/en/latest/tutorials/custom_metric_obj.html for details on the `custom_metric`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "85_Xgboost f1 0.82419 trained in 35.24 seconds\n",
      "86_RandomForest f1 0.80803 trained in 38.83 seconds\n",
      "87_RandomForest_SelectedFeatures f1 0.820737 trained in 30.29 seconds\n",
      "88_RandomForest f1 0.814491 trained in 36.12 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\YanPC\\Downloads\\automl_hw2-master\\.venv\\lib\\site-packages\\xgboost\\training.py:38: UserWarning: `feval` is deprecated, use `custom_metric` instead.  They have different behavior when custom objective is also used.See https://xgboost.readthedocs.io/en/latest/tutorials/custom_metric_obj.html for details on the `custom_metric`.\n",
      "c:\\Users\\YanPC\\Downloads\\automl_hw2-master\\.venv\\lib\\site-packages\\xgboost\\training.py:38: UserWarning: `feval` is deprecated, use `custom_metric` instead.  They have different behavior when custom objective is also used.See https://xgboost.readthedocs.io/en/latest/tutorials/custom_metric_obj.html for details on the `custom_metric`.\n",
      "c:\\Users\\YanPC\\Downloads\\automl_hw2-master\\.venv\\lib\\site-packages\\xgboost\\training.py:38: UserWarning: `feval` is deprecated, use `custom_metric` instead.  They have different behavior when custom objective is also used.See https://xgboost.readthedocs.io/en/latest/tutorials/custom_metric_obj.html for details on the `custom_metric`.\n",
      "c:\\Users\\YanPC\\Downloads\\automl_hw2-master\\.venv\\lib\\site-packages\\xgboost\\training.py:38: UserWarning: `feval` is deprecated, use `custom_metric` instead.  They have different behavior when custom objective is also used.See https://xgboost.readthedocs.io/en/latest/tutorials/custom_metric_obj.html for details on the `custom_metric`.\n",
      "c:\\Users\\YanPC\\Downloads\\automl_hw2-master\\.venv\\lib\\site-packages\\xgboost\\training.py:38: UserWarning: `feval` is deprecated, use `custom_metric` instead.  They have different behavior when custom objective is also used.See https://xgboost.readthedocs.io/en/latest/tutorials/custom_metric_obj.html for details on the `custom_metric`.\n",
      "c:\\Users\\YanPC\\Downloads\\automl_hw2-master\\.venv\\lib\\site-packages\\xgboost\\training.py:38: UserWarning: `feval` is deprecated, use `custom_metric` instead.  They have different behavior when custom objective is also used.See https://xgboost.readthedocs.io/en/latest/tutorials/custom_metric_obj.html for details on the `custom_metric`.\n",
      "c:\\Users\\YanPC\\Downloads\\automl_hw2-master\\.venv\\lib\\site-packages\\xgboost\\training.py:38: UserWarning: `feval` is deprecated, use `custom_metric` instead.  They have different behavior when custom objective is also used.See https://xgboost.readthedocs.io/en/latest/tutorials/custom_metric_obj.html for details on the `custom_metric`.\n",
      "c:\\Users\\YanPC\\Downloads\\automl_hw2-master\\.venv\\lib\\site-packages\\xgboost\\training.py:38: UserWarning: `feval` is deprecated, use `custom_metric` instead.  They have different behavior when custom objective is also used.See https://xgboost.readthedocs.io/en/latest/tutorials/custom_metric_obj.html for details on the `custom_metric`.\n",
      "c:\\Users\\YanPC\\Downloads\\automl_hw2-master\\.venv\\lib\\site-packages\\xgboost\\training.py:38: UserWarning: `feval` is deprecated, use `custom_metric` instead.  They have different behavior when custom objective is also used.See https://xgboost.readthedocs.io/en/latest/tutorials/custom_metric_obj.html for details on the `custom_metric`.\n",
      "c:\\Users\\YanPC\\Downloads\\automl_hw2-master\\.venv\\lib\\site-packages\\xgboost\\training.py:38: UserWarning: `feval` is deprecated, use `custom_metric` instead.  They have different behavior when custom objective is also used.See https://xgboost.readthedocs.io/en/latest/tutorials/custom_metric_obj.html for details on the `custom_metric`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "89_Xgboost f1 0.786802 trained in 36.68 seconds\n",
      "90_ExtraTrees_SelectedFeatures f1 0.79156 trained in 29.59 seconds\n",
      "91_ExtraTrees f1 0.780269 trained in 30.98 seconds\n",
      "92_ExtraTrees f1 0.765319 trained in 33.46 seconds\n",
      "93_DecisionTree f1 0.688807 trained in 20.1 seconds\n",
      "94_NeuralNetwork_SelectedFeatures f1 0.749842 trained in 27.06 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\YanPC\\Downloads\\automl_hw2-master\\.venv\\lib\\site-packages\\supervised\\utils\\learning_curves.py:113: FutureWarning: The behavior of Series.argmax/argmin with skipna=False and NAs, or with all-NAs is deprecated. In a future version this will raise ValueError.\n",
      "c:\\Users\\YanPC\\Downloads\\automl_hw2-master\\.venv\\lib\\site-packages\\supervised\\utils\\learning_curves.py:113: FutureWarning: The behavior of Series.argmax/argmin with skipna=False and NAs, or with all-NAs is deprecated. In a future version this will raise ValueError.\n",
      "c:\\Users\\YanPC\\Downloads\\automl_hw2-master\\.venv\\lib\\site-packages\\supervised\\utils\\learning_curves.py:113: FutureWarning: The behavior of Series.argmax/argmin with skipna=False and NAs, or with all-NAs is deprecated. In a future version this will raise ValueError.\n",
      "c:\\Users\\YanPC\\Downloads\\automl_hw2-master\\.venv\\lib\\site-packages\\supervised\\utils\\learning_curves.py:113: FutureWarning: The behavior of Series.argmax/argmin with skipna=False and NAs, or with all-NAs is deprecated. In a future version this will raise ValueError.\n",
      "c:\\Users\\YanPC\\Downloads\\automl_hw2-master\\.venv\\lib\\site-packages\\supervised\\utils\\learning_curves.py:113: FutureWarning: The behavior of Series.argmax/argmin with skipna=False and NAs, or with all-NAs is deprecated. In a future version this will raise ValueError.\n",
      "c:\\Users\\YanPC\\Downloads\\automl_hw2-master\\.venv\\lib\\site-packages\\supervised\\utils\\learning_curves.py:113: FutureWarning: The behavior of Series.argmax/argmin with skipna=False and NAs, or with all-NAs is deprecated. In a future version this will raise ValueError.\n",
      "c:\\Users\\YanPC\\Downloads\\automl_hw2-master\\.venv\\lib\\site-packages\\supervised\\utils\\learning_curves.py:113: FutureWarning: The behavior of Series.argmax/argmin with skipna=False and NAs, or with all-NAs is deprecated. In a future version this will raise ValueError.\n",
      "c:\\Users\\YanPC\\Downloads\\automl_hw2-master\\.venv\\lib\\site-packages\\supervised\\utils\\learning_curves.py:113: FutureWarning: The behavior of Series.argmax/argmin with skipna=False and NAs, or with all-NAs is deprecated. In a future version this will raise ValueError.\n",
      "c:\\Users\\YanPC\\Downloads\\automl_hw2-master\\.venv\\lib\\site-packages\\supervised\\utils\\learning_curves.py:113: FutureWarning: The behavior of Series.argmax/argmin with skipna=False and NAs, or with all-NAs is deprecated. In a future version this will raise ValueError.\n",
      "c:\\Users\\YanPC\\Downloads\\automl_hw2-master\\.venv\\lib\\site-packages\\supervised\\utils\\learning_curves.py:113: FutureWarning: The behavior of Series.argmax/argmin with skipna=False and NAs, or with all-NAs is deprecated. In a future version this will raise ValueError.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "95_NeuralNetwork f1 0.700129 trained in 25.74 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\YanPC\\Downloads\\automl_hw2-master\\.venv\\lib\\site-packages\\supervised\\utils\\learning_curves.py:113: FutureWarning: The behavior of Series.argmax/argmin with skipna=False and NAs, or with all-NAs is deprecated. In a future version this will raise ValueError.\n",
      "c:\\Users\\YanPC\\Downloads\\automl_hw2-master\\.venv\\lib\\site-packages\\supervised\\utils\\learning_curves.py:113: FutureWarning: The behavior of Series.argmax/argmin with skipna=False and NAs, or with all-NAs is deprecated. In a future version this will raise ValueError.\n",
      "c:\\Users\\YanPC\\Downloads\\automl_hw2-master\\.venv\\lib\\site-packages\\supervised\\utils\\learning_curves.py:113: FutureWarning: The behavior of Series.argmax/argmin with skipna=False and NAs, or with all-NAs is deprecated. In a future version this will raise ValueError.\n",
      "c:\\Users\\YanPC\\Downloads\\automl_hw2-master\\.venv\\lib\\site-packages\\supervised\\utils\\learning_curves.py:113: FutureWarning: The behavior of Series.argmax/argmin with skipna=False and NAs, or with all-NAs is deprecated. In a future version this will raise ValueError.\n",
      "c:\\Users\\YanPC\\Downloads\\automl_hw2-master\\.venv\\lib\\site-packages\\supervised\\utils\\learning_curves.py:113: FutureWarning: The behavior of Series.argmax/argmin with skipna=False and NAs, or with all-NAs is deprecated. In a future version this will raise ValueError.\n",
      "c:\\Users\\YanPC\\Downloads\\automl_hw2-master\\.venv\\lib\\site-packages\\supervised\\utils\\learning_curves.py:113: FutureWarning: The behavior of Series.argmax/argmin with skipna=False and NAs, or with all-NAs is deprecated. In a future version this will raise ValueError.\n",
      "c:\\Users\\YanPC\\Downloads\\automl_hw2-master\\.venv\\lib\\site-packages\\supervised\\utils\\learning_curves.py:113: FutureWarning: The behavior of Series.argmax/argmin with skipna=False and NAs, or with all-NAs is deprecated. In a future version this will raise ValueError.\n",
      "c:\\Users\\YanPC\\Downloads\\automl_hw2-master\\.venv\\lib\\site-packages\\supervised\\utils\\learning_curves.py:113: FutureWarning: The behavior of Series.argmax/argmin with skipna=False and NAs, or with all-NAs is deprecated. In a future version this will raise ValueError.\n",
      "c:\\Users\\YanPC\\Downloads\\automl_hw2-master\\.venv\\lib\\site-packages\\supervised\\utils\\learning_curves.py:113: FutureWarning: The behavior of Series.argmax/argmin with skipna=False and NAs, or with all-NAs is deprecated. In a future version this will raise ValueError.\n",
      "c:\\Users\\YanPC\\Downloads\\automl_hw2-master\\.venv\\lib\\site-packages\\supervised\\utils\\learning_curves.py:113: FutureWarning: The behavior of Series.argmax/argmin with skipna=False and NAs, or with all-NAs is deprecated. In a future version this will raise ValueError.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "96_DecisionTree f1 0.688807 trained in 20.68 seconds\n",
      "97_NearestNeighbors f1 0.652527 trained in 22.73 seconds\n",
      "98_NearestNeighbors f1 0.652527 trained in 22.87 seconds\n",
      "99_NearestNeighbors f1 0.639112 trained in 33.12 seconds\n",
      "100_DecisionTree f1 0.706651 trained in 23.31 seconds\n",
      "* Step hill_climbing_2 will try to check up to 12 models\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[32], line 11\u001b[0m\n\u001b[0;32m      9\u001b[0m train_y \u001b[38;5;241m=\u001b[39m train_y\u001b[38;5;241m.\u001b[39mcopy()\u001b[38;5;241m.\u001b[39mreshape(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m     10\u001b[0m \u001b[38;5;28mprint\u001b[39m(train_y)\n\u001b[1;32m---> 11\u001b[0m \u001b[43mautoml\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain_x\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcopy\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_y\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\YanPC\\Downloads\\automl_hw2-master\\.venv\\lib\\site-packages\\supervised\\automl.py:433\u001b[0m, in \u001b[0;36mAutoML.fit\u001b[1;34m(self, X, y, sample_weight, cv, sensitive_features)\u001b[0m\n\u001b[0;32m    406\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mfit\u001b[39m(\n\u001b[0;32m    407\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m    408\u001b[0m     X: Union[numpy\u001b[38;5;241m.\u001b[39mndarray, pandas\u001b[38;5;241m.\u001b[39mDataFrame],\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    414\u001b[0m     ] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m    415\u001b[0m ):\n\u001b[0;32m    416\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Fit the AutoML model.\u001b[39;00m\n\u001b[0;32m    417\u001b[0m \n\u001b[0;32m    418\u001b[0m \u001b[38;5;124;03m    Arguments:\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    431\u001b[0m \u001b[38;5;124;03m        AutoML object: Returns `self`\u001b[39;00m\n\u001b[0;32m    432\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 433\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_fit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msample_weight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcv\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msensitive_features\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\YanPC\\Downloads\\automl_hw2-master\\.venv\\lib\\site-packages\\supervised\\base_automl.py:1195\u001b[0m, in \u001b[0;36mBaseAutoML._fit\u001b[1;34m(self, X, y, sample_weight, cv, sensitive_features)\u001b[0m\n\u001b[0;32m   1191\u001b[0m     trained \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mensemble_step(\n\u001b[0;32m   1192\u001b[0m         is_stacked\u001b[38;5;241m=\u001b[39mparams[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mis_stacked\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[0;32m   1193\u001b[0m     )\n\u001b[0;32m   1194\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1195\u001b[0m     trained \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mparams\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1196\u001b[0m params[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstatus\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtrained\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m trained \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mskipped\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1197\u001b[0m params[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfinal_loss\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_models[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m]\u001b[38;5;241m.\u001b[39mget_final_loss()\n",
      "File \u001b[1;32mc:\\Users\\YanPC\\Downloads\\automl_hw2-master\\.venv\\lib\\site-packages\\supervised\\base_automl.py:401\u001b[0m, in \u001b[0;36mBaseAutoML.train_model\u001b[1;34m(self, params)\u001b[0m\n\u001b[0;32m    397\u001b[0m \u001b[38;5;66;03m# start training\u001b[39;00m\n\u001b[0;32m    398\u001b[0m logger\u001b[38;5;241m.\u001b[39minfo(\n\u001b[0;32m    399\u001b[0m     \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTrain model #\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_models)\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m / Model name: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mparams[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mname\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    400\u001b[0m )\n\u001b[1;32m--> 401\u001b[0m \u001b[43mmf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mresults_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel_subpath\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    403\u001b[0m \u001b[38;5;66;03m# keep info about the model\u001b[39;00m\n\u001b[0;32m    404\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mkeep_model(mf, model_subpath)\n",
      "File \u001b[1;32mc:\\Users\\YanPC\\Downloads\\automl_hw2-master\\.venv\\lib\\site-packages\\supervised\\model_framework.py:254\u001b[0m, in \u001b[0;36mModelFramework.train\u001b[1;34m(self, results_path, model_subpath)\u001b[0m\n\u001b[0;32m    251\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(learner\u001b[38;5;241m.\u001b[39mmax_iters):\n\u001b[0;32m    252\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcallbacks\u001b[38;5;241m.\u001b[39mon_iteration_start()\n\u001b[1;32m--> 254\u001b[0m     \u001b[43mlearner\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    255\u001b[0m \u001b[43m        \u001b[49m\u001b[43mX_train\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    256\u001b[0m \u001b[43m        \u001b[49m\u001b[43my_train\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    257\u001b[0m \u001b[43m        \u001b[49m\u001b[43msample_weight\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    258\u001b[0m \u001b[43m        \u001b[49m\u001b[43mX_validation\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    259\u001b[0m \u001b[43m        \u001b[49m\u001b[43my_validation\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    260\u001b[0m \u001b[43m        \u001b[49m\u001b[43msample_weight_validation\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    261\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlog_to_file\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    262\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_max_time_for_learner\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    263\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    265\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mparams\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124minjected_sample_weight\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mFalse\u001b[39;00m):\n\u001b[0;32m    266\u001b[0m         \u001b[38;5;66;03m# print(\"Dont use sample weight in model evaluation\")\u001b[39;00m\n\u001b[0;32m    267\u001b[0m         sample_weight \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\YanPC\\Downloads\\automl_hw2-master\\.venv\\lib\\site-packages\\supervised\\algorithms\\catboost.py:225\u001b[0m, in \u001b[0;36mCatBoostAlgorithm.fit\u001b[1;34m(self, X, y, sample_weight, X_validation, y_validation, sample_weight_validation, log_to_file, max_time)\u001b[0m\n\u001b[0;32m    222\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel\u001b[38;5;241m.\u001b[39mset_params(iterations\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mparams\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnum_boost_round\u001b[39m\u001b[38;5;124m\"\u001b[39m))\n\u001b[0;32m    223\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mearly_stopping_rounds \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mparams\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mearly_stopping_rounds\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;241m50\u001b[39m)\n\u001b[1;32m--> 225\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    226\u001b[0m \u001b[43m    \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    227\u001b[0m \u001b[43m    \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    228\u001b[0m \u001b[43m    \u001b[49m\u001b[43msample_weight\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msample_weight\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    229\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcat_features\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcat_features\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    230\u001b[0m \u001b[43m    \u001b[49m\u001b[43minit_model\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmodel_init\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    231\u001b[0m \u001b[43m    \u001b[49m\u001b[43meval_set\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43meval_set\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    232\u001b[0m \u001b[43m    \u001b[49m\u001b[43mearly_stopping_rounds\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mearly_stopping_rounds\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    233\u001b[0m \u001b[43m    \u001b[49m\u001b[43mverbose_eval\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    234\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    236\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel\u001b[38;5;241m.\u001b[39mbest_iteration_ \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    237\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m model_init \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[1;32mc:\\Users\\YanPC\\Downloads\\automl_hw2-master\\.venv\\lib\\site-packages\\catboost\\core.py:5100\u001b[0m, in \u001b[0;36mCatBoostClassifier.fit\u001b[1;34m(self, X, y, cat_features, text_features, embedding_features, sample_weight, baseline, use_best_model, eval_set, verbose, logging_level, plot, plot_file, column_description, verbose_eval, metric_period, silent, early_stopping_rounds, save_snapshot, snapshot_file, snapshot_interval, init_model, callbacks, log_cout, log_cerr)\u001b[0m\n\u001b[0;32m   5097\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mloss_function\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;129;01min\u001b[39;00m params:\n\u001b[0;32m   5098\u001b[0m     CatBoostClassifier\u001b[38;5;241m.\u001b[39m_check_is_compatible_loss(params[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mloss_function\u001b[39m\u001b[38;5;124m'\u001b[39m])\n\u001b[1;32m-> 5100\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_fit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcat_features\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtext_features\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43membedding_features\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msample_weight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbaseline\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43muse_best_model\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   5101\u001b[0m \u001b[43m          \u001b[49m\u001b[43meval_set\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mverbose\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlogging_level\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mplot\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mplot_file\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcolumn_description\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mverbose_eval\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmetric_period\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   5102\u001b[0m \u001b[43m          \u001b[49m\u001b[43msilent\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mearly_stopping_rounds\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msave_snapshot\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msnapshot_file\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msnapshot_interval\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minit_model\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlog_cout\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlog_cerr\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   5103\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\n",
      "File \u001b[1;32mc:\\Users\\YanPC\\Downloads\\automl_hw2-master\\.venv\\lib\\site-packages\\catboost\\core.py:2319\u001b[0m, in \u001b[0;36mCatBoost._fit\u001b[1;34m(self, X, y, cat_features, text_features, embedding_features, pairs, sample_weight, group_id, group_weight, subgroup_id, pairs_weight, baseline, use_best_model, eval_set, verbose, logging_level, plot, plot_file, column_description, verbose_eval, metric_period, silent, early_stopping_rounds, save_snapshot, snapshot_file, snapshot_interval, init_model, callbacks, log_cout, log_cerr)\u001b[0m\n\u001b[0;32m   2315\u001b[0m allow_clear_pool \u001b[38;5;241m=\u001b[39m train_params[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mallow_clear_pool\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[0;32m   2317\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m log_fixup(log_cout, log_cerr), \\\n\u001b[0;32m   2318\u001b[0m     plot_wrapper(plot, plot_file, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mTraining plots\u001b[39m\u001b[38;5;124m'\u001b[39m, [_get_train_dir(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mget_params())]):\n\u001b[1;32m-> 2319\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_train\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   2320\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtrain_pool\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2321\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtrain_params\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43meval_sets\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2322\u001b[0m \u001b[43m        \u001b[49m\u001b[43mparams\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2323\u001b[0m \u001b[43m        \u001b[49m\u001b[43mallow_clear_pool\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2324\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtrain_params\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43minit_model\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\n\u001b[0;32m   2325\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   2327\u001b[0m \u001b[38;5;66;03m# Have property feature_importance possibly set\u001b[39;00m\n\u001b[0;32m   2328\u001b[0m loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_object\u001b[38;5;241m.\u001b[39m_get_loss_function_name()\n",
      "File \u001b[1;32mc:\\Users\\YanPC\\Downloads\\automl_hw2-master\\.venv\\lib\\site-packages\\catboost\\core.py:1723\u001b[0m, in \u001b[0;36m_CatBoostBase._train\u001b[1;34m(self, train_pool, test_pool, params, allow_clear_pool, init_model)\u001b[0m\n\u001b[0;32m   1722\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_train\u001b[39m(\u001b[38;5;28mself\u001b[39m, train_pool, test_pool, params, allow_clear_pool, init_model):\n\u001b[1;32m-> 1723\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_object\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_train\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain_pool\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtest_pool\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mparams\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mallow_clear_pool\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minit_model\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_object\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43minit_model\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[0;32m   1724\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_set_trained_model_attributes()\n",
      "File \u001b[1;32m_catboost.pyx:4645\u001b[0m, in \u001b[0;36m_catboost._CatBoost._train\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32m_catboost.pyx:4694\u001b[0m, in \u001b[0;36m_catboost._CatBoost._train\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "automl = AutoML(\n",
    "    mode=\"Compete\",\n",
    "    ml_task=\"binary_classification\",\n",
    "    total_time_limit=TRAIN_TIME_LIMIT_MLJAR,\n",
    "    eval_metric=\"f1\",\n",
    "    random_state=SEED,\n",
    "    results_path=path.join(OUTPUT_DIR_MLJAR, UNIQUE_ID, \"tmp\"),\n",
    ")\n",
    "train_y = train_y.copy().reshape(-1)\n",
    "print(train_y)\n",
    "automl.fit(train_x.copy(), train_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loaded = AutoML(\n",
    "    mode=\"Compete\",\n",
    "    ml_task=\"binary_classification\",\n",
    "    total_time_limit=TRAIN_TIME_LIMIT_MLJAR,\n",
    "    eval_metric=\"f1\",\n",
    "    random_state=SEED,\n",
    "    results_path=\"output\\\\mljar\\\\20240114_002215\",\n",
    ")\n",
    "\n",
    "print(valid_x.shape, valid_y.shape)\n",
    "\n",
    "print(train_x.shape, train_y.shape)\n",
    "predictions = loaded.predict_proba(valid_x.copy().reshape(-1))\n",
    "\n",
    "score = balanced_accuracy_score(valid_y, predictions)\n",
    "\n",
    "print(f\"Model Balanced Accuracy: {score}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_path = path.join(OUTPUT_DIR_MLJAR, UNIQUE_ID, \"mljar_model_proba.txt\")\n",
    "dump_proba(automl, test_x, output_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ensemble_predict(X, model1, model2):\n",
    "    pred1 = model1.predict_proba(pd.DataFrame(X)).values[:, 1]\n",
    "    pred2 = model2.predict_proba(X)[:, 1]\n",
    "    print(pred1, pred2)\n",
    "    avg_pred = (pred1 + pred2) / 2\n",
    "\n",
    "    return avg_pred\n",
    "\n",
    "\n",
    "final_predictions = ensemble_predict(test_x, predictor, automl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.makedirs(path.join(\"ensamble\", UNIQUE_ID), exist_ok=True)\n",
    "\n",
    "np.savetxt(\n",
    "    path.join(\"ensamble\", UNIQUE_ID, \"123manual_model_pred.txt\"),\n",
    "    final_predictions,\n",
    "    delimiter=\"\\n\",\n",
    "    comments=\"\",\n",
    "    header='\"313201\"',\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
